# 2. The vector space and its products

Vectorising data in a common space

## todo

- the tables are not meant to be looked at -- cf. a typical report table; the quote from MF d&P is really relevant here -- present the contrast?
- the rows are sometimes called 'the observations' -- connect to MF on observation
- the response column is what has to be constructed -- it is added on 
- the basic functions and their transformation of the linear model into a non-linear one
- go straight into the data -- talk about how hastie and others do this;
- the thing about looking at patterns -- someone elses turkish carpet
- Beniger article on history of stats graphics; Marchese on tables;
- bring in `iris` discussion
- do library dependencies with datasets added; create map of datasets; add datasets from other sources; 
- pull apart some diagrams using inkscape -- use this to show what happens to the abstract spaces
- mention how things that don't seem to be vector spaces are put into vector spaces (e.g. market basket analysis)
- use the metaphor of the house architecture and real estate as a figure to help explain the diagramming
- reference history of tables -- e.g. downloaded chapter -- need to get from samsung laptop
- discuss problem of tables -- how they diagrammaticaly generate high D spaces that we then wrestle with, mightly and again diagrammatically -- fallacy of misplaced concreteness?
- see history of statistics in terms of plurality of attempts to deal with the diagramming of the table
- add Foucault on the appearance of deep order in life/language/labour -- this is a really important thread to draw through - in this case through the reconstruction of the _table_. 
- add quotes from Foucault on the table of space of order, and ask how he thought about quantity, etc; and what is happening to tables today.
- quotes from Wark on the vector
- measure of distance -- and its use in knn
- fitting a line -- the cost of the mode
- creation of volumes by multiplication 
- the dot product or inner product - creates the vector space
-  Knowledge as a 'mechanism of statements and visibilities'
-  generalization as what cannot be seen

## quotes to use

>Strata could never organize themselves if they did not harness diagrammatic matters or functions and formalize them from the standpoint of both expression and content. 144 [@Guattari_1988, 144]

## Dataset as auratic diagrams

>In the form of the disciplinary distribution, on the other hand, the table has the function of treating multiplicity itself, distributing it and deriving from it as many effects as possible [@Foucault_1997, 149]

I sometimes have the feeling that machine learners  want to go everywhere in the world, that they aim to take on any problem, that they see no data as too difficult to learn on. This formulation from Matthew Kirk's _Thoughtful Machine Learning_ is typical:

> Machine learning is an amazing application of computation because it tackles problems that are straight out of science fiction. These algorithms can solve voice recognition, mapping, recommendations, and disease detection. The applications are endless, which is what makes machine learning so fascinating [@Kirk_2014, 11]

It is hard to find dissenting or sceptical voices (although Anand Rajaraman and David Ullman in their _Mining Massive Datasets_ do voice doubts about the power of machine learning algorithms [@Rajaraman_2012]). The epistemic imperium is not only due to the generic mathematical or computational techniques. It might arise from the data, and in particular the indexical aura of the datasets that play across play across many pages in _Elements of Statistical Learning_.   The power of the techniques  hinges on a very specific diagrammatization of data that has already begun to appear in the NAND truth table. Machine learning literature does admittedly display a somewhat stunning variety of datasets. The plots of New Zealand fishing patterns lie next to plots of factors in South African heart disease. This seemingly diasporic variety of data as well as the variety of domains and situates the datasets index are salient components of the experience of reading these texts.  What do we make of the datasets? The datasets associated with the book can be read in various ways.  

```{r elemstatlearn_data, echo=FALSE, results = "asis"}
library(ElemStatLearn)
library(xtable)
datasets_results <-data(package='ElemStatLearn')$results[,3:4]
datasets = as.data.frame(datasets_results)
dataset_count <- nrow(datasets)
dataset_table = xtable(datasets, label = 'table:datasets_elemstatlearn')
print(dataset_table)

```

The `r dataset_count` datasets  shown in \ref{table:datasets_elemstatlearn'} typify the diaspora of domains found in [@Hastie_2009; @Hastie_2001]. As mentioned above (by myself, but also by Kirk),  their domains span the scientific, clinical, commercial and media fields. Note that they include speaking, seeing, writing and reading, as well as the broader concerns of galaxies, caner, climate and national economies. This span of interests is not unusual. To give a another example, one of the most cited journal papers in the machine learning literature is Leo Breiman's 2001 article on random forests [@Breiman_2001]. In some ways, it simply continues what statistics as a field has always done: rove  across scattered fields ranging from astronomy to statecraft, from zoology to epidemiology, gleaning data as it goes (see [@Stigler_1986; @Hacking_1990] for samples of this trekking movement). 

In _Elements of Statistical Learning_ and the field of machine learning more generally, something is moving through these datasets. Their diversity is almost aleatory, as if the datasets were the fruit of random derivè in the world.  Rather than random drifting, I think it more likely the field of machine learning performs something like one of its own statistical procedures in 'bagging' (bootstrap aggregating [@Hastie_2009, 300]) datasets in order to average its predictions out across different domains. While the datasets span many domains -- vowels, ozone, bone density, marketing, prostate cancer, and spam -- their diversity  demonstrates the mobility of the techniques. The shape and organisation of these datasets attest to a certain set of alignments or architectures even that we should pay close attention to. The tabular form, the practices of naming and labelling, and the sorting of different data types exemplified in these diverse datasets can tell us a lot about how machine learning organises and energises its movement through worlds. For instance, the different shapes and composition of the datasets could be read as providing indications of the spatial forms that  machine learning algorithms actually can grapple with. There are many different ways to encounter data. Let us imagine it happens somewhat by chance, as when walking down a city street we notice something about a building for the first time. Some feature, old or new, attracts our attention. What is happening in such situations?

[^71]: Glass,  Breast cancer,  Diabetes,  Sonar,  Vowel,  Ionosphere,  Vehicle,  Soybean,  German credit,  Image,  Ecoli,  Votes,  Liver,  Letters,  Sat-images,  Zip-code,  Waveform,  Twonorm,  Threenorm,  Ringnorm,  [@Breiman_2001, 12].


## Four first steps into the data: media, medicine, business and biology

>References to things act simultaneously as reference to (and within) activities. [@Lynch_1993, 193]

'This book is about learning from data' write Hastie, Tibshirani and Friedman on the first page of _Elements of Statistical Learning_, and they rapidly do indeed begin to iterate through some data. On the second page of the book [@Hastie_2009, 2], a table of spam email word frequencies appears (and the problem of spam classification is canonical in the machine learning literature - we return to in Chapter 5). They come from the dataset `spam` [REF TBA]. On the third page, a complicated data graphic appears (Figure 1.1, [@Hastie_2009, 3]. It is a scatterplot matrix of the `prostate` dataset included in the `R` package `ElemStatLearn,` the companion `R` package for the book. In a third example, a set of scanned handwritten numbers appears. These scans are images of zipcode or postcode numbers written on postal envelopes taken from the dataset `zip` [REF TBA], and they differ from both the `spam` table and `prostate` plots because they directly resemble something in the world, which, however, happens to be numbers, and is, therefore, probably already recruited into data-making and data-circulating processes.  The final example in the introduction, 'Example 4: DNA Expression Microarrays,' draws this time from biology, and particularly, high-throughput genomic biology, the kind of science that produces large amounts of data about something in the world by running many tests, or by constructing devices that generate many measurements, in this case, a DNA microarray.[^10] The image shown here is perhaps most striking. No numbers are shown, only a colorful heatmap with brighter colours standing for higher levels of gene expression, and darker colours for lower levels. For all its dense color, the data shown here is a sample of 100 of the approximately 7000 genes in the dataset `nic` [REF TBA] [@Hastie_2009, 6]. In comparison to the medical data from the `prostate` dataset with its 97 rows of 10 columns, the microarray dataset has 64 columns of data.[^11] Both are cancer datasets, but the `nci` dataset cannot be shown in its entirety because it refers to 60 different cell lines ranging across colon, breast, prostate, ovarian and renal cancers, as well as the thousands of different genes.

The combined effect of these four example datasets deriving from network media, from medicine, from business administration and from cutting-edge life science (c.2000) is to suggest a tremendous, indeed almost spectacular convergence, one that in principle could surprise us because there is otherwise little contact between the places these datasets come from. In passing, we should note that the _Elements of Statistical Learning_ is not alone in this juxapositioning opening. Very similar example sets can be seen in most machine learning publications. To give just a few examples: Andrew Ng's CS229 lectures, for instance, treat spams  in Lecture 5 [@Ng_2008h]; Rachel Schutt and Cathy O'Neil's _Doing Data Science_ discusses spam in Chapter 4 [@Schutt_2013], and Peter Flach's _Machine Learning The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] introduces spam in the first few pages. Similar parallels exist around the image recognition problem (exemplified by handwritten digits), around the measurements dataset (`prostate` in the case of [@Hastie_2009], and in relation to the larger, impossible-to-see-the-patterns datasets of the cancer microarray data). How is this conformation and coherence being done? The repetition of data sets, the juxtaposition of discontinuous domains, and forms of movement construct and order continuities in the service of various forms of predictive and inferential knowledge. Amidst thn miscible juxtapositions we are dealing with here lies, it seems, a _regularity_ or a common space that invites archaeological work.

[^10]: This kind of data will be the focus of a later chapter (Chapter 6). Machine learning during the 1990s and 2000s was in some ways boosted heavily by the advent of genomic biology with its large, enterprise style knowledge endeavours such as the Human Genome Project.

[^11]: The data derives from a publication in _Nature Genetics_ [@Ross_2000] analysing gene expression in the cancer cell lines maintained as experimental models by the US National Cancer Institute.


## Truth is no longer in the table

'Things, in their fundamental truth,' writes Foucault in _The Order of Things_ 'have now escaped from the space of the table' [@Foucault_1992, 239]. Foucault of course was writing in these pages about the fabled emergence of life, labour and language as the anchoring vertexes of a new triangle of knowledge and power structuring the figure of the 'human' in the 19th century. The table as a space of order  was abandoned because it could no longer serve as the 'common place for representation and things, for empirical visibility and for the essential rules' (239). Before the emergence of the sciences of the human -- political economy, linguistics and biology -- knowledges such as natural history, the general grammars, and philosophies of wealth (such as Adam Smith's work)  had ordered empirical materials of diverse provenance in tables or grids. These tables themselves supplanted the earlier much more variegated Renaissance tabulations. In those tables, an image or figure from myth might lie alongside a measurement or a count of occurrences, and this proximity was ordered by systems of analogical association that spanned what we might today, in the light of the 19th century, still want to separate.  While the history of tables as data forms reaches a long way back (see [@Marchese_2013] for a broad historical overview that reaches back to Mesopotamia), and tables still definitely abounded and indeed multiplied in the 19th and then 20th centuries, Foucault argues that the system of grids that permitted ranking, sorting and ordering in tables had between the end of the Renaissance and the early nineteenth been based on 'buried similitudes' and 'invisible analogies' [@Foucault_1992, 26]. The grid or table was a way of bringing the otherwise scattered and diverse resemblances into exhaustive enumeration where they could be subjected to analysis by counting and comparison. The table as space of order did not stand alone. Algebra or _mathesis_ more generally applied to 'simple nature' (planets in movement, dynamics of falling bodies, etc), but taxonomy dealt with more complex natures. Even here, a system of signs (the groupings established by the eighteenth century taxonomist Carl Linnaeus), sought to reduce complex natures (plants, animals, etc.) to simpler forms as columns and rows in a table. Importantly, the table as space of order was a space of imagination in that one could begin to see continuities and the genesis of things (including grammar, nature and wealth) by carefully looking at the table. Conversely, the table encompassed all order.   'Hedged in by calculus and genesis,' Foucault suggest, 'we have the area of the _table_' [@Foucault_1992, 73]. Note in passing that calculus and algebra help bound the table, not because everything has been mathematized, but because it laid down a general possibility of ordering a field of identity and differences.

In the nineteenth century, such tabulations were replaced by a different form of ordering. Foucault figures this change as shattering:

>The space of order, which served as a _common place_ for representation and for things, for empirical visibility and for the essential rules, which united the regularities of nature and the resemblances of imagination in the grid of identities and differences, which displayed the empirical sequence of representations in a simultaneous table, and made it possible to scan step by step, in accordance with a logical sequence, the totality of nature's elements thus rendered contemporaneous with one another -- this space of order is from now on shattered: there will be things, with their own organic structures, their hidden veins, the spaces that articulates them, the time that produces them; and then representation, a purely temporal succession, in which those things address themselves (always partially) to a subjectivity [@Foucault_1992, 239-240].

Life, labour and language -- the figure of the human  -- famously replace the exhaustive, aggregative, synoptic classificatory tables of the Classical age. From now on there are things, organically structured, relating to a place and changing in time, and there are representations addressed to (and constitute) of a subjectivity. Double interiority takes over. Things such as a language, a species or an economic system have their own genesis, and our knowledges and indeed experience too become finite, historical, with their own dynamics and internal life. Knowledge is, for instance,  epistemologized in various ways so as to become scientific. Writing in the 1960s, Foucault postulates that 'our thought still belongs to the same dynasty' (243).

This brief resumé of one of the main arguments in _The Order of Things_ might help us see what is being diagrammed in the data tables we find in machine learning, and in particular, in the series of tables of data with which _Elements of Statistical Learning_ begins. The question here is how we make sense of these tables in terms of the broader diagram they make up. Presumably the spam, prostate, image and genetic data are not put together in the way Linnaeus would have put together a table of living things. Yet their regular juxtaposition does suggest we should be looking for alignments and analogies between them. At the same time, the topics of these tables -- they concern work, life, language and economy in various ways -- maps very readily onto the anchor points, the new empiricities of labour, life and language.  The paradox here is that things may longer be hedged in by the table, their fundamental truth may have escaped from the space of the table, and yet we are confronted by many more tables. In many senses, we are hedged in by tables, and many different mechanisms animate and multiply tables around us.[^22] In certain respects the tables we see in the first pages of [@Hastie_2009] are much more classical in their scale and in their relatively immutability. In many settings in which machine learning operates, tables change rapidly in scale and sometimes in organisation. Regardless of the differences between the tables of data used to demonstrate  machine learning techniques and data tables used in contemporary operational environments, the key point is that the multiplication and juxtaposition of different tables today suggests that we might be seeing the advent of a neo-classical common space of order for representation and things, for regularities and resemblances, for nature and imagination, or science and media. If that is so, then the question will be what kind of scanning, what kind of step by step movement, traverse them.

[^22]: By and large, I am not discussing networking and database infrastructures here. In other work, I have attempted to account for the multiplication of tables in databases but also in mundane practices of ordering. See [@Mackenzie_2011; @Mackenzie_2012]. My analysis there pivots mainly on a set-driven account of data derived from the work of Alain Badiou. Although I've already been using some set terminology here -- as in 'dataset' -- I'm persisting with a more geometrical and algebraic account of datasets in order to better deal with some of the graphisms common in machine learning. 


## The epistopological transformations of tables

```[r ElemStatLearn_data, echo=TRUE}
library('ElemStatLearn')
datasets = data(package='ElemStatLearn')
datasets$results[,3]
```

The collocation of the four introductory examples of spam, handwriting, cancer and biology is not unusual. As we have already seen, it is relatively common in machine learning literature to juxtapose datasets from science, business, government and media, and then show how one can move between them. The `ElemStatLearn` `R` package brings, as we have seen in the previous chapter, brings with it around 20 different datasets, including the four we see in the opening examples. At this point, I'm not so much interested mapping the domains these datasets relate to, but in exploring how we might look at and speak about such tables.   Building on the diagrammatic account of the _Elements of Statistical Learning_ developed in the previous chapter, this chapter attends to both the regularity and the instability associated with the data forms in machine learning. It does this by following a variegated faultline that runs to a less or greater degree across all machine learning. The faultline runs between what is said and what is shown, between statements and visibilities. Put prosaically, in the informal diagram of machine learning, the graphics and the text cannot quite hold together. They are divided in ways that can be seen by, I propose, by following some transformations of data tables into predictions and inferences.

In moving along this fault, we need to be proceed epistopologically. The word 'epistopological' is a neologism combining 'epistopic' and 'topological.'  The 'epistopic' term comes from the work of the science studies scholar Mike Lynch, whose account of scientific practice is usefully focused on ordinariness. He proposed the 'epistopic' as a way of connecting 'familiar themes from epistemology and general methodology' [@Lynch_1993, 280] with localized practices, or with the local achievement of coherence. In other words, as the term itself suggests, an epistopic connects a general epistemic theme with a place, a 'local complex of activities' (281).  This emphasis on epistemic-placeness is very useful in several ways, especially if it can be brought to bear on the problem of how the 'local complex' of a specific dataset can be transformed by a general epistemic practice such as machine learning.  It's interest in the couplings between 'graphism' and practice should help make sense of some of the graphic forms we see in _Elements of Statistical Learning_ as forms of movement that epistemologize data, or that engender epistemic talk about data. It's concern with the connections between seemingly generic scientific epistemological statements about  error, bias, variation, confidence, expectation and likelihood and graphically visible and down-to-earth practices such as drawing a line, labelling a point or sorting a list of values is a good antidote to the sometimes ballistic epistemological trajectories fuelled by statistical machine learning (e.g. the infamous and heavily cited comments by the former _Wired_ magazine editor, Chris Anderson, on the 'end of theory').  But what kind of transformations occur here? Over a decade ago, the cultural theorist Brian Massumi wrote that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@Massumi_2002, 184]. Much earlier, Gilles Deleuze had conceptualised Michel Foucault's philosophy as a topology, or 'thought of the outside' [@Deleuze_1988], as a set of movements that sought to map the diagrams that generated a 'kind of reality, a new model of truth' [@Deleuze_1988, 35]. More recently, this topological thinking has been extended and developed by Celia Lury amongst others. In 'The Becoming Topological of Culture,' Lury, Luciana Parisi and Tiziana Terranova suggests that 'a  new rationality is emerging: the moving ratio of a topological culture' [@Lury_2012, 4]. In this new rationality, practices of ordering, modelling, networking and mapping co-constitute culture, technology and science [@Lury_2012, 5].  At the core of this new rationality, however, lies a new ordering of continuity. The 'ordering of continuity,' Lury, Parisi and Terranova propose, takes shape 'in practices of sorting, naming, numbering, comparing, listing, and calculating' (4). The phrase 'ordering of continuity' is slightly odd, since we don't normally think of continuities as subject to ordering. In many ways, that which is continuous bears within it its own ordering, its own immanent seriation or lamination. But in the becoming topological of culture movement is itself undergoing a transformation according to these authors. Rather than movement as something moving from place to place relatively unchanged (as in geometrical translation), movement should be understood as more like an animation, a set of shape-changing operations. These transformations, I would suggest, should be legible in the way that machine learning, almost the epitome of the processes of modelling and calculation that Lury, Parisi and Terranova point to, itself moves through the data. And indeed, the juxtaposition of spam, biomedical data, gene expression data and handwritten digits already suggests that topological equivalences, and a 'radical expansion' of comparison might be occurring. Bringing epistopics and topologies together might, I suggest, help trace, map and importantly diagram some of the movements into the data occurring today.


[^12]: The _locus classicus_ account of such transformation is perhaps Michel Foucault's _The Order of Things: An Archaeology of the Human Sciences_ [@Foucault_1992], and his account of what happened to tables in the 18th century will be of some use in the pages to follow. 

## Surface and depths: the problem of volume in data 

If the epistopological is a mode of diagramming, of finding those traits in data tables that are capable of combining or conjugating with each other, but are not reducible to each other, how does one do it in practice? Let us see how Hastie, Tibshirani and Friedman begin to move through the datasets. First of all, they select the first three (`spam`, `prostate` and `zip`), leaving aside the cancer genes (`nci`). They say:

>The first three examples described in Chapter 1 have several components in common. For each there is a set of variables that might be denoted as inputs, which are measured or preset. These have some influence on one or more outputs. For each example the goal is to use the inputs to predict the values of the outputs. This exercise is called supervised learning. We have used the more modern language of machine learning.[@Hastie_2009, 9].

In the ensuing discussion,  they begin to dissect the tables and point to various discontinuities that run down the tables. The major discontinuity is already remarked in the common cut between 'inputs which are measured' and values that might called 'outputs.' Not all tables are cut this way, and this cut could be made differently. Importantly, the cut is epistopic since the table now shows that some things are measured and some might not be (the values of the outputs). In addition to this vertical discontinuity, a horizontal discontinuity can also be seen (and Hastie et. al. go on to describe it). Some of the variables in the table are continuous, others are discrete, and indeed may be categorical or nominal (for instance, when predicting the actual digit from the handwritten digits, the predicted digit itself is a categorical variable, since the prediction must choose one of the digits 0,1,2,3,4,5,6,7,8 or 9).  Discrete qualitative data appears frequently in machine learning, or wherever classification problems can be found. (I will have much more to say about classification and machine learning classifiers in later chapters. For the moment, we need only note that prediction the class to which things belong is a major topic of interest.)

```{r prostrate, echo=TRUE, message=FALSE, results = 'asis', fig.cap='Scatter plot matrix of prostate data'}
library('ElemStatLearn')
library(xtable)
data('prostate')
pairs(prostate[, -10], main='Scatterplot matrix of the `prostate` dataset', cex=0.2)
print(xtable(head(prostate), label= 'tab:prostate', caption='First rows of the \`prostate\` dataset'))
```

Of the three example datasets  (`prostate`, `spam` and `zip`), the authors return most frequently to `prostate.` This dataset derives from the work of urologists working at Stanford [@Stamey_1989], and, as [@Hastie_2009, 67] points out, concerns various clinical measurements performed on men who were about to undergo radical prostatectomy. The measurements range across the volume and weight of the prostate, as well as levels of various prostate-related biomarkers such as PSA -- prostate specific antigen. Several rows from the dataset are shown in Table \ref{tab:prostate}. In the first pages of the book, they had already quite exhaustively plotted all the variables in the dataset against each other using a scatter plot matrix [@Hastie_2009,3] (show in \ref{fig:prostate}, and they return to the same data on almost a dozen occasions in the course of the book.

The transformation between the Table \ref{tab:prostate} and the Figure \ref{fig:prostate} already evinces a vector \index{vector} of movement commonly found around such datasets. On the one hand, the table arrays all the different data types indifferently in rows and columns. The relation between the different data types (the log of the weight of prostate - `lwp` and `age` for instance) is quite hard to see. Moreover, different kinds of variables stand side by side. `svi`, short for 'seminal vesicle invasion' is a categorical variable. It takes the values 'true' or 'false,' shown here as `1` or `0`, but the other variables either measure or count things (years, sizes, or levels of antigens).  On the other hand, the scatter plot matrix also takes the form of a grid-like figure, but the squares of the grid are not occupied by numbers but by `x-y` plots of different pairs of variables in the `prostate` dataset. The 'matrix' of figures shows of 72 plots is mirrored across the diagonal that runs from the top-life to bottom right in Figure \ref{fig:prostate}. Taking this folding into account, see 36 unique plots with different data in each one. Each sub-plot displays the relation between two variables in the dataset as a scatter plot. Note how certain variables such as `svi` are not very amenable to plotting in this way. More importantly perhaps, note how certain combinations of variables result in visible forms can be seen as signs of relations between different variables. The matrix constructs a space in which contrasts between the pairs of variables that yield scattered clouds of points and those that show distinct clusters or tendencies can be seen. In the light of these contrasts (and I use 'light' here in an almost literal sense to refer to the way in which the architecture of the figure creates a space in which light scatters in varying patterns), th `prostate` dataset begins to expose relations that might be worth knowing about. We have moved on from the bare table of the dataset to a transformed tabulation, from a textual-numerical grid to a geometrical-textual grid. Everything remains on the surface of a grid here, but the grid functions differently in the scatterplot matrix. 

All of this lies somewhat in a pre-machine learning space. Similar tables and plots are part and parcel of statistical data analysis more generally. But the point of the scatterplot matrix is not to exhaust the dataset, but rather to highlight the need to constantly revisit it (12 times in the _Elements of Statistical Learning_) in order to tease out the hidden relations that remain opaque to even the most exhaustive matrices of plots. We cannot clearly see in the scatterplot matrix more than pairs of variables in relation. If the crucial diagnostic measure in this case is the level of the PSA (prostate specific antigen), how do we know what combinations of other measurements might be associated with its elevation? What is multiple variables affect the level of PSA?[^31] This question can be pursued by scanning the matrix of plots but not very stably since different data analysts might see different associations combining with each other there. Different statements could be supported by the same figure. Perhaps worse, the very question of relation between multiple variables and the predicted levels of PSA suggests the existence of a hidden volume, an occluded or internal space in the figure that cannot be seen, or that cannot be brought to light. This volume is not the measured volume of the prostate but the virtual volume suggested by both the dataset table and the scatterplot matrix, a nine dimensional space defined by the relation between all nine variables in the dataset. When Foucault wrote of truth escaping the table, he might well have pointed towards the higher-dimensional volumes of the _vector space_ into which lines were already beginning to regress even in the late 18th century.[^33]. Put differently, I am suggesting there is a common space of order beginning to take shape, and strenuous efforts are made in machine learning to ensure that as much as possible goes into that space. 

[^31]: Despite the intensive work that Hastie and co-authors conduct on the `prostate` data, all with a view to better predicting PSA levels using volumes and weights of prostates, etc., Stamey and other urologists more than a decade or so concluded that PSA is not a good biomarker for prostate cancer. Stamey writes in 2004:
    What is urgently needed is a serum marker for prostate cancer that is truly proportional to the volume and grade of this ubiquitous cancer, and solid observations on who should and should not be treated which will surely require randomized trials once such a marker is available. Since there is no such marker for any other organ confined cancer, little is likely to change the current state of overdiagnosis (and over-treatment) of prostate cancer, a cancer we all get if we live long enough unbound points in the matrix [@Stamey_2004, 1301]

[[^33:] Carl Friedrich Gauss and Adrien-Marie Legendre's work on linear regression at this time is well-known. The first independent use of linear regression was Gauss' prediction of the location of an 'occluded volume,' the position of the asteroid Ceres after it reappeared in its orbit behind the sun. [@Stigler_2002] -- TBA page ref

## The expansion of the vector space

To show this space in the making, we might follow what happens to just one or two columns of the `prostate` data in the common vector space as it is prepared for machine learning. In the `prostate` dataset, some variables are continuous quantitative values, and some are categorical (they represent membership in a group or category) or ordinal variables (they represent a ranking or order). But a variable such as `svi` has `True` or `False` values. How can such values be positioned in the vector space, and thus be subject to manipulation by linear algebra? In order to put classifications or categories into vector space, they need to be translated into the same _basis_ as the quantitative variables with their rather more obvious geometrical and linear coordinate values. How does one geometrically or indeed algebraically render a category so that it can be mobilised in the way that Equation \ref{eq:linear_model_vector}, the vector form of the linear regression model, suggests it might be? The problem is solved via a form of binary coding:

>Qualitative variables are typically represented numerically by codes. The easiest case is when there are only two classes or categories, such as “success” or “failure,” “survived” or “died.” These are often represented by a single binary digit or bit as 0 or 1, or else by −1 and 1. ... When there are more than two categories, several alternatives are available. The most useful and commonly used coding is via dummy variables. Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is “on” at a time [@Hastie_2009, 12] 

Again, the details are not so important here as the transformations that the common space permits once things inhabit it. Note that a single qualitative or categorical variable expands into 'a vector of K binary variables or bits.' The dimensions of the vector space expand accordingly, and the linear regression model treat these added dimensions as variables to be included in the model. Qualitative data, once coded in this way, can be multiplied, added, and in short, handled algebraically using the same aggregate operations we saw in discussing linear algebra more generally. Note also that not only has the vector space expanded here, this expansion smooths over important gaps or differences that figure large in the dataset. The different kinds of variables -- qualitative and quantitative, discrete and continuous, nominal and ordinal -- can be accommodated in the expanded dimensions  of the vector space, at the cost of multiplying dimensions, and making the problem of seeing the volumes and densities of data distributed in this space more challenging.   

## Traversing behind the light 

The character of the transformations in `prostate` that ensue in _Elements of Statistical Learning_ are difficult to summarise. Once this hidden volume in the data is glimpsed, many strenuous efforts will be made to bring it to light, even sometimes blatantly disregarding relevant obstacles in the domain that originally produced the data.[^34] These efforts will proceed along different lines. Sometimes this space is treated as one filled with constantly varying proximities and similarities, and machine learning techniques gather and order these differences (for instance, as in the _k_ nearest neighbours model) or in unsupervised methods such as k-means clustering [@Hastie_2009, 513].  More commonly, the tools of machine learning draw lines through the volume using some version of linear regression, perhaps the most important modelling technique in modern statistics.  The importance of the linear modelling, finding lines of best fit,  should not be under-estimated. Drawing lines at various angles is perhaps the main which in the volume of data is traversed. One sign of the centrality of the line in machine learning can be seen, for instance, from the contents page of the book [@Hastie_2009, xiii-xxii]. After the introduction of the linear model in the first chapter and its initial exposition in chapter 2 ('Overview of Supervised Learning'), it forms the central topics of chapter 3 ('Linear Methods for Regression'), chapter 4 ('Linear methods for classification'), chapter 5 ('Basis Expansions and Regularization'), chapter 6 ('Kernel Smoothing Methods'), much of chapter 7 ('Model Assessment and Selection'), chapter 8 ('Model Inference and Averaging'), major parts of chapter 9 ('Additive Models, Trees and Related Methods'), important parts of chapter 11 ('Neural Networks' -- neural networks can be understood as a kind of regression model), the anchoring point of chapter 12 ('Support Vector Machines and Flexible Discriminants') and the main focus in the final chapter ('High Dimensional Problems').[^35] While it is more or less obvious to eye from the scatterplot matrix that lines could be drawn through the clouds of points as a way of defining directions of movement, the way to draw  these lines in higher dimensions is not obvious. The line of best has a readily graspable geometrical intuition to it, even in higher dimensions, and that line can be diagrammed, as we have seen in the preceding chapter, by making use of the equations of linear algebra, the field of mathematics that deals with lines in spaces of arbitrary dimensions. It is no accident that linear algebra is such a taken-for-granted part of  machine learning that its techniques for finding intersections between lines and planes, of manipulating collections of lines and surfaces through mappings and transformations (rotations, displacements or translations, skewing, and scaling), and above all, treating systems of equations using aggregate forms such as matrices and vectors. It brings with it a set of formalisations -- vector space, dimension, matrix, determinant, coordinate system, linear independence, eigenvectors and eigenvalues, inner-product space, etc. -- that appear over and over again in the toolboxes of machine learning.[^36]

[^34]: This problem of detachment affects machine learning quite seriously in some settngs. I return to this problem and various responses to it in Part II of the book in the context of a comparison between epidemiological models and the Google Flu Trends (see Chapter 7). 

[^35]: A similar topic distribution can be found in Andrew Ng's CS229 Lectures on Machine Learning. More than half of the 20 lectures concern linear models and their variants. See [@Ng_2008a; @Ng_2008b;@Ng_2008c;@Ng_2008d].

[^36]: Along with statistics and probability, linear algebra is a such an important part of machine learning that many books and courses recommend students complete a linear algebra course before they study machine learning. Cathy O'Neill and Rachel Schutt advise:
    When you’re developing your skill set as a data scientist, certain foundational pieces need to be in place first—statistics, linear algebra, some programming [@Schutt_2013, 17]

We will have reason to examine some important ways in which linear algebra structures machine learning practices in later chapters, but for the moment, it might be understood as offering a way to draw lines through spaces that can only be expressed diagrammatically in the form of equations, not in the form of figures.[^41] Already in the preceding chapter, in the context of the discussion of the basic linear regression model, linear algebra was at work. We discussed there the 'mainstay of statistics,' the linear regression model. 

\begin {equation}
\label {eq:linear_model}
\hat{Y}=\hat{\beta_0}  + \sum^p_{j=1}X_j\hat{\beta_j}
\end {equation}


\begin {equation}
\label {eq:linear_model_vector}
\hat{Y} = X_T\hat{\beta}
\end {equation}

The concision of this way of diagramming the drawing of line through a high dimensional space derives largely from linear algebra. Reading Equation \ref{eq:linear_model} from left to right, the expression $\hat{Y}$ already points to a set of values, or a vector of $y$ values, such as all the `lpsa` or PSA readings included in the `prostate` dataset. Similarly, the term $X_j$ points to the table of all the other variables in the `prostate` dataset. Since there are 8 other variables, and close to 100 rows, $X$ is a matrix of values, addressable by coordinates. Finally $\beta_j$ are the pivotal coefficients that determine the slope of the lines drawn. The problem of calculating optimal values of $\beta$ has attracted and continues to attract the attention of statisticians for a long time (at least two centuries). The second expression Equation  \ref{eq:linear_model_vector} relies more fully on linear algebra. This is the linear model written in 'vector form' [@Hastie_2009, 11]. The right hand side comprises two operations $X^T$, the transpose or rotation of the data, and implicitly -- why is multiplication hardly ever shown, but simply conveyed by putting terms alongside each other -- an _inner product_ of the $X$ matrix and the $\beta$ parameters (to use model talk) or coefficients (to use linear algebra talk). So, in the expression for Equation \ref{linear_model} we can begin to the diagramming of a line that cannot be fully drawn in any figure, only projected onto the dimensions of a plot. In spite of these limitations, it can be readily imagined and conceptualised within the expandable dimensions of vector spaces. While that line can never fully come to light, the diagram expresses a way of constructing it, if only we can find ways of drawing it through the data. Much effort, many techniques, and whole sub-fields of science attach themselves to various ways of constructing and making statements about such lines. More abstractly, we might see such expressions as epistopological in the way that they diagrammatically hold together the flat enunciation of tabulated data with the figurative visibility of lines drawn through points plotted in a coordinate space. Between the statement of the data and the figure of data, the diagrammatic expression of the linear model creates a new kind of anamorphic or diagonal space, 'constituting hundreds of points of emergence or creativity, unexpected conjunctions or improbable continuums' as Deleuze puts it in describing such diagrams [@Deleuze_1988,35]. 


[^41]: Learning machine learning, and learning to implement machine learning techniques, is largely a matter of implementing series of matrix multiplications. As Andrew Ng advises his students,

    > Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [@Ng_2008a, 10:50]

    In other parts of his teaching, and indeed throughout the practice exercises and assignments, Ng stresses the value of implementing machine learning techniques for both understanding them and using them properly. But this is one case where implementation does not facilitate learning. Ng advises his learners against implementing their own matrix handling code.    They should instead  use the 'great linear algebra libraries' found in 'almost any programming language.' 'Linear algebra libraries' multiply, transpose, decompose, invert and generally transform matrices and vectors. They will be 'highly optimised' not because every programming language has been prepared for the advent of machine learning on a large scale, but rather more likely because matrix operations are just so widely used in image and audio processing.  Happily, Ng observes, that means that 'you can make lots of predictions' [@Ng_2008e].   It seems that generating predictions and hypotheses outweighs the value of understanding how things work on this point. 

## Drawing lines in a common space of transformation

In high school mathematics, at least since WWII, students have been taught how to solve systems of linear equations, first using algebra, and then using matrix operations. Solving a system of linear equations means finding those values of the variables that satisfy all the equations in the system. If such values can be found, we know that the lines expressed by equations of the form $y = ax_1 + bx_2 + c$ intersect at a point, along a line, in some sub-space. In all of these cases, the diligent mathematics students solves to find the common space (show in \ref{fig:common_space}), even if it consists in a single point, inhabited by the elements of the system.

\begin{figure}
  \centering
      \includegraphics[width=0.5\textwidth]{figure/intersection.pdf}
        \caption{Common space in linear equations}
  \label{fig:common_space}
\end{figure}

Similarly, although the mathematics is slightly more complicated than high school level (although not by much), drawing the line of best fit through a set of data points can be seen as solving a system of linear equations. Viewed in terms of linear algebra, the 'solution' to the linear model is given in equation \ref{eq:linear_model_solution}:

\begin {equation}
\label {eq:linear_model_solution}
\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}
\end {equation}

The derivation of the analytical 'ordinary least squares' solution relies on some calculus as well as a range of linear algebra operations such as matrix transpose, inner product and matrix inversion, the details of which need not trouble us here. (As usual, and in keeping the diagrammatic reading of these forms, these operations all consist of ways of either moving or combining numbers in practice.) The relevant point is that equation \ref{eq:linear_model_solution} assigns some values to $\hat\beta$ and therefore gives a slope and intercept for a line drawn through the points in their full dimensional vector space (nine dimensions in the case of `prostate`).[^38] Perhaps more importantly, the linear algebraic expression of these operations presupposes that all the data, both the values used to build the model and the predicted values the model may generate as it is refined or put into operation somewhere, are contained in a common space, the vector space, a space whose formation and transformation can be progressively ramified and reiterated by various lines that either separate volumes in the space, or head in a direction that brings along most of the data. Not all of these lines are bound to be straight, and much of the variety and dispersion visible in machine learning techniques comes from efforts to construct different kinds of lines or different kinds of 'decision boundaries' (in the case of classification problems) in vector space (for instance, the k-nearest neighbors method does not construct straight lines, but somewhat meandering curves that weave between nearby vectors in the vector space; see [@Hastie_2009, 14-16]). Whether they are straight or not, the epistemic problem of these lines remain. While their production, whether through linear algebraic operations or some other means, occurs in the vector common space, their epistopology remains both potentially a generative site and a problematic one. Typically, many different statistical tests (Z-scores or standard errors, F-tests, confidence intervals, and then prediction errors) will be applied to any estimate of the parameters of even the basic linear regression model, well before most advanced or sophisticated models and techniques (cross-validation, bootstrap testing, subset and shrinkage selection) begin to re-configure the model in more radical ways. 

To take a relatively straight forward example of this, the first treatment of the `prostate` data in _Elements of Statistical Learning_ appears in an early chapter on linear regression. They construct a linear regression model of the data in order to predict PSA values, the biochemical marker thought to be associated with prostate cancer. Hastie, Tibshirani and Friedman describe their treatment of the data:

>We fit a linear model to the log of prostate-specific antigen, `lpsa`, after first standardizing the predictors to have unit variance. We randomly split the dataset into a training set of size 67 and a test set of size 30. We applied least squares estimation to the training set, producing the estimates, standard errors and Z-scores shown in Table 3.2. [@Hastie_2009, 50]

As is common in machine learning texts, terse descriptions of data preparation such as 'first standardizing the predictors to have unit variance' often take for granted the transformations in the vector space. In this case, 'standardizing the predictors' entails a series of transformations of the data using linear algebra we have just been discussing. If we turn to the code that might have been written to implement this standardization, we can also begin to understand how these transformations are naturalized or taken for granted. Presuming they were using `R`, the code they use to centre all the data around `0` might look something like this:

```{r prostate, echo=TRUE}
library(ElemStatLearn)
data(prostate)
columns_to_standardize = c(1,2,3,4,6,9)
prostate_standard = as.matrix(prostate[, columns_to_standardize]) 
standardize_operator <- function(x) {
        n = nrow(x)
        ones = rep(1, n)
        H = diag(n) - (1/n) * (ones %*% t(ones))
        H %*% x
}
prostate_centred = standardize_operator(prostate_standard)
prostate_standardized = scale(prostate_standard)
```

`R` has a `scale` function that more or less does what lines of code shown above do, and in less lines. (It is shown in the last line of code.) The code vignette shown above does things in a more elaborate and extended way than necessary because I am interested in rendering visible the forms of movement occurring in data re-constituted as a vector space. After it loads the data, and selects the columns of the table to be standardized (remember the `prostate` data includes columns such as `svi` -- seminal vesicle invasion -- that contain only boolean True/False values and therefore cannot be standardized), the code defines a function called `standardize_operator`. Again, this code apparatus is not strictly necessary here, but it points to the fact that vector spaces are operational spaces. Movement in vector space relies on operations that translate, rotate, skew or re-center sheets and surfaces. The components of the `standardize_operator` attract most of the computation here. Operators such as `diag` and `%*%` are linear algebraic operations that construct matrices and multiply them (so-called 'dot product') respectively. In the process of centering the data around `0`, the code constructs new vectors (`ones`), a new matrix `H` that in the last line of the function transforms the relevant columns of the `prostate` data in a matrix-matrix multiplication. 

[^38]: As we will see in the following chapter, it is not always possible to calculate the parameters of a model analytically. Especially in relation to contemporary datasets that have very many variables and many instances (rows in the table), linear algebra approaches become unwieldy in their attempt to produce exact results, and machine learning steps in with a variety of computational optimisation techniques. In many cases

## Vectorisation and the transformation of common space 

Several specific points in the lines of code above  -- the  `diag` and the `rep`  function as well as the `%*%` operator  -- demonstrate forms of densely coiled movement or superimposition commonly found in software such as  `R` and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo): _vectorised_ transformations of data.  As a programming language, `R` is striking for its many  vectorised constructs.  There are many in the core language and many to be found in packages (the popular 'plyr' package; versions of 'ply' can also be found in recent Python data analysis libraries such as `numpy` or `pandas` [@McKinney_2012]).  In many cases, these vectorised operations occur implicitly. `R` sometimes presents difficulties for programmers trained to code using so-called procedural programming languages because it so thoroughly embraces the notion of the _vector_ -- and hence, regards all data as inhabiting vector space. In many mainstream programming languages, transformations of data rely on loops and array constructs in which some operation is successively repeated on each element of a data structure. Moreover,  in vectorised languages such as `R`, transformations of a data structure  expressed in one line of code  simultaneously affect all the elements of the data structure. As the widely used _R Cookbook_ puts it, 'many functions [in `R`] operate on entire vectors ...  and return a vector result' [@Teetor_2011, 38]. Or as _The Art of `R` Programming: A Tour of Statistical Software Design_ by Norman Matloff puts it, 'the fundamental data type in `R` is the _vector_' [@Matloff_2011, 24], and indeed in `R`, all data is vector. There are no individual data types,  only varieties of vectors in `R`. Or, as _R in a Nutshell_  puts the point: 'in `R`, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@Adler_2010, 17]. 

```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
     vector1 <- c(0,1,2,3,4,5,6,8,9,10)
     vector2 <- c(0,1,2,3,4,5,6,8,9,10)

     #procedural programming-style looped addition
     result_looped = vector()
     for (i in vector1) {
        result_looped[i] = vector1[i] + vector2[i]
     }
     result_looped

     #vectorised addition
     result_vectorised  <- vector1 + vector2
     result_vectorised
```

The practical difference between the two approaches to moving through data is illustrated in the code vignette above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardware optimisations or carefully-crafted low-level linear algebra libraries. More importantly, this is a different mode of movement. Operations now longer step through a series of coordinates that address data elements, but wield plans, surfaces, cross-sections or stratifications of the vector space. 

 A further level of vectorisation appears in the vectorisation of functions in `R`. Specific `R` constructs such as `apply`, `sapply`, `tapply`, `lapply`, `mapply` exemplify these vectorised functions. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc.), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of data set in turn and doing something), `R` offers ways of dealing with  them all at once. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@Church_1936;@Church_1996]. The functional programming style of applying functions to functions seems strangely abstract.  These -ply constructs  and the implicit vectorisation of many operations on data structures are very can be written concisely and tersely.  They  transform data in ways that readily adapt to  increasingly parallel contemporary chip architectures, and the increasingly Cyclopean infrastructures of cloud computing. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, is not speed but transformation. -plying makes working with data less like iteration (number, text, table, list, etc.), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the epistemic-topological movements into data. 

While the examples of vectorised computation that I have shown are relatively trivial -- re-centering a table of names around the their mean values,  adding sequences of numbers -- these vectorised operations have  both concrete and abstract implications.  The concrete implication we have just seen:  the implicit and explicit use of vectorised processes feeds directly into the scaling up of computational  work on data. Not only vectorised computation, but the various styles of programming and infrastructure that lend themselves to simultaneous transformations of large matrices or tables of data lie at the heart of not only machine learning, but in the intensive processing of data in many different places. The rendering of moving images in  computer graphics depends implicitly on vectorised  transformations of matrices of numbers. The more abstract implications of vectorisation and the forms of movement it encourages and  proliferates bring us back to the problem of drawing a line through the data distributed in the common vector space. In short, vectorising computation makes the common vector space, which we might understand as a resurgent form of the table, operationally concrete. It is no longer a formal diagram, but a machinic process that multiplies and propagates into the world along many diagonal lines.  Here we can return to the linear model that Hastie and co-authors apply to the `prostate` data. 

## 'We fit a linear model'

'We fit a linear model' write Hastie and co-authors, referring to the `prostate` data. Statistical modelling, data-mining, pattern recognition, recommendation systems, network modelling and machine learning rely very much on  the transformation called 'fitting a model.' 'Fitting a model' to data is often literally implemented, as we will see, by  transforming data in the common vector space.  Hence,  describing vectorised transformations of   data in  `R` and other computing environments (for instance, the popular `Map-Reduce` architecture invented at Google Corporation to speed up its search engine services [@Mackenzie_2011]) is not a matter of once again intoning a litany of familiar increases in  speed or efficiency of computation. Vectoral movements into data tap into and are interwoven with the transformation of data along much more diagrammatic lines.[^1] 

```{r prostate_model, echo=TRUE}
library(ElemStatLearn)
library(xtable)
data(prostate)
columns_to_standardize = c(1,2,3,4,6,8,9)
prostate_standard = as.matrix(prostate[, columns_to_standardize]) 
prostate_standard = as.data.frame(scale(prostate_standard))
prostate_standard = cbind(prostate_standard, gleason=prostate$gleason, svi = prostate$svi, train = prostate$train)
train = prostate$train ==TRUE
prostate_model = lm(lpsa~., prostate_standard[train,-10])
tab1 = xtable(summary(prostate_model), caption = 'Fitting a linear model', label='tab:prostate_linear_regression')
print(tab1)
```

From the epistopological viewpoint, the most obvious result of fitting a linear model is the production not of a line on a diagram or in a graphic. As we have seen, such lines cannot be easily rendered visible. Instead, the model generates a new table of numbers (see Table \ref{tab:prostate_linear_regression}) called 'coefficients' and some new numbers, _statistics_.  This table is not as extensive as the original data, the $\mathbf{X}$ and $Y$ vectors. But the names of the variables in the dataset appear as rows in the new table, a table that describes something of how a line has been fitted by the linear model to the data. The columns of the table now bear abbreviated and much more statistical names such as `estimate` (the estimated values of $\hat{\beta}$, the key parameters in any linear model), `Std. Error`, `t value`, and the all important _p_ values written as `Pr(|t|)`.  The numerical values ranging along the rows mostly range from -1 to 1, but the final column includes values that are incredibly small: `1.47e-06` is a few millionths. Other statistics range around the outside the table: the `F-statistic`, the `R-squared` statistic, and the `Residual standard error`. I don't propose discussing these in any great detail here.[^101] No matter how we understand these statistics, they constitute a transformation of the `prostate` dataset. The numbers of the \ref{tab:prostate} become epistopic here, since they now appear as a set of standard errors, estimates, t-statistics, and _p_ values, that together indicate how likely the estimated values of $\beta$ are, and therefore how well the diagonal line expresses the relations between different dimensions of the dataset in the common vector space.

How have these statistics, which all act as qualifications and qualifications on the line whose direction and point of entry is given the $\hat{\beta}$ coefficents, arisen? We seem to have crossed some threshold between drawing a line through an occluded data volume and generating propositions about the way that line passes through the data. The epistopic character of Table \ref{tab:prostate_linear_regression} pivots around the line, a line we cannot see, but whose coefficients, the variables $\hat{\beta}$ are now given to us a column or more strictly speaking as a _vector_ of nine elements. The point is that this vector is a product of operations in the common vector space. We have seen the equation for those operations already in equation \ref{eq:linear_model_solution}. As Hastie and co-authors write, the 'unique solution' to the problem of fitting a linear model to a given dataset using the most popular method of 'least squares' [@Hastie_2009, 12] is given by that equation. 

This is quite a tightly coiled expression of how to calculate the parameters $\hat{\beta}$ for a linear model. The two matrices involved are the $X$ input variables (all of the `prostate` variables apart from the variable chosen as the response variable, in this case `lpsa`, the log of the PSA level) and a 1-dimensional matrix $y$, the `lpsa` values. These matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix). Calculating $\hat{\beta}$ for  the `prostate` data only requires one line of `R` code:

>`beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y`.

Again, the implicit vectorisation of the `R` code, the fact that it already concretely operates in the common vector space, forms the basis of the concise diagrammatizing of Equation \ref{eq:linear_model_solution} as a machine process. More importantly, the vector of values that result from the vectorised multiplication, transposition and inversion of matrices creates the new vector defined by $\hat{\beta}$ whose estimated values will be subjected to the statistical tests of significance, variation, and error that we see in Table \ref{tab:prostate_linear_regression}{tab:prostate_linear_regression}. We will have occasion to return to these estimates, since the play of values that starts to appear even in just fitting one model will become much more significant when fitting hundreds or thousands of models, as some machine learning techniques. This is an important differentiation: it is not typical machine learning practice to construct one model, characterised by a single set of statistics (F scores, R^2 scores, _t_ values, etc.). In practice, most machine learning techniques construct many models, and the efficacy of some predictive techniques derives often from the multiplication or indeed proliferation of models. Techniques such as neural networks, cross-validation, bagging, shrinkage and subset selection, and random forests, to name a few, generate many statistics, and navigating the multiple or highly variable models that result becomes a major concern.[^300] An epistopic abundance will appear here -- bias, variance, precision, recall, training error, test error, expectation, Bayesian Information Criteria, etc. as well as graphisms such as ROC (Receiver-Operator-Characteristics) curves.

[^300]: These meta-modelling concerns return in force in the following chapters. Put simply, the models start to drive the dimensional expansion of the common vector space. At the same time, the models multiplied by the machine learning techniques mean that the models themselves in their multiplicity become the topic of statistical analysis. 

In  the linear regression as shown by equations \ref{eq:linear_model} and  \ref{eq:linear_model_solution}, something important happened in the common vector space. A diagonal line was drawn in a volume whose outlines, form, shape or density could not in principle be seen, could not emerge into visibility, except in the diagrammatic processes of the models. Every   machine learning technique introduces lines, surface, partitions and curves into the common vector space. Irregular volumes produced by data flatten or smooth onto a pre-given forms, forms whose shape and variations can be controlled by higher level operations in the space. Sometimes these operations flatten the vector space down into lower dimensions, sometimes they expand the vector space into a great many new dimensions (as we saw with 'dummy variables' that embody categories, and as we will see with support vector machine classifiers in a later chapter). 

## The epistopological mode of movement

At one level, this chapter has suggested we see in machine learning some indications of a broad, powerful yet quite subtle shift in forms, and in the tabular form in particular, of knowledge. If Foucault is right, this is not the first such transformation in the table. But perhaps the creation of the expansive, common vector space is the first time a quasi-tabular form that cannot be surfaced on a page or screen, only symbolised or diagrammed using matrices, has become the centre of so much practical attention. This topological transformation of the table pulls and re-aligns communication and infrastructures, and it it certainly acts as a powerful tensor on scientific knowledges and technical operations of many different kinds. 

In following what happens to vectors, lists, matrices, arrays, dictionaries, sets, dataframes, or series or tuples in data, we might get a sense of how the predictive models, the supervised and unsupervised learners, the classifiers, the decision trees and the neural networks in machine learning, to name a few techniques, take shape as transformations that fold and refold matrices and vectors in a common vector space. The praxiographic challenge lies in attending to this fluctuating vector space in ways that maintain and indeed augment its value-relevance, its concreteness, and attachments to lives and places. We lack good intuitions of how to do that because of the ways in which data as vector space has been constructed and described. While we now often hear about algorithms, predictions and smart devices affecting our lives, the plural actuality of what they compose largely still eludes us. Often data is  represented as if it is an homogeneous mass or a continuous flow, but  it takes many different shapes and has many different _densities_.[^104]  By thinking of data density, we perhaps also gain a better sense of the heft and weave of data. Data  spaces out in many different density shapes, depending on how the data has been generated or instanced. Whatever the starting point (a measuring instrument, people clicking and typing on websites, a device like a camera, a random number generator, etc.), it is inevitable that later transformations will remap  it  to different shapes and forms. A given machine learning algorithm, data visualisation or database query will need the data to be in specific vectoral shapes (vectors,  matrices, arrays, etc), and fit within a certain volume or scale.    The process of composing data for statistical, visual, predictive or even storage purposes, maps a concrete situation, some state of affairs, onto forms imbued with various geometrical, probabilistic, decisionist abstractions often expressed in terms of functions or mathematical models. If, as I have been proposing, we seek contact with the practices, or the concrete value-situation, then the reshaping and reflowing of densities matters greatly.   This forming and reforming of data is evidence of  implicated relations. These practices attest to what Whitehead called 'strain': 'a' feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms' [@Whitehead_1960, 310]. In many machine learning models, for instance, the exemplified forms are straight or flat loci (see the discussion of line fitting, hyperplanes, decision boundaries in the next three chapters).  Yet  different practices also seek to elicit relations that strain the linear or geometrical shaping of data, that show how it does not fit.  Some  practices working in the name of linear forms effectively trace strain feelings. These feelings are the affective connectors between the abstractions and the concrete-value situations in which data arises. They are vital to the psychic life of data, and the ongoing reality-multiples it figures. 

The critical question is whether by unwinding some of these operations, for instance, by seeing how  matrix multiplication ripples through different treatments of data in a linear regression model (even such an elementary one), we get closer to the vitality of the multiple/multiplying concrete value-situations that connect calculation and feeling.  My suggestion is that the praxiographic description of how machine learning techniques vectorise and multiply data densities  as abstractions provides a fairly direct way begin to name and unravel the processes of knowing, predicting and deciding on which many aspects of the turn to data rely.  The following three chapters enlarge on this general point about multiplication in different ways. As we will see, the matrix operations we have just been viewing are themselves organised by other layers of intuition that explore shape, movement and surfaces in much more convoluted forms.  (Without mentioning it, the same multiplication operation, the so-called _dot product_ lay at the core of the perceptron algorithm code I quoted from Wikipedia earlier.)  Experience in so many settings -- media, consumption, science, security, etc -- is embedded in the hyperspaces of matrix transformation, and matrix multiplications. 

[^104]:   I loosely borrow the term 'density' from statistics, where *probability density functions* are often used to describe the hardly ever uniform distribution of probabilities of different values of a variable. Sensing density as a form of variation matters greatly both in machine learning itself, where algorithms seek purchase on unevenly distributed data, and in any broader praxiography of data. Probability densities are discussed in much more detail in Chapter 4 and 5.

More broadly, I have been suggesting that the way in which machine learning does into data can be understood as an epistopological transformation. Many of the transformations that move from tables of data to predictions or inferences are epistopic. That is, they move from tables of numbers and words through graphic grids to statements that are epistemic in character. This movement seeks to define thresholds that epistemologize patterns and regularities in the data that cannot be seen easily or can be seen only exceptionally by looking at the table, or even by looking at statistics on the values in the table. At the same, the transformations are topological by virtue of the unstable dimensionality of the tables as they move across these epistemological thresholds that divide the data from the predictions. In the construction of the common vector space in linear algebra, and then its practice in code rather than solely in the analytical space of equations, machine learning opens new volumes or dimensionalities that could not be seen, or rendered in either tables of data or graphic figures of data. The vectorisation of data is topological then because it creates functions, often in very practical code constructs, that generate new dimensionalities or new volumes that both aid and invite movement into the data. 

What is at stake in epistopological transformation? The broad stake, at the risk of over-emphasising it, is the production of new kinds of realities, and in particular the new kinds of continuums that seem to constantly devolve around machine learning. The same kind of model at work in recognising zip codes might be found in the navigation system of an autonomous vehicle. The datasets that _Elements of Statistical Learning_ ranges sides by side evince this improbable continuum.  My understanding of these stakes at this point is largely framed by Foucault's account of the zone of slippage between what is said and what is seen, between statements and visibilities. 'Between the figure and the text we must admit a whole series of crisscrossings' wrote Foucault [@Foucault_1972, 66]. For present purposes, I have been treating datasets as a kind of texts and the machine learning techniques as a series of crisscrossing that move data into various figures, moving the data into various forms of visibility and across epistemological thresholds of error measurements (training error, generalization error, etc.).

But why must figure and text, visibilities and statements relate to each other problematically at all? Why this criss-crossing between figure and text, which we see especially in the form of the equation-diagram? If a new kind of operational reality is broadly coming into view through formations such as machine learning, formations in which people and things, knowledge and power, recombine in novel forms, then understanding the distribution of elements that make up this emerging common space of decision, classification, prediction and anticipation matters vitally. In the closing pages of _The Archaeology of Knowledge_, Foucault writes:

>the positivities that I have tried to establish must not be understood as a set of determinations imposed from the outside on the thought of individuals, or inhabiting it from the inside, in advance as it were; they constitute rather the set of conditions in accordance with which a practice is exercised, in accordance with which that practices gives rise to partially or totally new statements, and in accordance with which it can be modified. These positivities are no so much limitations imposed on the initiative of subjects as the field in which that initiative is articulated [@Foucault_1972, 208-209].

Here Foucault refers to the restricted freedom that discursive practices and formations open for us. It is increasingly difficult for science, media, government and business to think and act outside data. The generalization of the epistopological transformations into data construct something like a set of conditions framing statements and for making things visible today. And yet Foucault is quite clear that amidst these 'positivities' of knowledge production, knowing the conditions, setting out the rules, and identifying the relations that support the density and complexity of practice is a pre-condition to any transformations in practice. 


[^1]:  Later chapters will discuss various ways in which the vectoral dimensionality of data or its rendering as _vector space_ scales up and scales down in machine learning. In terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  `R`. A fuller discussion of dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.

[^2]: Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. 

[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.

[^4]: I discuss the competitions and forms of machine learning subjectification in a later chapter. 

[^5]: Isabelle Stenger's  book on Whitehead offers a deeply philosophical introduction to his work [@Stengers_2002]. [TBA: English version]
 
[^101]: Some of these statistics will appear again in later chapters in situations where they play pivotal roles in the practice of machine learning. 

## References

## Extra stuff


Introducing the linear model, the authors of the textbook immediately resort to matrix notation. There is nothing particular mathematically elusive here, only a highly aggregated set of sums and multiplications. While such expressions are not easy to read with knowing what all the symbols mean, they offer a very direct opening to thinking about machine learning more generally.  This model is the 'simple but powerful linear model' (11) in which $Y$, the so-called 'response variable' is modelled by adding together  (the $\sum$ operator means adding them) a combination of the input variables or 'features' in  $X_j$ and a value of the intercept $\beta_0$.  Even if this does not make much sense without reading more, we can begin to see that the linear model, a mainstay of machine learning, concretely takes the form of vectors and matrices added and multiplied.  The subscripts $j=1$ refer to individual input variables.   Importantly, as Hastie and co-authors go on to say, 








 In fact, a major goal is to disentangle some different forms of vectorisation, or different ways of inhabiting vector space, associated with data today. But to emphasise this abstract mode of existence of data as a vector space  is not to say that the practical transformations of data in various forms of code and computing infrastructure can be ignored. Just the opposite is the case: the  abstract understanding of data as vector space generates many scaling potentials to which particular implementation in code and computing hardware respond. This concretising dynamism arising from abstraction is not new. We need only think of the way in which Marx describes the replacement of highly distributed artisanal manufacture by interconnected systems of machines in factories driven by prime-movers (Watt's steam engine above all) to see a similar process of reorganisation driven by  a numerate abstraction, in that case the cost of labour (see Chapter XV of [@Marx_1986]).  


In the machine learning literature, the composition of data is sometimes expressed quite formally. For instance, Peter Flach's _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] formalises the relation between data and shaped data density like this:

> Features, also call attributes, are defined as mappings $f_i: \mathcal{I} \rightarrow \mathcal{F}_{i}$ from the instance space $\mathcal{i}$ to the feature domain $\mathcal{F}_{i}$. We can distinguish features by their domain. [@Flach_2012, 298]

'Features' or 'attributes' often appear as  columns in a table. They are sometimes already present in the data (for instance, the house price dataset features are pre-defined), but they are sometimes constructed or engineered from the data. As the prominent machine learning Pedro Domingos writes in a recent overview of the difficulties of doing machine learning: 'feature engineering is more difficult because it is domain-specific, while learners can be largely general purpose' [@Domingos_2012, 84]. Note that Domingos uses 'domain' here to refer to a concrete situation, whereas Flach's domain refers to a set of values that can be input into a function or a machine learning algorithm. This coincidence in choice of words is actually symptomatic. People reshape data in different ways and for different purposes, sometimes in the name of a concrete situation, sometimes in view of the form of a particular mathematical function or a computational infrastructure (for instance, the amount of RAM heavily affects  modelling). At times, data is  folded together in order to contain it or reduce its dimensionality so that it fits somewhere. At other times, much work goes into expanding the dimensionality of data in order to find ways of eliciting relations that remain somewhat latent or hidden in it, that strain or contort its form in ways that are not easily seen. 

\printindex
