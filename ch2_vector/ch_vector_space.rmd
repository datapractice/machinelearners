\chapter{The vector space and its products}
\label{ch:vector}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')

library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
```

>In the form of the disciplinary distribution ... the table has the function of treating multiplicity itself, distributing it and deriving from it as many effects as possible [@Foucault_1997, 149] \index{table} \index{Foucault, Michel}

I sometimes have the feeling that machine learners  want to go everywhere in the world, that they aim to take on any problem, that they see no data as too difficult to learn on. This formulation from Matthew Kirk's _Thoughtful Machine Learning_ is typical:

> Machine learning is an amazing application of computation because it tackles problems that are straight out of science fiction. These algorithms can solve voice recognition, mapping, recommendations, and disease detection. The applications are endless, which is what makes machine learning so fascinating [@Kirk_2014, 11] \index{Kirk, Matthew}

It is harder but not impossible to find dissenting or sceptical voices.  Computer scientists such as Anand Rajaraman and David Ullman in their _Mining Massive Datasets_ voice doubts about the power of machine learning algorithms and contrast these algorithms with specialized search and retrieval techniques [@Rajaraman_2012]. Similarly, the mathematician Cathy O'Neil has trenchantly criticised the rapid extension of machine learning approaches in finance. Regardless of these reservations and criticisms, the appearance of 'endless' applications is not misleading. Machine learning has, once you start looking for it, become astonishingly generalized. The epistemic imperium is not only due to the generic mathematical or computational techniques but I will suggest from the advent of a new model of truth crossing a threshold and creating a new epistemic continuum. The continuum arises from a very specific diagrammatization of data that has already begun to appear in the NAND truth table and the perceptron in chapter \ref{ch:diagram}. On this continuum, all data is associative. In _Elements of Statistical Learning_,  plots of New Zealand fishing patterns lie next to plots of factors in South African heart disease. \index{\textit{Elements of Statistical Learning}} This seemingly diasporic variety of data as well as the variety of domains and situates the datasets index are salient components of the practical assemblages in machine learning.

```{r elemstatlearn_data_2, echo=FALSE, results = "asis"}
library(ElemStatLearn)
library(xtable)
datasets_results <-data(package='ElemStatLearn')$results[,3:4]
datasets = as.data.frame(datasets_results)
dataset_count <- nrow(datasets)
dataset_table = xtable(datasets, caption='Datasets in _Elements of Statistical Learning_', label = 'tab:datasets_elemstatlearn')
print.xtable(dataset_table, type='latex')
```

The `r dataset_count` datasets  shown in \ref{tab:datasets_elemstatlearn'} typify the diaspora of domains found in [@Hastie_2009; @Hastie_2001]. As mentioned above,  they span scientific, clinical, commercial and media fields. Note that they include many patches and pathways of everyday life -- speaking, seeing, writing and reading -- as well as specific scientific objects of knowledge such as galaxies, cancer, climate and national economies. This span of interests is not unusual for machine learners, which is notable for the unexpected conjunctions it creates.  To give a another example, the statistician Leo Breiman's 2001 article on random forests [@Breiman_2001] \index{random forest} is one of the most cited journal papers in the machine learning literature and displays a similarly diagonal line across human and non-human worlds: \index{Breiman, Leo}

>Glass,  Breast cancer,  Diabetes,  Sonar,  Vowel,  Ionosphere,  Vehicle,  Soybean,  German credit,  Image,  Ecoli,  Votes,  Liver,  Letters,  Sat-images,  Zip-code,  Waveform,  Twonorm,  Threenorm,  Ringnorm,  [@Breiman_2001, 12].

In some ways, the improbable conjunction of spam email and cancer detection in machine learning  simply continues what statistics as a field has always done: rove  across scattered fields ranging from astronomy to statecraft, from zoology to epidemiology, gleaning data as it goes (see [@Stigler_1986; @Hacking_1990] for samples of this trekking movement). \index{statistics} But in _Elements of Statistical Learning_ and the field of machine learning more generally, something more is moving through and associating these datasets. Rather than attending to any particular dataset, crossing between different data elements seems most important.  The datasets span many domains -- vowels, ozone, bone density, marketing, prostate cancer, and spam -- and their diversity  demonstrates the mobility of machine learners. There are many different ways to encounter data. The diversity of the data could be seen as almost aleatory, as if the datasets were the fruit of random derivè in the world.  Rather than random drifting, I think it more likely the field of machine learning performs something like one of its own statistical procedures in 'bagging' (bootstrap aggregating [@Hastie_2009, 300]) datasets in order to average its predictions out across different domains. \index{bagging} The repeated sampling of the world bootstraps machine learning as a field into its operational mode of generalization.\index{generalization}  

The tabular form, the practices of naming and labelling, and the sorting of different data types exemplified in these diverse datasets can tell us a lot about how machine learning organises and energises its movement through worlds.  The different shapes and composition of the datasets provide indications of the functioning of machine learners as they make knowledge or operationalise power relation. Already in table \ref{tab:datasets_elemsstatlearn} differences between datasets labelled 'training' and 'test,' appear quite often. We might also attend to differences between data called 'simulated' and other kinds of data. These datasets have visible forms of alignments or architectures that we should pay close attention to. If machine learning can be understood as a constantly evolving diagram or an abstracting multiplicity, the way it moves through data and associates different forms of data matter. This movement and connecting is guided by the shape of the data in some ways. But as we will see in this chapter, it also generates data in an increasingly extensive, heavily reinforced space, that I will term the *common vector space.* \index{common vector space}   Building on the diagrammatic account of the _Elements of Statistical Learning_ developed in the previous chapter, this chapter attends to both the regularity and the instability associated with the common vector space in machine learning.

## Mixing media, medicine, business and biology

>References to things act simultaneously as reference to (and within) activities. [@Lynch_1993, 193]

'This book is about learning from data' write Hastie, Tibshirani and Friedman on the first page of _Elements of Statistical Learning_, and they rapidly do indeed begin to iterate through some data. On the second page of the book [@Hastie_2009, 2], a table of spam email word frequencies appears (and the problem of spam classification is canonical in the machine learning literature - we return to in chapter \ref{ch:probability}). They come from the dataset `spam` [@Cranor_1998]. On the third page, a complicated data graphic appears (Figure 1.1, [@Hastie_2009, 3]. It is a scatterplot matrix of the `prostate` dataset included in the `R` package `ElemStatLearn,` the companion `R` package for the book. In a third example, a set of scanned handwritten numbers appears. These scans are images of zipcode or postcode numbers written on postal envelopes taken from the dataset `zip` [REF TBA], and they differ from both the `spam` table and `prostate` plots because they directly resemble something in the world, which, however, happens to be numbers, and is, therefore, probably already recruited into data-making and data-circulating processes.  The final example in the introduction, 'Example 4: DNA Expression Microarrays,' draws this time from biology, and particularly, high-throughput genomic biology, the kind of science that produces large amounts of data about something in the world by running many tests, or by constructing devices that generate many measurements, in this case, a DNA microarray.[^2.10] The image shown here is perhaps most striking. No numbers are shown, only a colorful heatmap with brighter colours standing for higher levels of gene expression, and darker colours for lower levels. For all its dense color, the data shown here is a sample of 100 of the approximately 7000 genes in the dataset `nci`  [@Hastie_2009, 6]. In comparison to the medical data from the `prostate` dataset with its 97 rows of 10 columns, the microarray dataset has 64 columns of data.[^2.11] Both are cancer datasets, but the `nci` dataset cannot be shown in its entirety because it refers to 60 different cell lines ranging across colon, breast, prostate, ovarian and renal cancers, as well as the thousands of different genes.\index{cancer}

The combined effect of these four example datasets deriving from network media, from medicine, from business administration and from cutting-edge life science (c.2000) is to suggest a tremendous, indeed almost spectacular miscibility, one that in principle could surprise us because there is otherwise little mixing between the places these datasets come from. In passing, we should note that the _Elements of Statistical Learning_ is not alone in this juxapositioning opening. Very similar example sets can be seen arrayed in most machine learning publications. To give just a few examples: Andrew Ng's CS229 lectures, for instance, treat spams  in Lecture 5 [@Ng_2008h]; Rachel Schutt and Cathy O'Neil's _Doing Data Science_ discusses spam in Chapter 4 [@Schutt_2013], and Peter Flach's _Machine Learning The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] introduces spam in the first few pages. Similar parallels exist around the image recognition problem (exemplified by handwritten digits), around the measurements dataset (`prostate` in the case of [@Hastie_2009], and in relation to the larger, impossible-to-see-the-patterns datasets of the cancer microarray data). How is this conformation and coherence being done? The repetition of data sets, the juxtaposition of discontinuous domains, and forms of movement construct and order continuities in the service of various forms of predictive and inferential knowledge. The miscible juxtapositions we are dealing with here produce, it seems, a _regularity_ or a continuous common space. The fact that this space displays strong tendencies to expand is worth considering carefully. 

[^2.10]: This kind of data will be the focus of a later chapter \ref{ch:genome}. Machine learning during the 1990s and 2000s was in some ways boosted heavily by the advent of genomic biology with its large, enterprise style knowledge endeavours such as the Human Genome Project.

[^2.11]: The data derives from a publication in _Nature Genetics_ [@Ross_2000] analysing gene expression in the cancer cell lines maintained as experimental models by the US National Cancer Institute.

## Truth is no longer in the table?

'Things, in their fundamental truth,' writes Foucault in _The Order of Things_ 'have now escaped from the space of the table' [@Foucault_1992, 239]. \index{table!history |(} Foucault of course was writing in these pages about the fabled emergence of life, labour and language as the anchoring vertexes of a new triangle of knowledge and power structuring the figure of the 'human' in the 19th century. The 'Classical' table as a space of order  was abandoned because it could no longer serve as the 'common place for representation and things, for empirical visibility and for the essential rules' (239). Before the emergence of the characteristic sciences of the human -- political economy, linguistics and biology -- knowledges such as natural history, the general grammars, and philosophies of wealth (such as Adam Smith's work)  had ordered empirical materials of diverse provenance in tables or grids. These tables themselves supplanted earlier much more variegated Renaissance tabulations. In those tables, an image or figure from myth might lie alongside a measurement or a count of occurrences, and this proximity was ordered by systems of analogical association that spanned what we might today, in the light of the 19th century, still want to separate.  While the history of tables as data forms reaches a long way back (see [@Marchese_2013] for a broad historical overview that reaches back to Mesopotamia), and tables still definitely abounded and indeed multiplied in the 19th and then 20th centuries, Foucault argues that the system of grids that permitted ranking, sorting and ordering in tables had between the end of the Renaissance and the early nineteenth been based on 'buried similitudes' and 'invisible analogies' [@Foucault_1992, 26]. The grid or table was a way of bringing the otherwise scattered and diverse resemblances into exhaustive enumeration where they could be subjected to analysis by counting and comparison. The table as space of order did not stand in isolation. It served a particular epistemic function. While algebra or _mathesis_ more generally applied to 'simple natures' (planets in movement, dynamics of falling bodies, etc.), table-based knowledges such as taxonomy dealt with more complex natures. Even in the tables, systems of signs -- the groupings established by the eighteenth century taxonomist Carl Linnaeus -- sought to reduce complex natures (plants, animals, etc.) to simpler forms as columns and rows in a table based on resemblances and similarities. \index{Linnaeus, Carl} \index{similarity}  Importantly, the table as space of order was a space of imagination in that one could begin to see continuities and the genesis of things (including grammar, nature and wealth) by carefully ordering and scanning the table. 'Hedged in by calculus and genesis,' Foucault suggest, 'we have the area of the _table_' [@Foucault_1992, 73]. \index{calculus} Note in passing that calculus  and calculations bound the table only in relation to 'simple natures' whose identity and difference can be understood in the form of movements, rates and change in position. This is an important limitation since it is precisely the complex natures in genesis that machine learning tries to engage with using algebra, calculus, statistics and computation.

In the nineteenth century, a different form of ordering shattered tabulation based on similarity and resemblance. Foucault figures this change as shattering:

>The space of order, which served as a _common place_ for representation and for things, for empirical visibility and for the essential rules, which united the regularities of nature and the resemblances of imagination in the grid of identities and differences, which displayed the empirical sequence of representations in a simultaneous table, and made it possible to scan step by step, in accordance with a logical sequence, the totality of nature's elements thus rendered contemporaneous with one another -- this space of order is from now on shattered: there will be things, with their own organic structures, their hidden veins, the spaces that articulates them, the time that produces them; and then representation, a purely temporal succession, in which those things address themselves (always partially) to a subjectivity [@Foucault_1992, 239-240].

Life, labour and language -- the figure of the human  -- famously replace the exhaustive, aggregative, synoptic classificatory tables of the Classical age. Tables still abound in newer orderings (almost any episode from the history of nineteenth and twentieth century statistics will confirm that; see [@Stigler_1986; @Stigler_2002]) \index{statistics!history}, but from now on there are things, organically structured, relating to a place and changing in time, and there are representations addressed to (and constitute) of a subjectivity. Double interiority takes over. Things such as a language, a species or an economic system have their own genesis, and our knowledges and indeed experience too become finite, historical, with their own dynamics and internal life. Knowledge is, for instance,  epistemologized in various domains so as to become scientific (through the use of experimental practice, certain ways of writing, etc.; see [@Shapin_???] for one account of this change), but in this change, the table itself is no longer the foundation of knowledge. It is one apparatus amongst many. \index{table!history|)}

This brief résumé of one of the main arguments in _The Order of Things_ might help us see what is diagrammed in the data tables we find in machine learning, and in particular, in the series of tables of data with which _Elements of Statistical Learning_ begins. The question here is how we make sense of these tables in terms of the broader diagram they make up. Presumably the spam, prostate, image and microarray data are not tabulated in the way Linnaeus would have put together a table of living things based on similarities and resemblances. Yet their regular juxtaposition does suggest we should be looking for alignments and resemblances between whole tables rather than particular cells or elements of a table. At the same time, the topics of these tables -- as we have seen, in the opening pages of _Elements of Statistical Learning_ they concern work, life, language and economy in various ways -- maps very readily onto the anchor points, the new empiricities of labour, life and language that took root at this time.  The paradox here is that things may no longer be hedged in by the table, their fundamental truth may have escaped from the space of the table, and yet we are confronted by many more tables. In many senses, we are hedged in by tables, and many different mechanisms animate and multiply tables around us.[^2.22] In certain respects the tables we see in the first pages of [@Hastie_2009] are much more classical in their scale and in their relatively immutability. In many settings in which machine learning operates, tables change rapidly in scale and sometimes in organisation. Regardless of any  differences between the tables of data used to demonstrate  machine learning techniques and data tables used in contemporary operational environments, the multiplication and juxtaposition of different tables today suggests that we might be seeing the advent of a non-classical common space of order for representation and things, for regularities and resemblances, for nature and imagination, or science and media. If that is so, then the question will be what kind of scanning, what kind of step by step movement, traverse them.

[^2.22]: By and large, I am not discussing networking and database infrastructures here. In other work, I have attempted to account for the multiplication of tables in databases but also in mundane practices of ordering. See [@Mackenzie_2011; @Mackenzie_2012]. My analysis there pivots mainly on a set-driven account of data derived from the work of Alain Badiou. \index{Badiou, Alain} Although I've already been using some set terminology here -- as in 'dataset' -- I'm persisting with a more geometrical and algebraic account of datasets in order to better deal with some of the graphisms common in machine learning. 

## The epistopic fault line in tables

```[r ElemStatLearn_data, echo=TRUE}
library('ElemStatLearn')
datasets = data(package='ElemStatLearn')
datasets$results[,3]
```

As we have already seen, it is relatively common in machine learning literature to juxtapose datasets from science, business, government and media, and then show how machine learners  can move between them. The `ElemStatLearn` `R` package brings, as we have seen in the previous chapter, brings with it around 20 different datasets, including the four we see in the opening examples. At this point, I'm not so much interested in gauging the rhetorical effect of this mixing of differences,  as in exploring how we might look at and speak about what happens at the edge of such tables. What is common is a fracture or fault. A fault line ramifies to a less or greater degree across all machine learning. It runs between what is said and what is shown, between statements and visibilities. As Gilles Deleuze writes in his account of Foucault, 'statements and visibilities ... grapple like fighters, force one another to do something or capture one another, and on every occasion constitute "truth"' [@Deleuze_1988, 67]. They are divided in ways that can be seen by, I propose, by following some diagrammatic transformations of data tables into predictions and inferences. \index{diagrammatic movement} Put prosaically,  the graphics and the text cannot quite hold together.  

In edging along this fault, we need to be proceed *epistopically*.  The term 'epistopic' \index{epistopic} comes from the work of the science studies scholar Mike Lynch \index{Lynch, Mike}, whose account of scientific practice is usefully focused on ordinariness\index{practice!scientific}. He proposed the 'epistopic' as a way of connecting 'familiar themes from epistemology and general methodology' [@Lynch_1993, 280] with localized practices, or with the local achievement of coherence. In other words, as the term itself suggests, an epistopic connects a general epistemic theme with a place, a 'local complex of activities' (281).  This emphasis on epistemic-placeness is very useful in several ways, especially if it can be brought to bear on the problem of what happens when the 'local complex' of a specific dataset encounters a general epistemic practice such as machine learning.  It's interest in the couplings between 'graphism' and practice should help make sense of some of the graphic forms we see in _Elements of Statistical Learning_ as forms of movement that epistemologize data, or that engender epistemic talk about data. It's concern with the connections between seemingly generic scientific-epistemological statements about  error, bias, variation, confidence, expectation and likelihood and graphically visible and down-to-earth practices such as drawing a line, labelling a point or sorting a list of values is a good antidote to the sometimes ballistic epistemological trajectories fuelled by statistical machine learning (e.g. the infamous and heavily cited comments by the former _Wired_ magazine editor, Chris Anderson, on the 'end of theory').[^2.41] 

[^2.41] We might add also approach the epistopic fault line in machine learning topologically \index{topology}. Over a decade ago, the cultural theorist Brian Massumi wrote that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@Massumi_2002, 184] \index{Massumi}. Much earlier, Gilles Deleuze had conceptualised Michel Foucault's philosophy as a topology, or 'thought of the outside' [@Deleuze_1988], as a set of movements that sought to map the diagrams that generated a 'kind of reality, a new model of truth' [@Deleuze_1988, 35]. More recently, this topological thinking has been extended and developed by Celia Lury amongst others. In 'The Becoming Topological of Culture,' Lury, Luciana Parisi and Tiziana Terranova suggests that 'a  new rationality is emerging: the moving ratio of a topological culture' [@Lury_2012, 4] {Lury, Celia}. In this new rationality, practices of ordering, modelling, networking and mapping co-constitute culture, technology and science [@Lury_2012, 5].  At the core of this new rationality, however, lies a new ordering of continuity. The 'ordering of continuity,' Lury, Parisi and Terranova propose, takes shape 'in practices of sorting, naming, numbering, comparing, listing, and calculating' (4). The phrase 'ordering of continuity' is interesting, since we don't normally think of continuities as subject to ordering. In many ways, that which is continuous bears within it its own ordering, its own immanent seriation or lamination. But in the becoming topological of culture, movement itself undergoes a transformation according to these authors. Rather than movement as something moving from place to place relatively unchanged (as in geometrical translation), movement should be understood as more like an animation, a set of shape-changing operations. These transformations, I would suggest, should be legible in the way that machine learning, almost the epitome of the processes of modelling and calculation that Lury, Parisi and Terranova point to, itself moves through the data. And indeed, the juxtaposition of spam, biomedical data, gene expression data and handwritten digits already suggests that topological equivalences, and a 'radical expansion' of comparison might be occurring. Bringing epistopics and topologies together might, I suggest, help trace, map and importantly diagram some of the movements into the data occurring today.

HERE

In the epistopic mode of diagramming, we find edges in data tables that are capable of combining or conjugating with each other, even if they are not reducible to each other.  Let us see how Hastie, Tibshirani and Friedman begin to move through the datasets. First of all, they highlight in the first three (`spam`, `prostate` and `zip`, leaving aside the cancer genes (`nci`)) what they have in common:

>The first three examples described in Chapter 1 have several components in common. For each there is a set of variables that might be denoted as inputs, which are measured or preset. These have some influence on one or more outputs. For each example the goal is to use the inputs to predict the values of the outputs. This exercise is called supervised learning. We have used the more modern language of machine learning.[@Hastie_2009, 9].

In the ensuing discussion,  they begin to dissect the tables and point to various discontinuities traversing the tables. The major discontinuity is already remarked in the common cut between 'inputs which are measured' and values that might called 'outputs.' Not all tables are cut by machine learners in this way, but this cut is epistopic since the table now shows that some things are measured and some might not be (the values of the outputs). In addition to this vertical discontinuity dividing tables, more diverse horizontal discontinuities can be seen (and Hastie et. al. go on to describe it). Some of the variables in the table are continuous, others are discrete, and indeed may be categorical or nominal (for instance, when predicting the actual digit from the handwritten digits, the predicted digit itself is a categorical variable, since the prediction must choose one of the digits 0,1,2,3,4,5,6,7,8 or 9) \index{data!categorical}.  Discrete qualitative data appears frequently in machine learning, or wherever classification problems can be found \index{classification}. (I will have much more to say about classification and machine learning classifiers in later chapters. For the moment, we need only note that prediction the class to which things belong is a major topic of interest.)

