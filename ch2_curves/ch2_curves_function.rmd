# Finding functions: lines and curves

## techniques
- logistic regression - on what?
- *k-nn* 
- decision tree
- supervised and unsupervised -- 

## examples
- housing price prediction; 
- cancer prognosis; 
- digit recognition; 
- credit scoring

## overview
- connection to matrix/vector manipulations
- machine learning as finding an approximation to the function that generated the data
    - the linear model
- can functions learn?
- can we learn functions?
	- diff  notions of function
	- translation between 2 senses of function - mathematical and algorithmic
	- finding functions through solving vs through approximation - gradient descent
- the visual forms of data -- nearly always a curve or line to find
    - Tim Ingold on lines
    - probability density functions as curves
- Deleuze and other on  functions
- gradient descent
    - cost functions and optimisation
    - partial observers

## quotes to use

The problematization of classifications, practices, things is an event.  Rabinow

science brings to light partial observers  in relation to functions within systems of reference.D& G, WiP, 129



It is the reason why ... in our direct apprehension of the world around us we find that curious habit of claiming a two-fold unity with the observed data. We are in the world and the world is in us. W, MoT, 227

'"value" is the world I use for the intrinsic reality of an event.  ... We have only to transfer to the very texture of realisation in itself that value which we recognise so readily in terms of human life' [@whitehead_science_1970, 116].  

The aboriginal data in terms of which pattern weaves itself are the shapes, of sense-objects, and of other eternal objects whose self-identity is not dependent on the flux of things SMW, 187-8


the challenge, which I deem a materialist challenge, is that whatever the mess and perplexity that may result, we should resist the temptation to pick and choose among practices Stengers, wondering, 2011, 379

I propose as a materialist motto: we never get a relevant answer if our practices have not enabled us to produce a relevant question Stengers, wondering, 373

Celebrating the exceptional character of the experimental achievemnt very effectively limits the claims made in the name of science. Stengers, wondering, 376

Taking seriously the singularity of experimental practices aslo leads us to understand the strong possibility of their destruction by the coming knowledge economy. The points is not that the scientific enterprise would lose a neutrality it never had.  â€¦ What is at risk is rather the very social fabric of scientific reliability, that is, the constitutive relation between an epxerimental achievmenet and the gathering of what can be called 'competent colleagues' Stengers, wondering, 377

## To do

- Add Valiant 1984 on learnable

## Introduction

'What I am attempting to do', writes Paul Rabinow,  'is to reflect on how it might be possible to transfigure elements of the equipment of modern method into a  form of modern meditation, and to bring the benefits and effects of that transformation to bear on enquiry' [@rabinow_anthropos_2003, 12]. This chapter also considers that possibility in a limited domain. The elements of the equipment of modern method here are the elements of statistical and machine learning, and the transformation that I'm exploring here are whether the techniques of machine learning can help us think about what is happening to data. The motivating intuition here is that what happens to and in data typifies transformations in naming, grouping, counting, classifying and deciding in many domains.  In making different somewhat things with data -- models, associations, maps, visualizations, databases, profiles, analyses, analytics --  new sensibilities and possibilities of experimentation appear. 

There is a fairly deep division in the practice of machine learning around the problem of knowing whether it works or not. While the field is almost despotically pragmatic in its concerns, albeit with some scattered pockets of highly abstract theorisation, it is troubled by the existence of two different kinds of learning: supervised and unsupervised. As my machine learning textbook writes: 

> With supervised learning there is a clear measure of success or lack thereof, that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations. Lack of success is directly measured by expected loss over the  joint distribution $Pr(X,Y)$. This can be estimated in a variety of ways including cross-validation. In the context of unsupervised learning, there is no such direct measure of success. ... This uncomfortable situation has led to heavy proliferation of proposed methods, since effectiveness is a matter of opinion and cannot be verified directly.  [@hastie_elements_2009, 486-7]

```{r value, echo=FALSE , cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

	text = paste(readLines('ch2_curves_function.rmd'), collapse='  ')
	value_count = length(gregexpr(text, pattern='value*', ignore.case =TRUE)[[1]])
	text_length = length(gregexpr(text, pattern='\\w+*', ignore.case =TRUE)[[1]])
```

I take some comfort from the uncomfortable situation of unsupervised learning. Amidst the optically dense pages of mathematical functions, plots of datasets and listing of algorithms, this frank admission that something cannot be measured and that this difficulty has led to proliferating methods seems to me quite promising ground to explore for possible transformations and changes. It is worth meditation. But for us, the difficulty is that this discomfort about learning does not encompass the entirety of the field. As the first part of the quoted text puts, 'with supervised learning there is a clear measure of success.' The almost unbounded optimism associated with machine learning (for instance as more or less unspoken foundation of  any analysis of 'big data')  runs pell mell across different techniques, but levels of predictive success vary widely with different techniques and different situations. In this chapter I do not try to track this heavy proliferation of methods (it is much more the topic of the next chapter). Instead I'm about to heavily proliferate a word whose meaning is somewhat unsupervisable: 'value(s).' `r value_count` of `r text_length` words in this chapter will consist of some form of the word value.  Instead of looking for clear measures of success, I will suggest that we might attempt a meditative or quasi-philosophical transformation of  machine learning along the lines proposed by A.N. Whitehead: '"value" is the world I use for the intrinsic reality of an event.  ... We have only to transfer to the very texture of realisation in itself that value which we recognise so readily in terms of human life' [@whitehead_science_1970, 116].  The texture of realisation for us today partially and unevenly composed of the processing of data. But we have yet to countenance the value of that realisation in its own terms. What is the intrinsic reality of the event of machine learning?

## The scientific function and its values

We know that machine learning  treats data as examples from which something might be learned.  Learning in machine learning means finding a function that can identify or predict patterns in the data. As _Elements of Statistical Learning_ puts it, ' our goal is to find a useful approximation $\hat{f}(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@hastie_elements_2009, 28].  Or as a leading theorist of learning theory Vladimir Vapnik puts it, 'learning is a problem of _function estimation_ on the basis of empirical data' [@vapnik_nature_1999, 291]. (Vapnik is said to have invented the support vector machine, probably the most heavily used machine learning technique of recent years.) The use of the term 'learning' in machine learning displays affiliations to the field of artificial intelligence, but the  attempt to find a 'useful approximation' -- the 'function-fitting paradigm' as [@hastie_elements_2009, 29] terms it -- stems mainly from statistics.  Not all accounts of machine learning frame the techniques in terms of function fitting. Some retain a much more explicit commitment to the notion of intelligent machines (see for example, [@alpaydin_introduction_2010, xxxvi] who writes: 'we do not need to come up with new algorithms if machines can learn themselves'). Despite any differences in the  framing of the techniques, all accounts of machine learning, even those such as _Machine Learning for Hackers_ that eschew any explicit recourse to mathematical formula,  depend on the  formalism and modes of thought associated with mathematical functions. Whether they are seen as forms of artificial intelligence or statistical models, the formalisms are directed to build 'a good and useful approximation to the desired output' [@alpaydin_introduction_2010, 41], or 'to use the sample to find the function from the set of admissable functions that minimizes the probability of error' [@vapnik_nature_1999, 31]. The linear  regression model that fits a line to a set of points ($\hat{Y} = X^T \hat{\beta}$) is just such a useful approximation to  the actual function that generated the data. The kind of visual pattern it identifies is really elementary: a straight line. The lengths to which machine learning is prepared to go to fit lines to situations is, as we will see, quite extraordinary.  The linear model undergoes some drastic deformations as lines stretch and fold into planes, hyperplanes,  and various curved and  fitted surfaces. 

Whether the function that 'underlies the predictive relationship' takes the form of a line, a plane, a curve, a tree, a forest, an ensemble, or a map as its models matters less than the  way in which an approximation is found or made. In making an approximation, machine learning practitioners make various decisions.  A basic decision, described in the first pages of any machine learning textbook, concerns whether the model they construct will do   supervised or unsupervised learning. Most machine learning uses supervised learning: it uses 'the presence of the outcome variable to guide the learning process' [@hastie_elements_2009, 2]. Less important yet still widely used, unsupervised learning has 'no measurements of the outcome' and seeks rather 'to describe how the data are organized and clustered' (2). (_k_-means is a widely used clustering algorithm). Whether  supervised or unsupervised, every machine learner implements some kind of model, and the choice of model cannot be formalised or algorithmically decided. It is impossible in principle to known whether a decision tree, a random forest, a neural network, a support vector machine or least angle regression model will work best. In practice people have favourite models, and have varying degrees of experience in implementing those models. In pattern recognition problems (for instance, in learning to recognise faces), neural networks are popular because they can be tuned to handle the many variations in patterns associated with images. In biomedical research, logistic regression and decision trees are widely used because they are easier to interpret.  While there is some agreement in the academic machine learning and data-mining communities based on the outcome of annual competitions between algorithms on difficult problems (for instance, the Association of Computing Machinery SIGKDD - Special Interest Group Knowledge Data Discovery runs the 'KDD Cup' annually [@kdd_call_2013]; Chapter 4 discusses machine learning competitions in some detail), new techniques or combinations of techniques constantly change the rule of thumb for model selection. 

A couple of critical questions present themselves when it comes to the practices of function finding and pattern recognition in machine learning. The first of these is fundamental: how can a function  learn? The philosopher of science Isabelle Stengers writes:

 > No function can deal with learning, producing, or empowering new habits, as all require and achieve the production of different worvlds, non-consensual worlds, actively diverging worlds [@stengers_deleuze_2005, 162]

I think this might not be quite right, or at least, it might  strictly limit our relation to functions.  In some ways, it is a fairly conventional position to take on mathematical functions.  They cannot learn or produce anything, only reproduce patterns that we already recognise.  Similar statements might be found in many philosophical writings on science. But unlike many philosophers of science,  in her writing Stengers explicitly affirms the achievements of experimental practice in order to defend science against being engulfed by the demands of the knowledge economy. She limits the claims made about science by seeking to highlight the specific power of science: 'celebrating the exceptional character of the experimental achievement very effectively limits the claims made in the name of science' [@stengers_wondering_2011, 376]. Limiting claims made for science might save  it from being totally re-purposed as a techno-economic innovation system. 

The problem here is that connection between a function and a given concrete experimental situation is highly contingent. Stengers argues that the ways in which mathematical functions impinge on matters of fact depends on a reference between a function and matter of fact constructed experimentally.   On this point, Stengers is justifiably adamant: 

>The reference of a mathematical function to an experimental matter of fact is neither some kind of right belonging to scientific reason nor is it an enigma, but actually the very meaning of an experimental achievement [@stengers_deleuze_2005, 157].

The generic term 'reference' here harbours a multitude of relations. The experiment achievement, the distinctive power of science, works through a tissue of relations that connect people, things, facts and mathematical functions in a highly heterogeneous weave. (This point has often been made in the social studies of science; see[@TBA]). When a biomedical experiment uses the  statistical procedure of _logistic regression_ to  'estimate the probability that a critically ill lups patient will not survive the first 72 hours of an initial emergency hospital visit' [@malley_statistical_2011, 5],  they are doing machine learning, and the value of their predictions is not captured by classical statistical approaches (analysis of variance, correlations, regression analysis, etc). As machine learning techniques and the underpinning mathematics of probabilistic learning theory circulate more widely across different scientific disciplines (geography, ecology, astronomy, epidemiology, genomics, chemistry, communication engineering), in each setting the experimental achievement consists in constructing references between the mathematical functions and the matters of fact generated by instruments, observations and measurements and cantilevered by previous experiments.  The question for Stengers is whether structure of referrals through experiments and the accumulated knowledge will be maintained if functions are said to learn. 

From  this perspective, claiming that mathematical functions can learn, or that algorithmic implementations of functions can learn (as in machine learning) might seem  tantamount to putting mathematics solely in the hands of the people who make predictions for finance, for customer relations management or for surveillance purposes, or in other words, those who have the technical capacity to collect large amounts of data and to build models based on it.  But from the standpoint of experimental practice, the problem here is not so much who uses the techniques, but whether or not the systems of reference that connects the function to a state of affairs is experimental. In many of the cases I've just mentioned, the system of reference is not very experimental. The learning, if there is any, is much more focused on re-making worlds such that they can be described, approximated or predicted by models.  When machine learning techniques become functions that classify who should have a bank account and who shouldn't, who should be offered a cheap deal on their next purchase and who shouldn't, or who should be allowed into the country and who shouldn't, the margins of experiment are tightly limited, tend to exclude the possibility of surprising results or objections from other interested parties. Much of what I discuss in this chapter will be seeking to establish a different relation to functions, a relation in which it becomes harder to say who or what is learning. That would be useful in that it neither says that functions learn or that humans learn, but together some learning might occur. It might also be useful in keeping open some space in which surprising realtions to the machine learning models remains possible. At least as I see it, the possibility of relating to  the functions differently is indispensable if we want the models and their mathematical functions to generate something we want to learn about. 

## Learning functions: 'read it, cover it, and do it yourself'

The second critical question is related to the first, and seemingly easier: how can we learn functions? There are two main senses of function in machine learning, and I have already been using both of them implicitly. The first mathematical sense refers to a relation between sets of values or variables. (A variable is a symbol that can stand for a set of numbers or other values.) A function is one-to-one relation between two sets of values. It maps a set of arguments (inputs) to a set of values (outputs, or to use slightly more technical language, it maps between a _domain_ and a _co-domain_. As we have already seen, mathematical functions are often written in formulae of varying degress of complexity. They are of various genres, provenances, textures and shapes: polynomial functions, trigonometric functions, exponential functions, differential equations, series functions, algebraic or topological functions, etc. Various fields of mathematics have pursued the invention of functions. In machine learning and information retrieval, important functions would include the logistic function (discussed below), probability density functions (PDF) for different probability distributions (Gaussian, Bernoulli, Binomial, Beta, Gamma; I will discuss these in greater depth in Chapter 5-6), cost functions, Langrangian functions, etc. Not all functions take numbers as inputs or outputs. Letters, words or almost any other symbol can be values in a function. 

While functions are often written in formulae, they can be written in different formulae and expressed in different graphic or sometimes geometric forms. Take the example of the logistic (or sigmoid) function. It can be written as:

$$f(x) = 1/(1+e^{-x})$$

It can be graphed as:

```{r logistic, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup'}
	
	x = seq(-100, 100, 0.01)
	y = 1/(1+exp(-x))
	plot(x,y)

```
This curviness of this function, as we will see, is very important in many classification and decision settings. How does it get into these settings? The second sense of function comes from programming and computer science. A function there is a part of the code of a program that performs some operation. The three lines of R code written to produce the plot of the logistic function are almost too trivial, but they show something of the transformations that occur when mathematical functions are operationalised in algorithmic form. The function is wrapped in a set of references. First, the domain of $x$ values is made much more specific. The formulaic expression $f(x) = 1/(1+e^{-x})$ says nothing explicitly about the $x$ values. They are implicitly real numbers (that is, $x \in \mathbb{R}$) in this formula but in the algorithmic expression of the function they become a sequence of `r length(x)` generated by the code. Second, the function itself is flattened into a single line of characters in code, whereas the typographically the mathematical formula had spanned 2-3 lines. Third, a key component of the function $e^-x$ itself refers to Euler's number $e$, which is perhaps the number most widely used in contemporary sciences due to its connection to patterns of growth and decay (as in the exponential function $e^x$ where $e = 2.718282$ approximately).  This number, because it is 'irrational,' has to be computed approximately in the algorithmic implementation. Finally, the plot of the function invokes a whole set of spatial and graphic conventions. For instance, it shows the $x$ values along aa horizontal axis, with negative values on the left and positive values on the right, and the $y$ values on a vertical axis at right angles to the $x$ axis, etc. These transformation between the formula expression of the function, the algorithmic and the graphic form are very mundane, mostly taken for granted in contemporary data practice. But in certain cases, they become much problematic and unstable. 

This description of the differences between functions in a mathematical sense as a mapping and functions in an algorithmic sense as an implementation of some repeated operations that might express a mathematical function is meant to highlight a key issue in learning functions. As we move from the mathematical formula to the three lines of R code that produces a plot of the function what has happened?  This is a kind of implementation of a function, and perhaps we learn something about the logistic function, that for instance, that  the $y$ values change  decisively between $0$ and $1$ across a very brief interval of $x$ values. Unlike the linear functions we saw in the house-price models (see previous chapter), logistic functions approximate a switch between $1$ and $0$, or other values such as  `yes` and `no`.  This rapid change in value will, as we see, proves incredibly useful  in machine learning: it opens up the possibility of using continuous-value function to approximate states of affairs where differences are much more heavily marked. That is, the logistic function can be used to classify or decide. 

The  S-shaped curve of the logistic function has quite a long history in statistics [@TBA - stigler], but also suggests another important transformation, somewhat orthogonal to the transformation between the mathematical function as formal abstraction and algorithm as implemented abstraction. The mathematical function $f(x) = 1/(1+e^{-x})$ holds together continuously varying numbers (the $x$ values) and discontinuous values: because $f(x)$ tends very quickly to converge on values of $1$ or $0$, it can be code as 'yes'/'no'; 'survived/deceased', or any other binary difference. The transformation between the $x$ values sliding continuously and the binary difference pivots on the combination of the  exponential function ($e^{-x}$), which rapidly tends towards zero as $x$ increases and rapidly tends towards $\inf$ as $x$ decreases, and the $1/(1+ ...) $, which converts high value denominators to almost zero, and low value demominators to one. This constrained path between variations in $x$ and their mapping to the value of the function $f(x)$ is mathematically elementary, but typical of the relaying of references that allows functions to intersect with and constitute matters of fact and states of affairs.  

## Logistic regression and classification

How does this take place practically? The logistic function appears frequently in machine learning literature, prominently as part of perhaps the most classical learning machine, the logistic regression model, but also as a component in other techniques such as neural networks.   Descriptions of logistic regression models appear in nearly all machine learning tutorials, textbooks and training courses. Logistic regression models are heavily used in biomedical research, where, as 'logistic regression is the default "simple" model for predicting a subject's group status' [@malley_statistical_2011, 43]. As Malley et.al. suggest, 'it can be applied after a more complex learning machine has done the heavy lifting of identifying an important set of predictors given a very large list of candidate predictors' (43).  Especially in comparison to more complicated models, logistic regression models are relatively easy to interpret because they are based on the linear model that we have been discussing already.  As Hastie et.al write: 'the logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $x$, while at the same time ensuring that they sum to one and remain in $[0,1]$' [@hastie_elements_2009, 119].  Paraphrased somewhat loosely, this says that the logistic regression model predicts what class or category a particular instance is likely to belong to.  We can see something of this predictive process from the basic mathematical expression for logistic regression in a situation where there are binary responses or two classes: 

$$Pr(G=K|X=x) = \frac{1}{1+\sum_{l=1}^{K-1}exp(\beta_l0 + \beta_l^Tx)}$$ [@hastie_elements_2009, 119]

In this expression, the linear model appears as $\beta_l0 + \beta_l^Tx$ where as usual $\beta$ refers to the parameters of the model and $x$ to  the matrix of input values. The linear model has, however, now been put inside the sigmoid function so that its output values no longer increase and decrease linearly. Instead they change exponentially between a minimum of $0$ and a maximum of $1$.  This variation on the linear model, supported by the logistic or sigmoid function allows the left hand side of the expression to move in a different register. The left hand side of the expression is statistical, and defines the probability ($Pr$) that a given response value ($G$) belongs to one of the pre-defined classes ($k = 1, ..., K-1$). In this case, there are two classes ('yes/no'), so $K=2$. Unlike linear models, that predict the $y$ values for a given set of $x$ inputs, the logistic regression model produces a probability that the instance represented by a given set of $x$ values belongs to a particular class.  These values can be treated as probabilities. When logistic regression is used for classification, values great than $0.5$ are usually read as class predictions. 

This account of logistic regression doesn't in any exhaust the subtleties and complexities of this relatively simple model. For our purposes, the important features of the model include the way that it transforms the functions of the linear model into a function that is able to perform classification by predicting probabilities of membership. Logistic regression models almost constitute a world of their own, judging by their ubiquity in the biomedical literature.  As at the time of writing , the major biomedical literature database [PubMed](http://www.ncbi.nlm.nih.gov/pubmed/?term=%22logistic+regression%22) returned over 160,000 papers since 1970 using or describing the use of logistic regression. A random sample of 100 of these illustrates the range of different situations in which the logistic regression model has been used to explore relations between different aspects of situations: 

```{r logisticregression, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
library

	library(rentrez)
	logistic_regression_results = entrez_search(db='pubmed', term='logistic regression', retmax=1000, usehistory='y')
	paper_summaries = entrez_summary(ids= sample(logistic_regression_results$ids, 100), db='pubmed', file_format='xml')
	titles = xpathSApply(paper_summaries, '//Item[@Name="Title"]', xmlValue)
	sample(titles, 10)
```
While these  (`r xpathSApply(logistic_regression_results$file, path='//eSearchResult/Count')`) results are only a small sample of even the biomedical literature and leave aside the many uses of logistic regression in others research, commercial, industrial and government settings, they do suggest how widely even a single machine learning technique can intervene in the production of matters of fact. In each of these papers, the logistic function has been used to predict or classify bodies, lives, individuals, organisms or other things in some way. In each case, a function has come to bear on some situation: `r paste(sample(titles, 5),  collapse='; ')`. The extraordinary dispersion of these settings should give us pause: homelessness, incest, wildlife, surgical infections, Uganda, Sweden, Vietnam, Wisconsin, etc, etc. In all of these settings, some variables have been selected as independent variables, and something else has been selected as the response variable to be classified: handwashing or not; homeless or not; caregiver at risk of depression or not; successful home kidney dialysis or not, blue-tongue syndrome or not; death after heart surgery or not, postmilitary adjustment to civilian life, etc. These lists unroll endlessly.   

Across them all, the matter of fact is woven together through a function that brings variables into relation through a system of references. Deleuze and Guattari make the somewhat somewhat surprising claim that  'the matter of fact [_etats des choses_] is a function: it is a complex variable that depends on a relation between at least two independent variables' [@deleuze_what_1994, 122].  The second part of the formulation is hardly surprising. Similar  claims could be found in almost mathematics or statistcs textbook. A function connects 'independent variables' (also known as 'predictors,' 'features', or 'regressors' [@wasserman_all_2003, 89])  to a 'dependent variable' (also known as the 'outcome' or 'response' variable) that is to predicted. But the first part of the formulation: 'the matter of fact is a function' is more provocative. Are they suggesting that a function makes or constructs a matter of fact? I do not think that they are advocating a social constructionist view of functions. The specification that a matter of fact is a function that 'depends on a relation between at least two independent variables' might be important  since it suggests that independent variables are multiple and that the relations are themselves highly variable. Indeed,  logistic regression models, and any of the other machine learning techniques, precisely try to estimate these relations.  Machine learning is a response to this variability and uncertainty, and as we will see soon, the ways in which it manages this variability and uncertainty lies at the heart of the techniques. But for the moment, the case of logistic regression, and in particular the way that it reshapes the lines and planes of linear regression models through the curve of the sigmoid function might help us see how a matter of fact can become a function. 


## Functions: at what cost?

The way in which we have learned the logistic function  by taking a textbook formula expression of it, implementing it in code and then plotting the function is not the way that machine learning typically learns the function that maps between the input data and the output variables (the so-called 'response variable'). It is not the way that mathematicians typically learn functions.  Much mathematical practice takes the form of finding a function that satisfies a set of constraints or limits. Typically, functions are learned by solving a problem posed in terms of equations. Mathematics textbooks are replete with demonstrations of this problem-solving activity, and learning mathematics is in large part becoming practiced in solving problems by finding a functions. Even in machine learning, some function-finding through solving systems of equations occurs.  For instance, the closed form solution of the least sum of squares problem for linear regression is given by  $\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$. As we saw in the previous chapter, this expression provides a very quick way to calculate the parameters of a linear model given a matrix of input and output values.  This formula itself is derived by solving a set of equations for the values $\hat{\beta}$, the estimated parameters of the model. In Week 2 of  CS229 'Machine Learning' lectures of 2008, Andrew Ng writes the steps needed to derive this closed form solution on the blackboard in the Stanford lecture theatre. He advises students to  learn these derivations. To learn machine learning, he advocates, he should 'read it, cover over the derivation and then do it yourself' [@stanforduniversity_lecture_2008]. The transitions between mathematically formalised functions, the lines of code, graphic plots, and the deductively derived solutions that we have just discussed are neither, as Stengers points out, evidence of scientific reason or enigmatic correspondences. The paths between these different practices are quite convoluted, and for any particular machine learning situation, they have to be negotiated and composed.

As we saw earlier, in his formulation of the 'learning problem', Vladimir Vapnik speaks of choosing a function that approximates to the data, yet  minimises the 'probability of error' [@vapnik_nature_1999, 31]. Unique 'closed form' solutions are quite unusual in machine learning. In machine learning, functions are normally learned through a process of approximation and optimisation that has little resemblance to the deductive solving of equations.  Even the apparently simplest data modelling procedure of fitting a line to a set of points is usually implemented differently in machine learning settings. This is a point where machine learning differs substantially from conventional statistical techniques. The use of approximation, optimisation and heuristic learning approaches is much more prevalent for reasons that have to do with the situations in which machine learning is put work.  For instance, describing the  application of machine learning to biomedical and clinical research, James Malley, Karen Malley and Sinisa Pajevic  contrast it to more conventional statistical approaches:

> working with statistical learning machines can push us to think about novel structures and functions in our data. This awareness is often counterintuitive, and familiar methods such as simple correlations, or slightly more evolved partial correlations, are often not sufficient to pin down these deeper connections. [@malley_statistical_2011, 5-6]

The novel structures and functions in 'our data' are  precisely the functions that machine learning technique seek to learn. As we will see, these structures and functions take various forms and shapes (lines, trees, curves, peaks, valleys, forests,  boundaries, neighbourhoods, etc.), and they can identify 'deeper connections' than the correlations we have saw in the house price or iris datasets.  

But how do we know whether a model is a good one, or that the function that a model proffers to us fits the functions in our data. (If it is not already obvious, in the world of machine learning it is simply taken as read that functions are in the world; 'Nature' has been thoroughly mathematised here.) One problem with  closed-form or analytical solutions typical of mathematical problem-solving is precisely their closed-form. To continue with key example of the closed form solution for linear regression ($\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$), we see that it estimates the parameters of the linear model by carrying out a series of operations on matrices of the data. These operations include matrix transpose, several matrix multiplications (so-called 'inner product') and matrix inversion (the process of finding a matrix that when multiplied by the input matrix yields the identity matrix, a matrix with $1$ along the diagonal, and $0$ for all other values).  When the dataset has a hundred or a thousand rows, these operations can be implemented and executed easily. But as soon as datasets become much larger, it is not easy to actually carry out these matrix operations even on fast computers. For instance, a dataset with a million rows and several dozen columns is hardly unusual today. Although linear algebra libraries are  carefully crafted and tested for speed and efficiency, there is no way that they can quickly carry out matrix multiplication (inner products) on million row datasets in a reasonable time. The closed form solution, even for the simplest possible structures in the data, begins to break down in this situation.  If, for instance, instead of working with the San Francisco house price dataset used in the previous chapter, we used much older Boston house price dataset available from the University of California Irvine machine learning data repository (http://archive.ics.uci.edu/ml/datasets/Housing), and tried to calculate the parameters of the model using the closed form solution, problems of scale arise immediately. With only 500 rows of data, and  12 variables, calculation of the parameters for a linear model to predict house prices is still workable, but with 5000 rows, my laptop starts to struggle for  minutes at a time.  With hundreds of variables and millions of rows, the closed form solution becomes increasingly unworkeable.

```{r name, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' } 

	import sklearn.datasets
	import numpy as np
	boston = sklearn.datasets.load_boston()
	boston.data.shape
	x = boston.data[:,0:10]
	y = boston.data[:,11]
	beta_hat = np.linalg.pinv(x.dot(x.transpose())).dot(x.transpose().dot(y))
```
There is another problem with the closed form approximation. The closed form solution is run once, and the model it produces is subject to no further computation, elaboration or variation. That is, the closed form solution  based on the so-called 'normal equations' delivers a fixed solution. The parameters $\hat{beta}$ define the line of best fit for the datasets on which they are based. A more typical machine learning approach  is to find a way of modelling how well the model has dealt with the data. It replaces the exactitude and precision of mathematically-deduced closed-form solutions with algorithms that search for a good solution, not 'the' solution. The combination of all the variables can be imagined as a surface whose contours include peaks and valleys. As we have seen, a linear model tries to find a line or plane or hyperplane (a higher dimensional plane) that fits this topography. Many different planes more or less fit the contours, but how do we choose the best one?  If we can't produce an exact answer to this problem, we could spend time trying out different parameters, varying some, and keeping the others the same until we find a set of parameters that seems to fit well.This is a classic machine learning scenario, where the machine is meant to learn something that programmers, engineers, scientists or statisticians cannot. How would the machine learning approach the line/plane of best fit?

In many machine learning techniques , and especially in the techniques of 'supervised learning', the search for an approximation to the function that generated the data is guided by another function called the 'cost function' (also known as the 'objective function' or the 'loss function'; both terms are somewhat evocative). There are various ways of learning functions, but cost functions are an essential component of many machine learning models. Deciding on the cost function means thinking about how the predictions relate to the values in the data set. Every cost function implicitly has some measure of the difference or distance between the prediction and the values actually measured. In supervised learning, cost functions work with the known values. Cost functions stage 'the act of fitting a model to data as an optimization problem' [@conway_machine_2012, 183]. Having defined a cost function, the learning algorithm can fit many models to the data, and use the cost function to decide which fits best. The cost functions themselves only provide a way of testing whether a given model performs better than another model in terms of changes in the parameters. So, the kind of model or type of prediction it performs does not change radically. If there is learning here, it is not some enigmatic form or a higher form of scientific reason. Just the opposite, the cost function does something more like create a place from which variations in  models can be viewed. 

A typical and widely cost function associated with regression is defined as: 

> $$J(\beta) := min_(\beta) \frac{1}{m} \sum\limits^m(h_\beta(x^{(i)} - y^{(i)}))^2$$

Again, the reading this kind of formula expression of a function is not easy. But several key terms stand out. First, the cost function $J(\beta)$ is a function of all the parameters of the model. Second, the function is defined in terms of the goal of minimizing the overall value of the expression. The $min$ describes the results of the repeated application of the function. Third, the heart of the function is a kind of average: it adds ($\sum$) all the  differences between the values of $y$ predicted by the model and the known values of $y$, and divides them by the number of values of y ($\frac{1}{m}$). This  is the so-called 'mean squared error' measure of prediction, a core measure of prediction in machine learning, and the basis of many standard statistical procedures. Mean squared error (MSE) is so taken for granted that machine learning textbooks such as _The Elements of Statistical Learning_ [@hastie_elements_2009] often rely on it without any further explanation. 

Importantly, the cost function itself does not say anything about how the values of the parameters are to be found. They could be generated randomly, or perhaps could be just a range of values  (e.g. 0 to 100). 

```{r gradient, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

	url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'
	boston_house =read.delim(url, sep='', header=FALSE)
	X = as.matrix(boston_house[, 1:13])
	Y = as.matrix(boston_house[,14])

	//normalise the data
	X_norm =cbind(X[,1], apply(X[,2:3], MARGIN=2, function(X) {(X-mean(X))/sd(X)}))
	Y_norm = (Y - mean(Y))/sd(Y)

	X_means = colMeans(X)
	X_sd = apply(X=X,MARGIN=2,FUN=sd )

	Y_mean = mean(Y)
	Y_sd = sd(Y)

	beta = matrix(0, nrow = ncol(X_norm))

	computeCost <- function(X,Y, beta) {
	    m = nrow(X)
	    h = X %*% beta
	    cost = 0.5 /m * sum( (h-Y)^2)
	    return(cost)
	}

```
 In the code fragment above, the R function `computeCost` takes as inputs the matrix of $X$ , the vector of $Y$ values (the outputs), and a vector of current values of   $\beta$, the model parameters. It predicts the $Y$ values by multiplying $X$ by $/beta$ (this is a linear model in which predictions are generated by multiplying variables by their respective parameters and then adding the result). In the cost line of code, it calculates the differences between all the predictions and known actual values, adds them, and averages them  to yield the 'cost' of these particular values of the parameters. In practice, the cost function will be calculated for each change in the model's parameters, and the calculated cost or loss will be checked to see if is lower or higher than previous values. If  lower, then the new model parameters are producing better predictions than previous values of the parameters. This is good because it means that the approximation to the function that generated the data is better.  Measures of error, or closeness and distance are crucial to machine learning. If we can learn functions non-deductively and if functions can learn functions, it is partly by virtue of the kinds of view on variation set up in the cost functions. 

## Gradient descent and the search for a partial observer

'Science brings to light partial observers  in relation to functions within systems of reference' wrote Gilles Deleuze and Feliz Guattari in their account of scientific functions [@deleuze_what_1994, 129]. I'm not sure whether Deleuze and Guattari were aware of the extensive work done on problems of mathematical optimization during the 1950-1960s, but their strong interest in the mathematics of differential calculus somewhat unexpectedly makes their account of functions highly relevant to machine learning. Many of the techniques of optimisation underlying machine learning techniques rely on differential calculus. 

How then are a stream values of the parameters generated for the cost function to be minimized? This is a key problem since lowering the MSE or any cost/loss/risk function implies a way to find better values. If the values of the model parameters were generated randomly, there would be little guarantee that any model was going to be better than the last one. Perhaps after trying enough random values, a good fit might appear. Or perhaps a good model might never appear. A more ordered approach might be to try a 'grid search' [@conway_machine_2012, 185], in which all the possible combinations of values for the model parameters are tested successively, just like a search party systematically combing some terrain for a missing person.   But with more than a few parameters, the matrix of values quickly becomes vast (100 different values for 10 parameters means calculating and comparing $100^10$ or $10^20$ values). It is impossible to move through large fields  of values without missing some important features. Both random and grid search are likely to fail because they do not actually take into account any structure in the data. If implemented, they might minimize the cost function or they might not. Hence, a third decision has to be made in any machine learning setting. Having chosen a type of model and a cost function, it is also necessary to choose an optimisation method. 

One widely used optimisation algorithm called 'gradient descent' is quite easy to grasp intuitively and it illustrates the process of searching for an approximation to the function that  produced the data. As in many formulations of machine learning techniques, the framing of the problem is finding the parameters of the model/function that best approximates to the function that generated the data. It calculates the parameters of a model in short. The algorithm can be written using calculus style notation as:

> Repeat until convergence:
>$$\theta_j := \theta_j - \alpha \frac {\partial }{\partial \theta_j}J(\theta_j)$$

This version of the algorithm is called 'batch gradient descent.' As always, in presenting these mathematical formula, I don't expect readers to read and understand them directly. Actually reading these formal expressions, and being able to follow the chain of references, and indexical signs that lead away from them in various directions is quite challenging. Many people who directly use machine learning techniques in industry and science would not often if ever need to make use of such expressions as they build models. They would mostly take them for granted, and simply execute them. My purpose here, however, is a bit different. Rather than explaining these formulations, my interest is following the threads and systems of reference that wind through them, and to identify the points of transition, friction or slippage that both allow them to work and also not be everything they claim to be. 

Given that this expression encapsulates the heart of a major optimisation technique, we might first of all be struck by its brevity. This is not an elaborate or convoluted algorithm. As the biostatisticians Malley, Mally and Pajevic observe, 'most of the [machine learning] procedures ... are (often) nearly trivial to implement' [@malley_statistical_2011, 6]. Below, I provide an implementation in a few lines of R code.  Note that this expression of the algorithm,   taken from the class notes for [Lecture 4](https://class.coursera.org/ml-003/lecture/index) of Andrew Ng's 'Machine Learning' course on Coursera, mixes an algorithmic set of operations  with function notation.  We see this in several respects:  the formulation includes the imperative 'repeat until convergence'; it also uses the so-called 'assignment operator' $:=$ rather than the equality operator $=$. The latter specifies that two values or expressions are equal, whereas the former specifies that the values on the right hand side of the expression should be assigned to the left.  Both algorithmic forms -- repeat until convergence, and assign/update values -- owe more to techniques of computation than to mathematical abstraction. In this respect more generally, by looking at implementations we begin to see how systems of references are put together. In this case, the specification for the gradient descent algorithm starts to bring us to the scene where data is actually reshaped according by the more abstract mathematical functions we have been describing (mainly using the example of the linear  regression and its classic algebraic form $y = \theta_0 + \theta_1x_1+ ... \theta_nx_n$). 

At the heart of this reshaping lies a different mathematical formalism: the partial derivative, $\frac {\partial }{\partial \theta_j}J(\theta_j)$. Like all derivatives in calculus, this expression can be interpreted as a rate; that is as the rate at which the cost function $J(\theta)$ changes with respect to the values of $\theta$ [^1].   If there is any learning in machine learning, it will take something like this form. But how could a partial derivative learn? Deleuze and Guattari again are useful on this point. They write that 'science brings to light partial observers  in relation to functions within systems of reference' [@deleuze_what_1994,129]. Again, it is not clear that they are referring to partial derivatives in particular, but the partial derivatives in gradient descent comprehensively demonstrate what they describe. The partial derivative in the gradient descent algorithm is a kind of observer moving through a function (the best approximation to the function that generated the data) within a system of reference. What is the partial derivative partially observing?  Remember that cost function $J(\theta)$ was defined as the average of the differences between predicted values and actual values squared ('mean squared error'). This function then itself has a particular curved form -- a parabola -- and this curve has a minimum value at the bottom (see the figure). 

```{r cost_function, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup'}

	x = seq(-100,100,0.5)
	y = x*x
	plot(x,y)

```

As always, the diagram or plot shows one, two or at most three dimensions of a curve that may in actual cases have hundreds or thousands of dimensions. We have to imagine that this curve is a hyperdimensional surface, but even in many dimensions, it is still parabolic and has a minimum value at the bottom of the dish. An observer that can manage to traverse this curve or hypercurve carries with it the possibility of minimising the cost function $J(\theta)$ and thereby optimising the approximation of a function that fits the data. 

How to move around on a surface in a way that tends to take you towards lower ground and into the valley? This problem hardly presents itself as a problem to us as we walk over hill and dale. But then maybe it would be more of a problem if we didn't have paths to follow and we were walking on a moonless night. Indeed the problem is perhaps more like crossing sand dunes at night than hiking in the mountains. The only sense of the way down is a feeling underfoot of which way the ground is sloping. This is the intuition that gradient descent abstractly follows in the data. On the one hand, the datasets in which gradient descent moves have no paths or roads in them. The algorithm has no gods' eye viewpoint on the data, even though it can move up and down the matrices as many times as it wants. Datasets  are in some respects highly organised spaces in that the data has been cut into dimensions, and grids by the rows and columns, but these dimensions have no particular value in themselves. We don't know which dimension is more important. On the contrary, the whole point of modelling the dataset is to give us some sense of what dimensions matter most in the states of affairs they pertain to.  But on the other hand, the parabolic curve/surface embodied in the cost function $J(\theta)$ already shapes the space traversed by the algorithm. A parabola, as mentioned above, is a kind of bowl (although this bowl might be in many dimensions). It is much easier to reach the bottom of a bowl, even a hyper-dimensional bowl, than to reach the bottom of a valley in  hilly terrain.  That is, there is little chance of  getting caught in some hollow halfway up the hillside. 

[^1]: The derivative \frac {\partial }{\partial \theta_j}J(\theta_j) is _partial_ because $\theta$ is a vector $\theta_0, \theta_1 ... \theta_j$.

```{r gradient_algorithm, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

	gradientDescentMulti <- function(X,Y, theta, alpha, iterations) {

	    m = nrow(X)
	    par(mfrow = c(1,2))
	    theta = as.matrix(theta, ncol=1)
	    theta_temp = c()
	    J = c()
	    theta_all = matrix(nrow=iterations, ncol=length(theta))

	    for (i in 1:iterations) {
	        # compute theta
	        h = X%*%theta
	        theta_temp = theta - alpha/m *sum ( apply(X, MARGIN=2, FUN = function(x) {x*(h-Y)}))

	        # update theta
	        theta = theta_temp
	        cat('theta current: ', theta, '\n')
	        cost = computeCost(X,Y, theta)
	        cat('cost: ',cost, '\n')
	        J[i] = cost
	        theta_all[i,]= theta
	    }
	    return(list(theta=theta_all,J=J))    
	}		

```

In the code snippet above, the implementation of gradient descent is quite brief. It occurs in one line: `theta_temp = theta - alpha/m *sum ( apply(X, MARGIN=2, FUN = function(x) {x*(h-Y)}))`. The brevity of this code again demonstrates the simplicity of the partial observer in many machine learning functions. refers to the direction and steepness of values on this slope.  The only structure that gradient descent takes into account is the value of the cost function, $J(\theta)$. It explores this surface by always trying to move downwards and given that the cost function is a parabolic curve, it is hard to move in the wrong direction. But the algorithm can move too quickly. It can overshoot the minimum value -- the base of the bowl   - and start climbing the other side, heading for even higher ground.  The value of the key parameter of the gradient descent algorithm itself $\alpha$  matters here. If $\alpha$ is too high, the algorithm might stride over the minimum value of $J(\theta)$. If $\alpha$ is too low, the algorithm might crawl too slowly down the gradient, and still be somewhere on the side of the parabolic curve before the algorithm ends.  When does the algorithm end?  In the simple implementation shown here, the algorithm ends after a certain number of iterations, typically several thousand times around  the gradient descent step. 

Gradient descent is a low-level algorithm in the sense that it does not predict or classify anything directly. It only minimises the value of a function, and does so according to a relatively simple strategy of always stepping in the most steeply sloping direction in the expectation of reaching the lowest point sooner or later. The effect of this search is to gradually adjust values of the linear model ($\hat{\beta}) so the differences between the actual values and the predicted values is reduced. Effectively, gradient descent starts by drawing a line/plane at random through the points, and then changes some of the parameters controlling slope and position of the line in order to reduce the difference between the predicted values and the actual values. But everything here depends on the cost function, and its regular geometry. Without that, gradient descent would very likely be wandering the dunes for ever. 

## Functions and  facts

What I have been describing about gradient descent is not restricted to linear models or the kinds of models that try to fit straight lines to data. Gradient descent is used much more widely. For instance, it is at the heart of the very important neural network models. It also can be used in classifiers such as logistic regression models.  All of these models -- linear models, logistic regression and neural networks -- are quintessential machine learners. The learning on data they perform relies on the way gradient descent navigates data by calculating partial derivatives in order to move downhill into the cost function valley. Deleuze and Guattari aptly describe this operation of learning: 'an etats des choses or â€œderivativeâ€ function depends on such a relation: an operation of depotentialization has been carried out that makes possible the comparison of distinct powers starting from which a thing or a body may well develop (integration)' [@deleuze_what_1994,122]. Two depotentializations can be seen in gradient descent. First, the algorithm sees all differences between the  distinct realities of actual and predicted in terms of a cost function. Second, the cost function itself is 'depotentialized' via the partial derivative operation $\frac {\partial }{\partial \theta_j}J(\theta_j)$. That is,  the rate of change of the cost function -- the partial derivatives -- become the guiding through for the comparisons that allow the cost function to be iteratively minimized and hence the parameters of the model to be optimised. But there is a massive missing bit here: the 'body or thing' has long been absent in this discussion. How do we get back to things or bodies from machine learning algorithms?

How do we feel about such depotentializations? Deleuze and Guattari equate 'derivative functions' and matters of fact. As Isabelle Stengers has argued, this important identification of functions and facts is largely glossed over in the English translation of 'etats des choses' as 'states of affairs' [@stengers_deleuze_2005, 153]. The literal translation -- 'states of things' -- and the normal translation -- 'matter of fact' -- would be preferable to 'states of affairs.' As we have seen above, she argues that it would be better to approach functions in terms of the system of references that render them experimentally relevant: 'functions may be defined by a reference which would be characterized as a â€˜â€˜matter of fact,â€™â€™ that is as â€˜â€˜holding togetherâ€™â€™ by themselves, and not as obtaining their definition from the states of affairs and the power dimensions that those states of affairs include' [@stengers_deleuze_2005, 153].  (These lines form part of a widing ranging debate in the social science and humanities about the best way to make sense of scientific and technical things: in this debate, the questions is whether behind every matter of scientific fact stands a state of affairs pervaded and shaped by power relations; in social constructivist accounts of science and technology, the answer to this question is normally 'yes.') For our purposes, the link Stengers makes between functions and facts is most important. Everything we have been discussed about 'the learning problem' in machine learning -- the choice of function class, deciding on a cost function, implementing a method of optimisation (gradient descent) -- still lies a long way from facts, it seems.  How could a function such as a parameterised linear model be a _matter of fact_ or as 'holding together by themselves'?

The Deleuze-Guattari account of scientific functions offers one way of understanding how machine learning creates anything. The key accomplishment of functions for them consists in a slowing down of chaos (or what we might call, following William James, â€˜pure experienceâ€™). A function is  â€˜a fantastic slowing down, and it is by slowing down that matter, as well as the scientific thought able to penetrate it with propositions, is actualized. A function is a Slow-motionâ€™ [@deleuze_what_1994, 118].  They see functions as particular way of constructing references within chaos. Here chaos is understood as formlessness that arises from speed, from the fact that forms appear and disappear rapidly. While it is hardly a commensense account of a world, we might think of it as the degree zero on knowing or thinking, the background noise or dark screen against which all form, differentiation, pattern or order eventuates.  As Stengers writes:

>Deleuze and Guattari defined the â€œcreation of scientific function by scienceâ€™s own specific meansâ€ they certainly did not agree with the old bearded-face explanation, but they nevertheless asked us to relate science as creation with scienceâ€™s â€œown specific meansâ€ [@stengers_deleuze_2005, 154].

The problem in bringing this account of scientific functions to bear on machine learning is that machine learning techniques are so heavily involved in pattern recognition. On this point, Deleuze and Guattari, and Stengers in their wake, are adamant. The specific means through scientific functions are created differs from the â€˜functions of the livedâ€™ that map onto perceptions and affections. Functions of the lived have â€˜consensual perceptions and affectionsâ€™ as their arguments [@stengers_deleuze_2005, 154], and in that respect they are complicated forms of recognition. In principle, they create nothing since they re-iterate established states of affairs. Scientific functions diverge from functions of the lived, and their implicitly consensual recognition because  they transform functions of the lived into matters of fact via experiments articulated through a mathematical function, or, as Stengers puts it, 'into a scientific matter of fact correlated to a scientific mathematical functionâ€™ (156). When Stengers suggests that a 'function cannot learn,' is this what she has in mind?  When data mining, pattern recognition, knowledge discovery, information retrieval or statistical machine learning are set to work on datasets, are they condemned pretty much to remain in the realm of 'consensual recognition', with all the limitations and lack of novelty that implies?

I'm hoping that the discussion of the different decisions involved in 'the learning problem' suggest something different. In transforming a state of affairs or a function of the lived, steps taken to address the learning problem can potentially transform perceived states of affairs in terms of a function and its variables.  But the chances of creating something here are strangely mixed. The  'consensual perceptions and affections, ' which we might think of in terms of the house price examples, as the bundles of feelings and values associated with number of bedrooms in a house, are part of existing states of affairs (the real estate market, with all its complicated dynamics). The function as a  nexus of relations allows these variables to vary, and indeed openly varies them in more or less constrained ways. The relation of variables to each other can be tested, and it can produce further matters of fact. The scientific function, then, is a kind of convention that remakes the ground on which it stands. As Stengers points out here and elsewhere in her work, the scientific function and its correlated matter of fact needs a specific kind of collective, animated by objections, in order to survive. The scientific collective raises objections to the convention and put it to the test. The scientific function is powerful to the extent that it leaves itself open to objections.  Matters of fact bifurcate functions of the lived into matters of fact (_etat de choses_) and states of affairs. They make a difference between knowledge as matters of fact and consensual perception or discourse. Conversely, scientific functions become vulnerable to takeover by functions of the lived to the extent that they are not open to objection. 


## Conclusion

In their analysis of the commons, Michael Hardt and Antonio Negri   recommend analysis of 'spectres of the commons':

	> Specters of the common appear throughout capitalist society, even if in veiled and mystified forms. Despite its ideological aversion, capital cannot do without the common, and today in increasingly explicit ways. To track down these specters of the common , we will need to follow the path of productive social cooperation and the various modes of abstraction that represent it in capitalist society [@hardt_commonwealth_2009, 153].

This somewhat adamant statement doesn't resonant entirely with the path followed in this chapter, but there are elements of its that are useful. I have sought in this chapter to describe one path of  abstraction that powerfully organises contemporary social cooperation today: the functions at work in machine learning. The core practices of learning associated with machine learning are processes of function approximation. The mapping between input and output values at work in predictive models and classifiers are all expressed as functions, whether in the forms of mathematical formula, in descriptions of algorithms or in lines of code written in some programming language like R, Python or MatLab.  As in much of this book, the  starting intuition here is that machine learning, like many other contemporary social and technical processes is a highly focused if somewhat diffractive and opaque lens on relationality or on how we live together. The processes of abstraction, generationalisation, modelling and theorisation embodied in cost functions, learning theory, gradient descent or the logistic function, although they often seem reductive, minimal, or simply vastly inadequate to the plurality of experience. Machine learning seeks to identify, classify and predict certain salient aspects of experience and relationality. Subject to machine learning,  'experience now flows as if shot through with adjectives and nouns and prepositions and conjunctions' as William James puts it [@james_essays_1996,94], according to what Willam James would term the 'proportional amount of unverbalized sensation which it still embodies.' The real estate prices dataset is a good example of this: real estate prices bear within them so many conjunctive relations that the machine learning models seek to bring to light. The proliferating variables in the Boston house price dataset are symptom of this pursuit of unverbalized relationality.  

There is a second important thread running through the discussion of machine learning understood as function approximation, and this relates to what Hardt and Negri understand as 'the commons.' The commons as a largely unverbalised entanglement of relations both highly generative and at the same time somewhat withdrawn. This is not to say that the commons is without shape or form, or that it cannot be conceptualised. On the contrary, it is full of conceptualisations and modes of abstraction. It's just that these abstractions are not always ours, our own, or subject to appropriation by us. In _Alien Phenomenology or What It's Like to Be a Thing_, Ian Bogost  describes how 'objects try to make sense of each other through the qualities and logics they possess. When one object caricatures another, the first grasps the second in abstract, enough for the one to make some sense of the other given its own internal properties'  [@bogost_alien_2012, 66]. Drawing on object oriented ontology developed by Graham Harman, Levi Bryant and Timothy Morton (amongst others; [REF TBA]), Bogost  somewhat echoes Whitehead whose account of feeling also ranges widely across objects, things, organisms, systems and thought ( for instance, Whitehead writes in _Modes of Thought_: 'The energetic activity considered in physics is the emotional intensity entertained in life' [@whitehead_modes_1958, 232]/). Philosophically speaking, machine learning techniques are interesting because they exemplify a kind of non-human sense-making of the logics and qualities that we cannot directly intuit or experient. The partial observers rappelling down  the multi-dimensional slopes in the gradient descent algorithm are somewhat observable to us. We can plot their progress via the cost functions. We only sense their progress  with great difficulties because the high dimensional spaces that move within. These spaces are abstract products, yet iteratively materialised and concretised. When I was implementing the gradient descent algorithm to predict house prices as part of Andrew Ng's machine learning class,  the process of putting together lines of code, running them on a downloaded datasets and plotting the results was not abstract. Certainly, as Whitehead would say, there was some 'conceptual experience' occurring here. The whole point of the predictive modelling is to expand one's scope for the 'entertainment of alternatives for ideal realization in the absence of any sheer physical realization' [@whitehead_modes_1958, 229]. Well, absence of sheer physical realization in the sense of going out and trying to buy a house in Boston or Portland, but not in the sense of work of implementing the algorithm in R code with all the cutting and pasting, tweaking and pushing that entails.  Physical realization was definitely part of the entertainment of alternatives in these sense that wrangling matrixes, aligning vectors and matrices, implementing the partial derivatives as functions in R, plotting the cost function to ensure that the optimization was progressing, and to see whether the parameters of the model had converged took precious time away from alternative entertainments. 

Because they have this relational capacity, what Stengers after Deleuze and Guattari calls 'â€˜â€˜holding togetherâ€™â€™ by themselves' [@stengers_deleuze_2005, 132], the functions we have been discussing do more than approximate. When in _The Elements of Statistical Learning_, Hastie et. al. speaking about finding an approximation  'to the function $f(x)$ that underlies the predictive relationship between input and output'  [@hastie_elements_2009, 28] they perhaps point to something worth thinking about. The 'predictive relationship between input and out' is not purely epistemic. It is perhaps something more epistemic in the sense that this relationship between input and output, if it is predictive, takes us into things, into the underlying function. From a classical critical standpoint, to attribute an underlying function to the data and hence to the world that data is indexed is a big mistake. It is mistaking something of ours -- the abstraction of a function -- for what really is, and it locks the world down. From another standpoint, attributing an unknown function to the world is frees the world up, if the attribution is done carefully, that is, experimentally.  Bear in mind here that the meaning of the experimental event here -- and I try to follow Stengers to the letter on this -- pivots on the act of confer  the power to speak to things. The learning theorist (whose work we will soon grapple with more concretely in the next chapter) suggests that the problem in machine learning is not to estimate the function on the basis of the data: 'mostly' he writes, 'however, we face another situation.  ... In other words, we face the problem of estimating the _values of the unknown function at given points' [@vapnik_nature_1999, 291].  I prefer this formulation of machine learning to the function approximation because it shifts the emphasis to values, to the intrinsic reality of the event, whose outlines, shapes, dimensionality and relationality we only ever encounter at 'given points' not in their entirety or plenitude.

## Extra stuff

'Correlated' here is the key term. What does it mean to correlate something a scientific mathematical function? 

The term 'correlate' is a problematic one, precisely because, as it is often argued, correlations do not tell us enough about what is happening. The presence of a correlation does not correlate with an actual relation. 


    This is why the power of such a convention changes in nature as soon as it leaves its birthplace and concerns human affairs where all protagonists are not enabled to object, where some are a priori defined as not mattering. The effectuation of the event, the meaning of the â€˜â€˜it works,â€™â€™ radically changes as soon as the tent is pitched on the settled ground of interests and power, and as soon as the habit of the public to accept being addressed as powerless, mere opinion, unable to object and propose, is an ingredient in its concept (Stengers, 2005, 162).


How do functions work? A wide variety of mathematical functions exist.  At the same time, functions exist as operational processes, as for instance, when a programmer write a function to sort records by date. This means that operate on data in the world, and introduce procedures into its many states of affairs. Both kinds of functions share some features. They both invoke numbers and symbols. They are both written in formal ways (equations, code). They both have arguments or variables. They both have various kinds of operators (+, -, =,etc.) that articulate variables , usually expressed in the form of symbols. 

The question is how we can relate these two different kinds of functions to data?  Does a mathematical or scientific function have the same mode of existence as an operation that proceduralises a state of affairs? Do the matters of facts map onto states of affairs through functions? I would suggest that although the two kind of functions are entwined, they do quite different things, and that comprehending their differences is vital to thinking through contemporary forms of data. 

According to Deleuze and Guattari in *What is Philosophy*, the function is the central inventive process in science. Functions are not demonstrated or deduced, but created (cf. Manuel Delanda on this). T

