# Machines finding functions

'Machines finding functions': there is a triple word play in the title of this chapter. From an _almost_ purely technical standpoint, machine learning can be understood as function finding, that is, finding a mathematical expression that approximates to the process that generated the data in question. The second sense of the title is that machine learning offers a plethora of techniques, and much machine learning work, at least for many practitioners, concerns not so much implementation of particular techniques (neural network, decision tree, support vector machine, logistic regression, etc.), but rather navigating the maze of methods and variations that might be relevant to a particular situation. Here we could recall that 'functions' abound in software and code, not in the mathematical sense, but in the sense of operational units of code( the `print` function in `Python` writes to the screen, for instance).   Finding the right function amidst the variety of functions affects any learning of machine learning. The third sense of the title is less technical, and concerns something more dynamically transcontextual. That is, how machine learning itself has itself come to operate as a powerful diagram for infrastructural, operational, financial, scientific, governmental and marketing processes. Here, the machines -- the classifiers and the predictive models -- have found functions as they have been inserted into infrastructures, experiments, and organisations. How do these different functions function together? What generality or positivity concatenates them? 

## Which sense of function?

The  mathematical sense of function is writ large in nearly all machine learning literature. 

>A classifier or classification rule is a function $d(\mathbf{x})$ defined on $\mathcal{X}$ so that for every $\mathbf{x}$, $d(\mathbf{x})$ is equal to one of the numbers $1, 2, ..., J$ [@Breiman_1984, 4]

Writing in the 1980s, the statistician Leo Breiman describes classifiers -- perhaps the key technical achievement of machine learning -- in terms of functions. A classifier _is_ a function. The equation of classifiers to functions is quite pervasive. The learning, the predictions, and the classifications produced by machine learning depend on functions. So we need an account of what goes on in this equation of function and classification if we are to make sense of the broader problematisation associated with machine learning. 

The identification of machine learning with functions appears in the first pages of most machine learning textbooks. Learning in machine learning means finding a function that can identify or predict patterns in the data. As _Elements of Statistical Learning_ puts it,

>our goal is to find a useful approximation $\hat{f}(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output [@Hastie_2009, 28]. 

In a highly compressed form, this statement of goals performs the triple play on function that referred to above. It contains _the_ function that generated the data as a foundation, it refers to 'finding  ... $\hat{f}(x)$', where the '^' indicates an approximation, and it avers to 'use'. Similar formulations pile up in the literature. A leading theorist of learning theory Vladimir Vapnik puts it this way: 'learning is a problem of _function estimation_ on the basis of empirical data' [@Vapnik_1999, 291]. (Vapnik is said to have invented the support vector machine, one of the most heavily used machine learning technique of recent years on the basis of his theory of computational learning.) The use of the term 'learning' in machine learning displays affiliations to the field of artificial intelligence, but the  attempt to find a 'useful approximation' -- the 'function-fitting paradigm' as [@Hastie_2009, 29] terms it -- stems mainly from statistics.  Not all accounts of machine learning frame the techniques in terms of function fitting. Some retain the language of intelligent machines (see for example, [@Alpaydin_2010, xxxvi] who writes: 'we do not need to come up with new algorithms if machines can learn themselves'). Despite any differences in the  framing of the techniques, all accounts of machine learning, even those such as _Machine Learning for Hackers_ [@Conway_2012] that eschew any explicit recourse to mathematical formula,  rely on the  formalism and modes of thought associated with mathematical functions. Whether they are seen as forms of artificial intelligence or statistical models, the formalisms are directed to build 'a good and useful approximation to the desired output' [@Alpaydin_2010, 41], or, put more statistically,  'to use the sample to find the function from the set of admissable functions that minimizes the probability of error' [@Vapnik_1999, 31].
We have seen some of this process already in the  previous chapter: the linear  regression model that fits a line to a set of points  is just such a useful approximation to  the actual function that generated the data. The kind of visual pattern it identifies is really elementary -- a straight line -- but the lengths to which machine learning is prepared to go to fit lines to situations is, as we will see, quite extraordinary.  The linear model undergoes some drastic deformations as lines stretch and fold into planes, hyperplanes,  and various curved and fitted surfaces, but it remains a function in the mathematical sense of a mapping between input or $X$ values and output or $Y$ values. 

As is often the case in working with a massive technical literature, the first problem in making sense of what is happening with function in machine learning concerns sheer abundance. The pages of [@Hastie_2009] are marked with score of references to 'functions': quadratic function, likelihood function, sigmoid function, loss function, regression function, basis function, activation function, penalty functions, additive functions, kernel functions,step function,  error function, constraint function, discriminant function, probability density function, weight function, coordinate function, neighborhood function, and the list goes on. Clearly we cannot expect to understand the functioning of all these functions in any great detail. However, even a glance through this prickly list of terms begins to suggest that not only is there quite a heavy reliance on functions in this field (as perhaps in many others), and that we need to understand the operations occurring in and around functions if we are to make sense of the statements and forms of seeing associated with the field. We can also already see in this list that the qualifiers of the term function are diverse. Sometimes, the qualifier refers to a mathematical form -- 'quadratic,' 'coordinate', 'basis' or 'kernel'; sometimes it refers to modelling or statistical considerations -- 'likelihood', 'regression', 'error,' or 'probability density'; and sometimes it refers to some other concern that might relate to a particular modelling device or diagram -- 'activation,' 'weight', 'loss,'  'constraint,' or 'discriminant.' These multiple modes of functions matter, since they permit the triple-play of function-finding in machine learning.     

Implicit too in the formal descriptions of machine learning as function finding cited above (for instance, Vapnik's or Hastie's formulation) is the core mathematical definition of a function. The primary mathematical sense refers to a relation between sets of values or variables. (A variable is a symbol that can stand for a set of numbers or other values.) A function is one-to-one relation between two sets of values. It maps a set of arguments (inputs) to a set of values (outputs, or to use slightly more technical language, it maps between a _domain_ and a _co-domain_.) As we have already seen, mathematical functions are often written in formulae of varying degrees of complexity. They are of various genres, provenances, textures and shapes: polynomial functions, trigonometric functions, exponential functions, differential equations, series functions, algebraic or topological functions, etc. Various fields of mathematics have pursued the invention of functions. In machine learning and information retrieval, important functions would include the logistic function (discussed below), probability density functions (PDF) for different probability distributions (Gaussian, Bernoulli, Binomial, Beta, Gamma, etc.;[^2]).

[^2]:  I will discuss these in greater depth in Chapter 5-6), as well as  cost functions and Langrangian functions, etc. Not all functions take numbers as inputs or outputs. Letters, words or almost any other symbol can be values in a function. 

Functions appear in machine learning in several different ways: as statements or utterances, as formula-diagrams, as graphic forms and in technical implementations as code. Any account of machine learning as function finding needs to map the concatenation of these different elements, none of which alone can anchor the 'learning' that goes on in machine learning. 

While functions are often diagrammatically written in formulae, they can often appear in different graphic or operational forms. Many of the functions listed above concern curves, and different ways of generating, moving along, finding or differentiating curves. The historical invention of the term 'function' by G.W. Leibniz in the 17th century relates to curves and their description. Functions for Leibniz describe variations in curves such as their slope, and these variations in curves still underpin key aspects of the function-finding done in machine learning. In contrast to the table, or even the generalized common vector space that encompasses, the curve and the many graphic genres that seek to show curves in different ways, relate to variations.

If for instance, we look through the several hundred colour graphic plots in [@Hastie_2009][^3], a striking mixture of network diagrams, scatterplots, barcharts, histograms, heatmaps, boxplots, maps, contour plots, dendrograms and 3D plots appear there. Many of these graphic forms are common in statistics (histograms and boxplots), but some relate specifically to data mining and statistical learning (for instance, ROC -- Receiver Operating Curve -- plots or curves, lines, and common statistical graphic forms appears. Only a small proportion of these graphics show data. Nearly all of them either show the results of some modelling technique, contrast the operations of different models, or diagram something of the way that models relate to data. Viewed in terms of their visual composition, the diagrams in the book are dominated not by perspective, or by line (although the framing device of the X-Y axes is ubiquitous), but by curves. Curving lines outnumber all other graphic forms, whether the curves are the 'decision boundaries,' the plots of 'training errors,' the 'contours of constant density' [@Hastie_2009, 109], or the 'regularization path' for the South African heart data [@Hastie_2009, 126]. There is a tension in these graphic forms. Consonant with the vectoral ideal of a straight line that either connects or cuts the data, many of the graphics in the book (and others like it, especially the books written by computer scientists) show straight lines. But these straight lines are deflected and bent in various directions by variations and uncertainties of various kinds. The curves that proliferate in the graphic devices of  machine learning often try to the redress the rigidity of the lines. Viewed very naively, the contrast between lines and curves in the graphic diagrams of a book  such as _Elements of Statistical Learning_ suggests that different forces are at work around the data, sometimes aligning and straightening relations (for instance, the many linear models) and sometimes tracing much more non-linear paths through the data (for instance, as in convolutional neural networks). Viewed less naively, the curves and their functions operate as the critical paths that connect data to worlds.  

[^3]: A montage of all the graphics in _Elements of Statistical Learning_ can be found at [TBC]

## Diagramming functions with curves

How do curves get into the data? One way they approach the data is through fitting.  Take the example of _sigmoid_ functions. These quite simple functions underpin many classifiers and animate many of the operations of neural network, including their recent re-incarnations in 'deep learning' [@Hinton_2006; @Mohamed_2011].  A well-known example of a sigmoid function, the logistic function, can be written as:

$$f(x) = 1/(1+e^{-kx})$$

And can be graphed as:

```{r logistic, echo=TRUE, fig.cap = 'Logistic curves', fig.lp = 'Figure', cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup'}
	
	x = seq(-100, 100, 0.01)
	y = 1/(1+exp(-0.100*(x-12)))
	y2 = 1/(1+exp(-1.0*(x-12)))
	plot(x,y)
    lines(x, y2)
```

This function, as we will see, is very important in many classification and decision settings precisely because of its _non_linear_ shape.  How does a function such as the sigmoid function do anything? Here the curve itself and even the name 'sigmoid' is the best guide. The S-shape of the sigmoid curve guides the operation of many of the techniques and is a good example of the polyvalence of curves. The logistic function has quite a long history in statistics [@Stigler_1986], since that curve suggests growth and change in various ways. As the historian of statistics J.S. Cramer writes:

>The logistic function was invented in the nineteenth century for the description of the growth of organisms and populations and for the course of autocatalytic chemical reactions [@Cramer_2004, 614].

The Belgian mathematician Pierre-FranÃ§ois Verhulst designated the sigmoid function the 'logistic curve' in the 1830-40s [@Cramer_2004, 616]. It was independently designated the 'autocatalytic function' by the German chemist Wilhelm Ostwald in the 1880s, and then re-invented under various names by biologists, physiologists and demographers during 1900-1930s (617). In nearly all of these cases, the function was used to fit a curve to data on the growth of something: populations, tumours, tadpoles tails, oats and embryos. The term 'logistic' returns to visibility in the 1920s, and has continued in use as a way of describing the growth of something that reaches a limit. The reference of the curve to growth comes from its changing slope. Growth starts slowly, increases rapidly and then slows down again. This pattern can be seen in organisms, in chemical reactions and in populations. Hence the logistic function has a well-established biopolitical resonance. In the second half of the twentieth century, it was widely used in economics. In all these settings and usages, the curve was a way of summarising and predicting growth. Census data, clinical or laboratory measurements supplied the actual values of $f(x)$ at particular times, the $x$ values. The task of the demographer, physiologist or economist was to work out the values of parameters such as $k$ that controlled the shape of the curve. 


But the sharp curve of the logistic function when the scaling parameter $k$ is larger suggests another important transformation, somewhat orthogonal to the description of rates of growth under limits.  The mathematical function $f(x) = 1/(1+e^{-x})$ can be treated as a way of mapping continuously varying numbers (the $x$ values) and discrete values. Because $f(x)$ tends very quickly to converge on values of $1$ or $0$, it can be coded as 'yes'/'no'; 'survived/deceased', or any other binary difference. The transformation between the $x$ values sliding continuously and the binary difference pivots on the combination of the  exponential function ($e^{-x}$), which rapidly tends towards zero as $x$ increases and rapidly tends towards $\inf$ as $x$ decreases, and the $1/(1+ ...) $, which converts high value denominators to almost zero, and low value demominators to one. This constrained path between variations in $x$ and their mapping to the value of the function $f(x)$ is mathematically elementary, but typical of the relaying of references that allows functions to intersect with and constitute matters of fact and states of affairs. This realisation -- that a continuously varying sigmoid function could also map discrete outcomes -- forms the basis of many  machine learning classifiers. So, a contemporary biostatistical machine learning textbook can write: we can use logistic regression to  'estimate the probability that a critically-ill lupus patient will not survive the first 72 hours of an initial emergency hospital visit' [@Malley_2011, 5]. If the same curve -- the logistic curve -- can describe quite different situations (the growth of a population, the probability that someone will die), then we can begin to see that sigmoid functions, and the logistic curve in particular, might be useful devices as approximations to the 'underlying function that generated the data.'  

## The curve as classifier

> The logistic regression model arises from the desire to model the posterior probabilities of the _K_ classes via linear functions in $x$, while at the same time ensuring that they sum to one and remain in $[0,1]$ [@Hastie_2009, 119]

[HERE] - add in stuff about the curve, about how functions came about as a way of relating to curves and lines, and then how many curves appear in Hastie, etc. Look at all the graphics here --- choose a couple

[[want to get to the point where I can say the power of the machine learning to learn, and hence appetite for data, for infrastructural transformations, etc, can all be understood in terms of this form of movement that pulls lines around the curve -- a kind of constraint; this is an antidote to the mythologised power of algorithms]

As soon as we move from the more theoretical or expository literature into the domain of practice, instruction and learning of machine learning, a second sense of function comes to the fore. The second sense of function comes from programming and computer science. A function there is a part of the code of a program that performs some operation. The three lines of R code written to produce the plot of the logistic function are almost too trivial, but they show something of the transformations that occur when mathematical functions are operationalised in algorithmic form. The function is wrapped in a set of references. First, the domain of $x$ values is made much more specific. The formulaic expression $f(x) = 1/(1+e^{-x})$ says nothing explicitly about the $x$ values. They are implicitly real numbers (that is, $x \in \mathbb{R}$) in this formula but in the algorithmic expression of the function they become a sequence of `r length(x)` generated by the code. Second, the function itself is flattened into a single line of characters in code, whereas the typographically the mathematical formula had spanned 2-3 lines. Third, a key component of the function $e^-x$ itself refers to Euler's number $e$, which is perhaps the number most widely used in contemporary sciences due to its connection to patterns of growth and decay (as in the exponential function $e^x$ where $e = 2.718282$ approximately).  This number, because it is 'irrational,' has to be computed approximately in the algorithmic implementation. Finally, the plot of the function invokes a whole set of spatial and graphic conventions. For instance, it shows the $x$ values along aa horizontal axis, with negative values on the left and positive values on the right, and the $y$ values on a vertical axis at right angles to the $x$ axis, etc. These transformation between the formula expression of the function, the algorithmic and the graphic form are very mundane, mostly taken for granted in contemporary data practice. But in certain cases, they become much problematic and unstable. 

This description of the differences between functions in a mathematical sense as a mapping and functions in an algorithmic sense as an implementation of some repeated operations that might express a mathematical function is meant to highlight a key issue in learning functions. As we move from the mathematical formula to the three lines of R code that produces a plot of the function what has happened?  This is a kind of implementation of a function, and perhaps we learn something about the logistic function, that for instance, that  the $y$ values change  decisively between $0$ and $1$ across a very brief interval of $x$ values. Unlike the linear functions we saw in the house-price models (see previous chapter), logistic functions approximate a switch between $1$ and $0$, or other values such as  `yes` and `no`.  This rapid change in value will, as we see, proves incredibly useful  in machine learning: it opens up the possibility of using continuous-value function to approximate states of affairs where differences are much more heavily marked. That is, the logistic function can be used to classify or decide. 

The  S-shaped curve of the logistic function has quite a long history in statistics [tBA - stigler], but also suggests another important transformation, somewhat orthogonal to the transformation between the mathematical function as formal abstraction and algorithm as implemented abstraction. The mathematical function $f(x) = 1/(1+e^{-x})$ holds together continuously varying numbers (the $x$ values) and discontinuous values: because $f(x)$ tends very quickly to converge on values of $1$ or $0$, it can be code as 'yes'/'no'; 'survived/deceased', or any other binary difference. The transformation between the $x$ values sliding continuously and the binary difference pivots on the combination of the  exponential function ($e^{-x}$), which rapidly tends towards zero as $x$ increases and rapidly tends towards $\inf$ as $x$ decreases, and the $1/(1+ ...) $, which converts high value denominators to almost zero, and low value demominators to one. This constrained path between variations in $x$ and their mapping to the value of the function $f(x)$ is mathematically elementary, but typical of the relaying of references that allows functions to intersect with and constitute matters of fact and states of affairs.  



There is deep disturbance in the practice of machine learning around the problem of knowing whether it works or not. While the field is almost despotically pragmatic in its concerns with classification and prediction (although in certain ways, curiously idealistic too), it is troubled by the persistence of two broadly different kinds of _learning_: supervised and unsupervised (as well as hybrid kinds such as semi-supervised learning). Writing around 2000, Hastie et. al. state: 

> With supervised learning there is a clear measure of success or lack thereof, that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations. Lack of success is directly measured by expected loss over the  joint distribution $Pr(X,Y)$. This can be estimated in a variety of ways including cross-validation. In the context of unsupervised learning, there is no such direct measure of success. ... This uncomfortable situation has led to heavy proliferation of proposed methods, since effectiveness is a matter of opinion and cannot be verified directly.  [@Hastie_2009, 486-7]


Curves lie at
