\chapter{Machines finding functions}
\label{ch:function}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')

library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
simpleCap <- function(x) {
    s <- strsplit(x, " |-")[[1]]
    return (paste(toupper(substring(s, 1,1)), tolower(substring(s, 2)), sep="", collapse=" "))
  }
```

> Because of a gradient that no doubt characterizes our cultures, discursive formations are constantly becoming epistemologized [@Foucault_1972, 195]

The opening pages of machine learning textbooks often warn or enthuse about the profusion of techniques, algorithms, tools and machines. \index{machine learners!variety of|(} 'The first problem facing you', writes Pedro Domingos, 'is the bewildering variety of learning algorithms available. Which one to use? There are literally thousands available, and hundreds more are published each year [@Domingos_2012, 1]. \index{Domingos, Pedro} 'The literature on machine learning is vast, as is the overlap with the relevant areas of statistics and engineering' writes David Barber in _Bayesian Reasoning and Machine Learning_[@Barber_2011,4]; 'statistical learning refers to a vast set of tools for understanding data' writes James and co-authors in an _Introduction to Statistical Learning with R_ [@James_2013,1]; or writing in in *Statistical Learning for Biomedical Data* the biostatisticians James Malley, Karen Malley and Sinisa Pajevic 'freely admit that many machines studied in this text are somewhat mysterious, though powerful engines'  [@Malley_2011,  257]. In _Thoughtful Machine Learning_ Matthew Kirk adds: 'flexibility is also what makes machine learning daunting. It can solve many problems, but how do we know whether we’re solving the right problem, or actually solving it in the first place?' [@Kirk_2014, ix]. \index{Kirk, Matthew}  The prefatory comments from Domingos, Barber, James, Malley and Kirk suggest  that machine learning comprises a rampant abundance of techniques. Much machine learning work, at least for many practitioners, concerns not so much implementation of particular techniques (neural network, decision tree, support vector machine, logistic regression, etc.), but rather navigating the maze of methods and variations that might be relevant to a particular situation. 

\begin{figure}
  \centering
      \includegraphics[width=0.95\textwidth]{figure/ml_map_scikit.pdf}
        \caption{`scikit-learn` map of machine learning techniques [TBA: ref to diagram]}
  \label{fig:mapping_functions}
\end{figure}

How does this effect of accumulation and profusion arise? And what do machine learners do about it? The publications I have just been referring to present that profusion as a problem of the piling up of scientific publications and attempt to manage it by writing textbooks that provide theories, indexes, maps and guides to the bewildering variety of machine learners. The anchoring textbook _Elements of Statistical Learning_ deploys tables, overviews, theories of statistical modelling, model assessment and comparison techniques to aid in navigating them. Parallel and complementary mappings accompany software libraries. The map of machine learning techniques shown in Figure \ref{fig:mapping_functions} comes from a particular software library written in `Python,` `scikit-learn` [@Pedregosa_2011].  \index{Python!scikit-learn library} This software library is widely used in industry, research and commerce. In contrast to the pedagogical expositions, theoretical accounts or guides to reference implementation, code libraries such as  `scikit-learn` tend to order the range of techniques by offering recipes and maps for the use of the *functions* the libraries supply. \index{function} The branches in the figure lay down paths through the profusion of techniques as a decision tree.[^3.01] Similarly, for `R` code, the *Comprehensive R Archive Network* tabulates key libraries of `R` code in  a  machine learning 'task view' [@Hothorn_2014].  
\index{machine learners!variety of|)}

```{r scikit-learn, engine='python', echo=TRUE, messages=FALSE, warnings=FALSE, results='asis'}
import sklearn
from sklearn import *
modules = dir(sklearn)
modules_clean = [m for m in modules if not m.startswith('_')]
print(modules_clean)
```

The architecture of these software libraries itself classifies and orders machine learners.  `Scikit-learn` for instance comprises a number of sub-packages:
Modules such as `lda` (linear discriminant analysis), `svm` (support vector machine) or `neighbors` (_k_ nearest neighbours) point to well-known machine learners, whilst `cross-validation` or `feature_selection` refer to ways of testing models or transforming data respectively. Again, machine learning patches together a variety of abstractive practices. \index{abstraction!levels of} These divisions, maps and classifications help order the techniques, but they obscure the problematic process that first generated a competing profusion of machine learners. That profusion comes from a slippage between three main senses of the function: function as mathematical relation or operator, function as concrete machinic operation and function in the sense of what something does.  \index{function!different senses of!mathematical, operational, biological}

```{r function_lit, results='asis'}

res = dbGetQuery(con, 'select distinct TI, PY, DE, TC from basic_refs where anchor == "machine_learning_WOS" and TI like "% function %" or DE like "%function %"')
fun_top = head(res[order(res$TC, decreasing=TRUE), c('PY', 'TI', 'DE', 'TC')], 100)
fun_top$TI = sapply(fun_top$TI, simpleCap)
fun_top$DE = tolower(fun_top$DE)
fun_top  = fun_top[sample(nrow(fun_top), 50),]
fun_top = fun_top[order(fun_top$PY),]
al = '|l|l|l|l|l|'
tab = xtable(fun_top, align=al, label='tab:function_in_ml', caption='Sample of highly cited machine learning publications referring to "function" in title or keyword')
align(tab) <- "p{0.1\\textwidth}|p{0.4\\textwidth}|p{0.4\\textwidth}| p{0.1\\textwidth}"
print(tab, type='latex', include.rownames=FALSE)

```

All three senses of function constantly coalesce in machine learning research. Table \ref{tab:function_in_ml} shows the titles and author-supplied keywords of a sample of well-cited machine learning publications. In these randomly chosen publications, mathematical functions -- 'kernel function,' 'discriminant function,' 'radial basis function' -- mingle with biological and engineering functions -- 'protein-binding function,' 'intestinal motor function', or 'rules to control locomotion.' Mathematical functions dominate here.  Machine learners 'find', 'estimate,' 'approximate,' 'analyse' and sometimes 'decompose' mathematical functions. The primary mathematical sense of a function refers to a relation between sets of values or variables. (A variable is a symbol that can stand for a set of numbers or other values.) A function is one-to-one relation between two sets of values. It maps a set of arguments (inputs) to a set of values (outputs, or to use slightly more technical language, it maps between a _domain_ and a _co-domain_.) As we have already seen, mathematical functions are often written in formulae of varying degrees of complexity. They are of various genres, provenances, textures and shapes: polynomial functions, trigonometric functions, exponential functions, differential equations, series functions, algebraic or topological functions, etc. Various fields of mathematics have pursued the invention of functions. In machine learning and information retrieval, important functions would include the logistic function (discussed below), probability density functions (PDF) for different probability distributions (Gaussian, Bernoulli, Binomial, Beta, Gamma, etc.). \index{probability!distribution} \index{function!mathematical} From an _almost_ purely mathematical standpoint, machine learning can be understood as function finding operations. Implicitly or explicitly, machine learners find a mathematical expression -- a function -- approximating an outcome of the social, technical, financial, transactional, biological, brain, heart or group process that generated the data in question. Regardless of the application, no single mathematical function perfectly or uniques expresses data. Many if not infinite functions can approximate any given data. The abundance of functions blurs the diagonal lines that run between the mathematical function defined in an equation and an operational machine learner designed to find specific values of the parameters of that function. The mathematical function and the software function are in diagrammatic relation, but the _diagonals_ that run between them are not always in a one-to-one mapping. \index{diagrammatic!diagonal}


## Supervised or unsupervised, who learns what?

The `scikit-learn` map of techniques addresses the problem of choosing a machine learner. This is only a starting point. No matter how powerful machine learners become,  they do not operate autonomously.  The techniques, models, forms of abstraction, data formats, and performance properties of algorithms have to be learned by people (reading books, attending classes, watching demonstrations, trying out software and code, etc. See Chapter \ref{ch:subjects}). Once learned, maps such as the one shown in Figure \ref{fig:mapping_functions} may become redundant. But other forms of close attention, monitoring, and observation remain crucial whenever  machine learners encounter data or wherever they enter the common vector space. 

A need to observe the machine organises the field of machine learning. The optics of this observation of how machine learners traverse the common vector space vary. \index{common vector space} Machine learning textbooks and courses usually distinguish 'supervised', 'unsupervised' and sometimes 'semi-supervised' learning. While the field is almost despotically pragmatic in its commitment to optimisation of  classification and prediction (although in certain ways, curiously idealistic too in its constant reuse of well-worked datasets such as `iris` or `South African heart disease`), it reluctantly accepts the existence of these two broadly different kinds of _learning_. \index{machine learning!learning|(} Writing around 2000, Hastie et. al. state: 

>With supervised learning there is a clear measure of success or lack thereof, that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations. Lack of success is directly measured by expected loss over the  joint distribution $Pr(X,Y)$. This can be estimated in a variety of ways including cross-validation. In the context of unsupervised learning, there is no such direct measure of success. ... This uncomfortable situation has led to heavy proliferation of proposed methods, since effectiveness is a matter of opinion and cannot be verified directly.  [@Hastie_2009, 486-7]

Supervised learning in general terms constructs a model by training it on some sample data (the training data \index{data!training} ), and then evaluating its effectiveness in classifying or predicting  test data \index{data!test} whose actual values are already known. The 'clear measure of success' they refer to in relation to in so-called 'supervised learning' is of relatively recent date.[^3.02]  Unsupervised machine learning techniques generally look for patterns in the data without any training or testing phases (for instance, _k_-means or principal component analysis do this, and both techniques have been heavily used for more than fifty years). In both supervised and unsupervised learning people look at the models to find out how the models traverse, fit, partition or map the data. At a general level, machine learning is a system of making statements and rendering relations visible through supervision. As I will suggest below, partial observers -- a term drawn from the work of Félix Guattari and Gilles Deleuze -- supervise the diagrammatic functioning of machine learning.   At the same time, opacity -- 'no direct measure of success' -- is generative in machine learning. \index{partial observer|see{function!partial observer}} Amidst the optically dense pages of mathematical functions, plots of datasets and listing of algorithms, _Elements of Statistical Learning_'s frank admission that something cannot be measured and that this difficulty has led to proliferating methods seems to me quite promising ground to explore for possible transformations and changes. But  this discomfort about unsupervised learning does seem to discourage its use.  If, as the first part of the quoted text puts it, supervised learning has a clear 'measure of success,' that success only seems to encourage further variations and comparisons that end up proliferating machine learners, their publications and their software implementations.  Levels of predictive success may vary widely with different techniques and different situations, but an almost unbounded optimism associated with machine learning (for instance as more or less unspoken foundation of  any analysis of 'big data')  runs pell-mell across all of them. \index{machine learning!learning|)}

## Which function operates?

\index{function!mathematical|(} Formally, the differences between machine learners appear as mathematical functions. The  mathematical sense of function is writ large in nearly all machine learning literature since functions diagram the relations on which devices and machines operate. Indeed, we might say that machine learning is nothing other than a machinic version of the functions that have long interested and occupied mathematicians. Importantly, functions support both the operations and the ordering of those operations.   Classifiers, or machine learners that allocate case to categories, are often identified directly with functions:

>A classifier or classification rule is a function $d(\mathbf{x})$ defined on $\mathcal{X}$ so that for every $\mathbf{x}$, $d(\mathbf{x})$ is equal to one of the numbers $1, 2, ..., J$ [@Breiman_1984, 4]

Writing in the 1980s, the statistician Leo Breiman describes classifiers -- perhaps the key technical achievement of machine learning and certainly the catalyst of many applications of machine learning  -- in terms of functions. A classifier _is_ a function $d(\mathbf{x}$ where is $\mathbf{x}$ is the data and $d$ ranges over numbers that map onto categories, rankings or or other forms of order and belonging. The equation of machine learners  to functions is quite pervasive. \index{Breiman, Leo} Learning, predictions, and the classifications produced by machine learning derive from functions. The identification of machine learning with functions appears in the first pages of most machine learning textbooks. Learning in machine learning means finding a function that can identify or predict patterns in the data. As _Elements of Statistical Learning_ puts it,

>our goal is to find a useful approximation $\hat{f}(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output [@Hastie_2009, 28]. 

In a highly compressed form, this statement of goals doubles the function. It contains _the_ function that generated the data as a foundation. This function figures as a ground truth almost physically or existentially imputed to the world. It also refers to 'finding  ... $\hat{f}(x)$', where the '^' indicates an approximation produced by an algorithmic implementations, and it avers to 'use'. Similar statements pile up in the literature. Perhaps more importantly, _learning_ here is understood as function finding. A leading theorist of learning theory Vladimir Vapnik again uses the language of approximation: 'learning is a problem of _function estimation_ on the basis of empirical data' [@Vapnik_1999, 291].[^3.03] \index{Vapnik, Vladimir}  The use of the term 'learning' in machine learning displays affiliations to the field of artificial intelligence, but the  attempt to find a 'useful approximation' -- the 'function-fitting paradigm' as [@Hastie_2009, 29] terms it -- stems mainly from statistics.  Not all accounts of machine learning emphasise learning as  function fitting. Some retain the language of intelligent machines (see for example, [@Alpaydin_2010, xxxvi] who writes: 'we do not need to come up with new algorithms if machines can learn themselves'). Despite any differences in the  framing of the techniques, all accounts of machine learning, even those such as _Machine Learning for Hackers_ [@Conway_2012] that eschew any explicit recourse to mathematical formula,  rely on the  formalism and modes of thought associated with mathematical functions. Whether they are seen as forms of artificial intelligence or statistical models, the formalisms are directed to build 'a good and useful approximation to the desired output' [@Alpaydin_2010, 41], or, put more statistically,  'to use the sample to find the function from the set of admissable functions that minimizes the probability of error' [@Vapnik_1999, 31]. Note the unobtrusive but crucial caveats in this formulation: functions must be found from a set of 'admissable functions.' Functions anchor machine learning so that we don't have to come up with algorithms, yet functions themselves have to be found under certain constraints.
\index{function!mathematical|)}

Which functions does machine learning admit? As is often the case in working with a massive technical literature, the first problem in making sense of what is happening with function in machine learning concerns sheer abundance. The pages of [@Hastie_2009] are marked with score of references to 'functions': quadratic function, likelihood function, sigmoid function, loss function, regression function, basis function, activation function, penalty functions, additive functions, kernel functions,step function,  error function, constraint function, discriminant function, probability density function, weight function, coordinate function, neighborhood function, and the list goes on. This list stands against a background of several hundred mathematical functions commonly used in science and engineering.[^3.05] Clearly we cannot expect to understand the functioning of all these functions in any great detail. However, even a glance through this prickly list of terms begins to suggest that not only is there quite a heavy reliance on functions in this field (as perhaps in many other science and engineering disciplines), but that the proliferation of functions might itself be a way to map some important  operations occurring in and around machine learning. We can also already see in this list that the qualifiers of the term function are diverse. Sometimes, the qualifier refers to a mathematical form -- 'quadratic,' 'coordinate', 'basis' or 'kernel'; sometimes it refers to statistical considerations -- 'likelihood', 'regression', 'error,' or 'probability density'; and sometimes it refers to some other concern that might relate to a particular modelling device or diagram -- 'activation,' 'weight', 'loss,'  'constraint,' or 'discriminant.' 

## What does a function learn?

Functions configure the diagrammatic functioning of machine learning. In what sense does a function learn? The philosopher of science Isabelle Stengers seems to view functions pessimistically:
\index{Stengers, Isabelle} \index{function!learning|(}

> No function can deal with learning, producing, or empowering new habits, as all require and achieve the production of different worlds, non-consensual worlds, actively diverging worlds [@Stengers_2005, 162]

In some ways, Stengers would, on this reading, be taking a fairly conventional position on  mathematical functions.  They cannot learn or produce anything, only reproduce patterns that we already recognise.  Similar statements might be found in many philosophical writings on science and on mathematics in particular.[^3.16] But elsewhere  in her writing Stengers explicitly affirms _experimental practice_, much of which depends on functions and their operations [@Stengers_2008]. It might be better to say that she limits the claims made about the functions in order to highlight the specific power of science: 'celebrating the exceptional character of the experimental achievement very effectively limits the claims made in the name of science' [@Stengers_2011, 376]. (Limiting claims made for science might save  it from being totally re-purposed as a techno-economic innovation system. ) \index{science!experiment}

The  connection between a given function and a given concrete experimental situation is highly contingent or indeed singular. Stengers argues that mathematical functions impinge on matters of fact via a reference between a function and an experimentally constructed matter of fact: 

>The reference of a mathematical function to an experimental matter of fact is neither some kind of right belonging to scientific reason nor is it an enigma, but actually the very meaning of an experimental achievement [@Stengers_2005, 157].

The generic term 'reference' here harbours a multitude of relations. The experimental achievement, the distinctive power of science, works through a tissue of relations that connect people, things, facts and mathematical functions in a highly heterogeneous weave.[^3.12] When a biomedical experiment uses _logistic regression_ to  'estimate the probability that a critically-ill lupus patient will not survive the first 72 hours of an initial emergency hospital visit' [@Malley_2011, 5],  they are doing machine learning, and the value of their predictions is not captured by classical statistical approaches (analysis of variance, correlations, regression analysis, etc.). As machine learning techniques and the underpinning mathematics of probabilistic learning theory circulate more widely across different scientific disciplines (geography, ecology, astronomy, epidemiology, genomics, chemistry, communication engineering), in each setting the experimental achievement consists in constructing references between the mathematical functions and the matters of fact generated by instruments, observations and measurements and cantilevered by previous experiments. The question from Stengers' standpoint is this: what happens  to the structure of referrals through experiments and the accumulated knowledge when functions are said to learn? \index{function!learning} In order to address this question, we need to delineate how functions function in machine learning. That could be in several different ways: as statements or utterances, as formula-diagrams, as graphic forms and in operational implementations as code. Any account of machine learning as function finding needs to map the concatenation of these different elements, none of which alone can anchor the 'learning' that goes on in machine learning. \index{function!learning|)}

At first glance, machine learning as a field is not very experimental (even it radically influences the conduct of experiments in many scientific fields; see chapter \ref{ch:genome}). It lacks the apparatus, the instruments, the laboratories, field sites or clinics of experimental practice. Experimentation, if there is any, takes place principally in the form of rendering diagrammatically visible the relays or referrals between different functions as they traverse data. \index{diagrammatic!experiment} While functions are often written in formulae, they are also diagrammed in graphic forms as plots and in operational forms as code or software. Many of the characteristic functions listed above appear in the graphic form of lines and curves. This diagrammatic entanglement of machine learners in curves is not surprising. The historical invention of the term 'function' by the philosopher G.W. Leibniz in the 17th century relates to curves and their description. Functions for Leibniz describe variations in curves such as their slope. Identifying and locating these _singularities_ in curves still preoccupies the function-finding done in machine learning. In contrast to the table, or the generalized common vector space that expands to accommodate all differences, the curve and the many graphic objects that seek to show curves in different ways, display continuous variation and singular points.\index{function!mathematical!invention of}

\begin{figure}
  \centering
\begin{subfigure}[b]{0.4\textwidth}
      \includegraphics[width=0.4\textwidth]{figure/hastie_2009_page_13.pdf}
        \caption[Linear regression acting as classifier]{Linear regression classifiered diagrammed through simulated data (Hastie, 2009, 13)}
  \label{fig:linreg_classifier_hastie}
\end{subfigure}

\begin{subfigure}[b]{0.4\textwidth}
      \includegraphics[width=0.4\textwidth]{figure/hastie_2009_page_15.pdf}
        \caption[\texit{k}-nearest neighbours classifier]{A k-nearest neighbours classifier diagrammed through simulated data (Hastie, 2009, 15)}
  \label{fig:knn_hastie}
\end{subfigure}
\end{figure}

The 15-nearest neighbour classifier shown in Figure \ref{fig:knn_hastie} layers simulated data (2 dimensions and an output variable with the values `BLUE` or `ORANGE` [@Hastie_2009, 12]) and a meandering line that classifies the simulated observations in terms of their class membership.  Viewed in terms of their visual composition, many machine learning diagrams are organised around such lines that contour, divide, bound or surround the data captured from or emitted by the world.  Curving lines outnumber all other graphic forms, whether the curves are the 'decision boundaries,' the plots of 'training errors,' the 'contours of constant density' [@Hastie_2009, 109], or the 'regularization path' for the South African heart data [@Hastie_2009, 126]. There is a constant tension in these graphic forms between line and curve. \index{graphic!line and curve} Consonant with the vectoral ideal of a straight line that either connects or cuts the data (as shown in Figure \ref{fig:linreg_classifier_hastie}), many of the graphics in the book (and others like it, especially the books written by computer scientists) show strong preferences for straight lines. But variations and uncertainties of various kinds detour the pursuit of the straight lines. Curves proliferate as machine learners try to the soften the rigidity of the lines or find regular paths for lines through irregular terrain. Viewed very naively, the contrast between lines and curves in the two figures above, a contrast replayed many times in _Elements of Statistical Learning_, suggests that different diagrammatic operations  are at work around the data, sometimes aligning and straightening relations (for instance, the many linear models) and sometimes tracing much more non-linear paths through the data (for instance, as in _k_ nearest neighbours or much more indirectly convolutional neural networks). In  the several hundred colour graphic plots in [@Hastie_2009], a striking mixture of network diagrams, scatterplots, barcharts, histograms, heatmaps, boxplots, maps, contour plots, dendrograms and 3D plots respond to this tension between linearity and curvature.[^3.3] Many of these graphic forms are common in statistics (histograms and boxplots), but some relate specifically to data mining and statistical learning (for instance, ROC -- Receiver Operating Curve -- or regularization path plots). A significant proportion of these graphics show no data from experiments or measurements, but nearly all of them either show the results of attempts to diagram a line or find a function (the two are almost synonymous) that refers the different data elements in the common vector space to each other. \index{function!diagram}

## Diagramming with curves: the logistic function

Curves, in their spectrum of curvatures ranging from flat to intricately convoluted, refer functions to concrete situations. As we have already seen, machine learners often refer to 'fitting', as well as  'over-fitting' and 'under-fitting.' _Fitting_ is a way of bringing functions into the data. As we saw in the previous chapter, the common vector space cannot be fully seen. Graphic plots and statistical summaries offer perspectival views on it, but machine learners traverse it by finding a mapping between the dimensions of the vector space. \index{common vector space} They do that by smoothing and aligning data.  Take the example of _sigmoid_ functions. These quite simple functions underpin many classifiers and animate many of the operations of neural network, including their recent re-incarnations in 'deep learning' [@Hinton_2006; @Mohamed_2011].  \index{function!sigmoid|seealso{function!logistic}} A well-known example of a sigmoid function, the logistic function, can be written as:

\begin {equation}
\label {eq:logistic_function}
f(x) = 1/(1+e^{-kx})
\end {equation}


```{r logistic_curve, result='hide'}
            #TBA: need to add plot labels for k	
	x = seq(-100, 100, 0.01)
    k = 0.1
	y = 1/(1+exp(-k*(x-12)))
    k = 12
	y2 = 1/(1+exp(-k*(x-12)))
	plot(x,y)
    lines(x, y2)
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/logistic_curve-1.pdf}
        \caption{Logistic or sigmoid function}
  \label{fig:logistic_curve}
\end{figure}

The logistic function (shown as Equation \ref{eq:logistic_function} and as two curves in Figure \ref{fig:logistic_curve}), as we will see, is very important in many classification and decision settings precisely because of its _non_linear_ shape and its constrained movement within a limited range of values (0 to 1).  How does a function such as the sigmoid function 'fit' anything? Here the curve itself and even the name 'sigmoid' is the best guide. The S-shape of the sigmoid curve is a good guide to operations associated with curves. The logistic function has quite a long history in statistics since that curve diagrams growth and change in various ways. \index{function!logistic} (As the historian of statistics J.S. Cramer writes: 'The logistic function was invented in the nineteenth century for the description of the growth of organisms and populations and for the course of autocatalytic chemical reactions' [@Cramer_2004, 614].[^3.29]  In nearly all of these cases, the function was used to fit a curve to data on the growth of something: populations, reactions, tumours, tadpoles tails, oats and embryos. The reference of the curve to growth comes from its changing slope. Growth starts slowly, increases rapidly and then slows down again as it reaches a limit. In the second half of the twentieth century, it was widely used in economics. In all these settings and usages, the curve was a way of summarising and predicting growth. Census data, clinical or laboratory measurements supplied the actual values of $f(x)$ at particular times, the $x$ values. The task of the demographer, physiologist or economist was to work out the values of parameters such as $k$ that controlled the shape of the curve. The logistic function had a well-established biopolitical resonance. 

Note that the curves showing in Figure \ref{logistic_curve} plot the same data ($\mathbf{X}$ and $y$ values), but differ in their curvature. This diagrammtic variation derives from the parameter $k$, which discreetly appears in the equation \ref{eq:logistic_function} next to $x$. Such parameters are vital control points in function fitting and any learning associated with that. Varying these parameters and optimising their values is the basis of 'useful approximation' in machine learning. Sometimes these parameters can be varied so much as to suggest entirely different functions. \index{function!parameters of} In \ref{fig:logistic_curve} for instance, $k=12$ produces a much sharper curve, a curve that actually looks more like a qualitative change, range than a smooth transition from $0$ to $1$. The sharp shape of the logistic function when the scaling parameter $k$ is larger suggests another important transformation, somewhat orthogonal to the description of rates of growth under limits.  The mathematical function $f(x) = 1/(1+e^{-x})$ can be treated as a way of mapping continuously varying numbers (the $x$ values) and discrete values. Because $f(x)$ tends very quickly to converge on values of $1$ or $0$, it can be coded as 'yes'/'no'; 'survived/deceased', or any other binary difference. The transformation between the $x$ values sliding continuously and the binary difference, classification or categorisation pivots on the combination of the  exponential function ($e^{-x}$), which rapidly tends towards zero as $x$ increases and rapidly tends towards $\inf$ as $x$ decreases, and the $1/(1+ ...) $, which converts high value denominators to almost zero, and low value demominators to one. This constrained path between variations in $x$ and their mapping to the value of the function $f(x)$ is mathematically elementary, but typical of the relaying of references \index{function!reference} that allows functions to intersect with and constitute matters of fact and states of affairs. This realisation -- that a continuously varying sigmoid function could also map discrete outcomes -- forms the basis of many  machine learning classifiers. So, a contemporary biostatistical machine learning textbook can write: we can use logistic regression to  'estimate the probability that a critically-ill lupus patient will not survive the first 72 hours of an initial emergency hospital visit' [@Malley_2011, 5]. If the same curve -- the logistic curve -- can describe quite different situations (the growth of a population, the probability that someone will die), then we can begin to see that sigmoid functions, and the logistic curve in particular, might be useful devices as approximations to the 'underlying function that generated the data.  \index{function!logistic}

## The cost of curves in machine learning

```{r logistic_occurrence, results='asis'}
query = "select  distinct TI, PY, DE, TC from basic_refs where TC >100 and (TI like '%logistic%' or DE like '%logistic%')"
res = dbGetQuery(con, query)
logdf = res[order(res$TC, decreasing=TRUE),]
logdf = logdf[-c(1,2),]
logdf$TI = sapply(logdf$TI, simpleCap)
logdf$DE = sapply(logdf$DE, simpleCap)
logdf = logdf[order(logdf$PY),]
view(head(logdf, 30))
tab = xtable(logdf[-c(1,2),], align=al, label='tab:logistic_in_ml', caption='Sample of highly cited scientific publications referring to "logistic regression" in title or keyword')
align(tab) <- "p{0.1\\textwidth}|p{0.4\\textwidth}|p{0.4\\textwidth}| p{0.1\\textwidth}"
print(tab, type='latex', include.rownames=FALSE)
```

How does this take place practically? As I have already mentioned, the logistic function appears frequently in machine learning literature, prominently as part of perhaps the most classical learning machine, the logistic regression model, but also as a component in other techniques such as neural networks (see table \ref{tab:logistic_in_ml} for a sample of well-cited publications). \index{machine learner!logistic regression}  Descriptions of logistic regression models appear in nearly all machine learning tutorials, textbooks and training courses (see Chapter 4 in [@Hastie_2009]). Logistic regression models are heavily used in biomedical research, where, as 'logistic regression is the default "simple" model for predicting a subject's group status' [@Malley_2011, 43]. As Malley et.al. suggest, 'it can be applied after a more complex learning machine has done the heavy lifting of identifying an important set of predictors given a very large list of candidate predictors' (43).  Especially in comparison to more complicated models, logistic regression models are relatively easy to interpret because they are superimposed on the linear model that we have been discussing already (see figure \ref{fig:linreg_classifier_hastie} and also chapters \ref{ch:diagram} and \ref{ch:vector}).  As Hastie et.al write: 'the logistic regression model arises from the desire to model the posterior probabilities of the $K$ classes via linear functions in $x$, while at the same time ensuring that they sum to one and remain in $[0,1]$' [@Hastie_2009, 119].  Paraphrased somewhat loosely, this says that the logistic regression model predicts what class or category a particular instance is likely to belong to, but 'via linear functions in $x$.'  We see something of this predictive desire from the basic mathematical expression for logistic regression in a situation where there are binary responses or $K=2$ : 

\begin {equation}
\label {eq:logistic_regression}
Pr(G=K|X=x) = \frac{1}{1+\sum_{l=1}^{K-1}exp(\beta_{l0} + \beta_l^Tx)} 
\end {equation}

[@Hastie_2009, 119]

In equation \ref{eq:logistic_regression}, the logistic function operates on linear functions and thus encapsulates lines in curves. That is, the linear model (the model that fits a line to a scattering of points in vector space) appears as $\beta_l0 + \beta_l^Tx$ (where as usual $\beta$ refers to the parameters of the model and $x$ to  the matrix of input values). The linear model has, however, now been put inside the sigmoid function so that its output values no longer increase and decrease linearly. Instead they follow the sigmoid curve of the logistic function, and range between a minimum of $0$ and a maximum of $1$. As usual, small typographic conventions express some of this shift. In equation \ref{eq:logistic_regression}, some new characters appears: $G$ and $K$. Previously, the response variable, the variable the model is trying to predict, appeared as $Y$. $Y$ refers to a continuous value whereas $G$ refers to membership of a category or class (e.g. survival vs. death; male vs female; etc.).

What does this wrapping of the linear model in the curve of the sigmoid logistic curve do in terms of finding a function? Note that the shape of this curve has no intrinsic connection or origin in the data. The curve no longer corresponds to growth or change in size, as it did in its nineteenth century biopolitical heyday. Rather,  the curvilinear encapsulation of the linear model allows the left hand side of the expression to move into a different register. The left hand side of the expression is now a probability function, and defines the probability ($Pr$) that a given response value ($G$) belongs to one of the pre-defined classes ($k = 1, ..., K-1$)[^3.5]. In this case, there are two classes ('yes/no'), so $K=2$. Unlike linear models, that predict continuous $y$ values for a given set of $x$ inputs, the logistic regression model produces a probability that the instance represented by a given set of $x$ values belongs to a particular class.  When logistic regression is used for classification, values greater than $0.5$ are usually read as class predictions of 'yes', 'true' or `1`.  As a result, drawing lines through the common vector space can effectively become a way of classifying things. Note that this increase in flexibility comes at the cost of a loss of direct connection between the data or features in the generalized vector space, and the output, response or predicted variables. They are now connected by a mapping that passes through the somewhat more mobile and dynamic operation of exponentiation $exp$, a function whose rapid changes can be mapped onto classes and categories.  

and with some new difficulties in estimating the all important weighting parameters $\beta$ in equation \ref{eq:logistic_regression}.  

HERE

[^3.3]: A montage of all the graphics in _Elements of Statistical Learning_ can be found at [TBC]

[^3.05]: The U.S. National Institute of Standards published *The Handbook of Mathematical Functions* in 1965 [@Abramowitz_1965]. This heavily cited volume, now also [versioned online](http://dlmf.nist.gov) lists hundreds of functions organised in various categories ranging from algebra to zeta functions. While a number of the functions and operations catalogued there surface in machine learning,  machine learners implement, as we will see, quite a narrow range of functions. 

[^3.01]: See Chapter \ref{ch:pattern} for discussion of decision trees in machine learning.

[^3.02]:  Only in the mid-1980s were the first theories of algorithmic learning formalised [@Valiant_1984].

[^3.03]: Vapnik is said to have invented the support vector machine, one of the most heavily used machine learning technique of recent years on the basis of his theory of computational learning. Chapter \ref{ch:dimension} discusses the support vector machine.

[^3.16]: A major reference here would be Ernst Cassirer [@Cassirer_1923] who posited a major philosophical-historical shift from ontologies of substance reaching back to Aristotle to a functional ontology emerging in 19th century as the notion of function was generalized across many mathematical and scientific fields. The idea of computable functions lies at the base of theoretical computer science and has been a topic of interest in some social and cultural theory (e.g. [@Parisi_2013]; see also my [@Mackenzie_1997]).  \index{Cassirer, Ernest}


[^3.12]: This point has often been made in the social studies of science; see  [@Latour_1993] for a very high-level account of this 

[^3.29]: The Belgian mathematician Pierre-François Verhulst designated the sigmoid function the 'logistic curve' in the 1830-40s [@Cramer_2004, 616]. It was independently designated the 'autocatalytic function' by the German chemist Wilhelm Ostwald in the 1880s, and then re-invented under various names by biologists, physiologists and demographers during 1900-1930s (617). The term 'logistic' returns to visibility in the 1920s, and has continued in use as a way of describing the growth of something that reaches a limit. 

[^3.5]: I leave aside any further discussion of probability in machine learning here. It is the topic of chapter \ref{ch:probability}. 
