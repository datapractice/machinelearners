# 4.  $N = all$: machine learning gets all the data

## structure

- hacking on how statistics became real -- tables of numbers; retrofitting of laws as regularities; the role of the social; constant crossover between medicine, state and science, 
- the fact that machine learning books are full of stats, but the traditional stats of hypothesis testing, tests of significance, measures of uncertainty or variation although some of this appears.
- statistical mechanisms in the service of something else -- follows on from argument about functions as partial observers and as things in the world (code) -- statistics becomes a device to order the progress of machines; and the laws that matter -- law of large of numbers; the normal distribution -- become techniques of controlling the proliferation of movements. 
- progression from 18th century joint probability through to 19th century distributions. 

## Introduction

In the final pages of _The Taming of Chance_, the philosopher Ian Hacking describes the work of the geodesic philosopher C.S. Peirce in terms of a twin affirmation of chance. On the one hand, Peirce, following the work of the psychophysicist Gustav Fechner and before him the astronomer-sociologist Adolphe Quetelet, makes the normal curve into an underlying reality.[^4.1] The 'personal equation,' the variation in measurements made by any observer, become 'a reality underneath the phenomena of consciousness' [@Hacking_1990, 205]. At the same time, and in order to show the underlying reality of the curve, 'Peirce deliberately used the properties of chance devices to introduce a new level of control into his experimentation. Control not by getting rid of chance fluctuations, but by adding some more' [@Hacking_1990, 205]. Peirce's belief in absolute chance, 'a universe of chance' as Hacking puts it, came in the way of a series of 'realization' of curves, in which first social, then biological and finally psychological variations were all understood as evidence of a generative function, the normal distribution or Gaussian function. In the century or so since, what happened to the thorough-going affirmation of statistical thought and probabilistic practice epitomised by Peirce? Hacking stresses that he does not understand Peirce as the precursor or the innovator of twentieth century statistical thought (Hacking's _Taming of Chance_ ends at 1900), but rather as 'the first philosopher completely to internalize the way chance had been tamed in the nineteenth century' (215). What would the equivalent philosopher-machine learner internalize today? What would such persons, working in science or media or government, hold firm in relation to chance, probability and statistics? 

[^4.1]: The historian of statistics Stephen Stigler provides a lengthy account of Fechner's work [@Stigler_1986].  TBA -- in what chapter?

In a broad sense, the setting that concerned Peirce in his work at the U.S. Government Coast Survey in the 1870s does not differ greatly from the setting that machine learners encounter. In the opening lines of the First Edition of _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman write:

>The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope [@Hastie_2009, xi]

At the end of the preface, they also cite, we might note in passing, Hacking's work: 'The quiet statisticians have changed our world' [@Hastie_2009, xii]. With some justification, we might ask therefore: what difference do the 'vast amounts of data ... generated in many fields' (xi) make to what machine learners internalize of their world? This question of what kind of world becomes thinkable through machine learning can be addressed partly by contrasting the 'taming of chance' achieved during the eighteenth and nineteenth centuries, and the statistical practices of machine learning today. Is machine learning a further taming of chance? What role does randomness and probability play in machine learning?

The broadest claim associated with statistical machine learning might be the simple expression shown in:

\begin {equation}
\label {eq:n_all}
N = \forall X
\end {equation}


In Equation \ref{eq:n_all}, $N$ refers to the number of observations (and hence the size of the dataset), the symbol $\forall$ means 'all' since this is the level of inclusion which many fields of knowledge in science, government, media, commerce and industry envisage, and $X$ refers to the data itself arrayed in common vector space. Note that this expression leaves some things out. $Y$, the response variable, for instance, may or may not be known.  While statistical techniques and practices have appeared in previous chapters, I focus here  on changes in probability practices associated with machine learning, and in particular, $N = \forall $X, the claim that with all the data, statistical thinking and the potentials of the statistical reasoning fundamentally change. The claim that with $N=\forall X$ everything changes has been widely discussed.[^4.2]  Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different ways in the course of the vignettes and comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data:

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]

In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is never really discussed. While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they do say that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of specific techniques of data crunching or the math is largely left aside. This is not to criticize a book that sets out to describe trends affecting business and government for a general readership, but without a sense of how statistical thinking animates almost all salient features of crunching the data and particularly the predictions, it becomes hard to see how the 'revolution' takes place. In other words, the shift from $N=n$ (some of the data) to $N=\forall X$ does not occur without other transpositions and rearrangements that do not simply concern choices about how much data to use, but also concern how data is given in the world and how it is thinkable.[^4.3] Following Hacking's core argument about how nineteenth century statistics transformed measurements (for instance, the mean as average of all measured values) into real quantities (for instance, mean as the ideal or abstract property of a population; e.g. life expectancy), we might see this shift between $n$ and $\forall X$, a shift very much animating and dependent on  machine learning, as an event akin to the advent of the Normal distribution (and indeed, $\mathnormal{N}$ is a standard symbol for the Normal distribution in statistics textbooks) as a way of thinking about populations and the control of populations [@Hacking_1975, 108]. Aligning the development of machine learning with the longer duration of statistical thinking might allow us in short to gauge a little better what is changing and where. 

[^4.2]: Rob Kitchin provides a very useful overview of these claims in [@Kitchin_2014]. While I will not analyse the claims about 'big data' in specific cases in any great detail, I  

[^4.3]: Part of this development has already been related in the previous chapter on 'learning' and function estimation. That chapter avoided any real discussion of statistical thought. Instead it explored the various sense of function and function finding that underpin machine learning. But much of that function finding and approximation garners referential weight through the statistical practices and modes of thought that accompany them. 

## Machine learning as statistics in reverse

One of Hacking's core arguments in _The Taming of Chance_ is that modern statistical thought transposed what had initially been a way of thinking about errors in measurement, and particularly astronomical observations, into real quantities, typically described by the normal distribution. Much could be said about the normal distribution and I turn to some of that below. But for the moment, the important point is that this transposition or inversion relied on several intermediate steps passing through probability calculus (particularly the work of Jacob Bernoulli and the binomial or heads-tails probability distribution in the 1690s [@Hacking_1975, 143]), on large numbers of measurements (the most famous being the chest measurements of soldiers in Scottish regiments, but these were only one flurry amidst an avalanche of numbers in the 1830-1840s), on an idea of many independent but minute causes producing events (particularly as developed in medicine but also in studies of crime), and the law of errors applying to measurements made by, amongst others, astronomers [@Hacking_1990, 111-112]. As Hacking points out, coins, suicides, crime, chest measurements, and astronomical observations all come together in a picture of statistical stability which remains, although somewhat blurred, indelibly legible in contemporary statistical thought. In this entanglement, observers and the observed changed places. The distribution of errors made by astronomers measuring the position of stars or planets because a distribution or variation inherent in things.  All of this seems a long way from machine learning, and in terms of years, the work of figures such as Poisson, Laplace, Quetelet and even Galton, is well-removed. In terms of tables and functions (the concerns of the preceding two chapters), the distance is not so great. There is greater variety in tables (partly due to the common vector space) and in functions, but the entwining or even swapping between what relates to an observation and what concerns the real, continues. Machine learners engage in that swapping or re-distributing of numbers all the time. Viewed from the standpoint of a Hacking, machine learning reverse-engineers the invention of modern statistical thinking.  It takes back the 'real quantities' that modern statistics had attributed to the populations of the world and puts them into devices, machine learners 
that people then observe, monitor and indeed measure again in many ways. The direct swapping between uncertainty in measurement and variation in real attributes that statistics achieved now finds itself re-routed and intensified because machine learning measures devices. 

This swapping or re-distribution is not a simple operation of attribution, as if machine learners somehow mistake a measurement for the world. Machine learning does take statistical thinking as given. When Hastie and co-authors write (as we saw in the last chapter) 'our goal is to find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@Hastie_2009, 28], they invoke the 'real quantities' first elaborated and articulated by statistical thinkers such as Quetelet. At the same time, the major structuring differences in machine learning as a field of knowledge-practice show the marks of this commitment to the reality of the statistical. 

----        -----
supervised  unsupervised
generative  discriminative
parametric  non-parametric
prediction  inference
bias        variance
-----------------------------------

Table: Some structuring differences in machine learning


Every text on machine learning is structured by this basic set of contrasts or indeed oppositions. The contrasts shown in \ref{table:} all have a statistical facet and sometimes anchoring  to them. Some refer to variations and errors (bias and variance), some refer to the underpinning statistical intuition in particular techniques (e.g. Naive Bayes or Latent Dirichlet Allocation are _generative_ models whereas logistic regression or support vector machines are _discriminative_), and others indicate different kinds of statistical knowledge (prediction seeks to anticipate while inference seeks to interpret, etc.). These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects and computer code associated with $N=\forall X$. Because they  anchor basic elements of  machine learning, a much more profuse set of techniques and formalisms derived from statistics more generally populate the field and organise its knowledge of its own techniques and its orientation to the worlds of industry, agriculture, earth science, genomics, etc. Reading and working with machine learning techniques usually means encountering and responding to some of that statistical apparatus drawn from statistics, but these are not typically the statistical tests of significance or variation. In contrast to a statistics textbook as the widely used _Basic Practice of Statistics_ [@Moore_2009] or even a more advanced guide such as _All of Statistics_ [@Wasserman_2003], where statistics (t-test, chi-squared test, etc.) hypothesis testing, and analysis of uncertainties (confidence intervals, etc) order the exposition, the machine learning texts invoke a thoroughly probabilistic conceptual apparatus, without much of the practice found in statistics. Statistical underpinnings may be fundamental, but this does not mean that  machine learners simply automate statistics.[^4.4]

[^4.4]: Leo Breiman writing in 2001 during the heyday of academic development of machine learning argues, describes the 'two cultures' of statistics: 'in the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling' [@Breiman_2001a, 200].

While these structuring differences are practically very important, and deeply shape certain kinds of practice in machine learning, the underlying component that allows swapping between knowledge and the world, between measurements and events is probability thinking, and in particular, the functions that describe variations in probability, probability distributions. Historically, two distributions loom large. The binomial distribution was explored extensively in the seventeenth and eighteenth centuries in the context of games of chance [@Hacking_1975, 57-134]. The normal distribution pervades nineteenth century statistical thinking as it generalizes across law, medicine, agriculture, finance and not least, sociology. In all of these settings, probability distributions are a common way of showing and  talking about _random variables_ in statistics. These distributions appear in countless shapes and forms in scientific, government and popular literature of many different kinds. Statistical graphics have a rich history and semiology that I do not discuss here (see [@Bertin_1983]). Perhaps the most famous function or mapping is the normal or Gaussian distribution:

\begin {equation}
\label {eq:gaussian_distribution}
f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end {equation}


The  function shown in equation (\ref{eq:gaussian_distribution}) is the so-called normal or Gaussian distribution. Its  mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed 'one of the major success stories in the history of science' [@Stigler_1986, 158], and it has a power-laden biopolitical history closely tied with knowledges and governing of  national and other  populations in terms of morality, health, and wealth (see [@Hacking_1975, 113-124]. The key symbols here include $\mu$, the mean and $\sigma$, the variance, a number that describes how widely dispersed the values of the variable, $x$ are. These two parameters together describe the shape of the curve. Given $\mu$ and $\sigma$, it become possible to map different outcomes to probabilities. Put more statistically, viewed in terms of functions such as the Gaussian distribution, events become random variables. Following the kind of definition that we find in a standard statistics textbook, every variable potentially becomes a function: 'a random variable is a mapping that assigns a real number to each outcome' [@Wasserman_2003,19]. The possibility of treating all variables as random variables, that is, as probability distributions was a significant historical achievement, but one that continues to develop. We might note the real power of probability distributions when conceptualised as real quantities in the world, not epiphenomenal by-products of inaccuracies in our observations or measuring devices. For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak in Figure 2 (and hence less than the population mean), they quickly render them somehow subject to a potentially institutionally and economically consequential trajectory. This is not a recent development. Since its inception in the social physics of Quetelet as a way of referring to a property of populations, the normal curve has promised or threatened to support the re-shaping and control of populations (in terms of health, morality and wealth). Since then, probability distributions and their corresponding curves have multiplies. Subsequent statisticians used dozens of different probability distributions to map continuous and discrete variations to real numbers. Other probability distributions abound — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see [@NIST_2012] for a gallery of distributions) — because outcomes occur in widely differing patterns. The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Queues are usually modelled using a Poisson distribution, which unfortunately for travellers, distributes waiting times very differently.  Similarly, it might be better to think of the probability of rain today in Lancaster in terms of a Poisson distribution that models that queue of clouds in the Atlantic just waiting to land on the northwest coast of England. Rather than addressing the question of if it will rain or not, a Poisson-based model might address the question of how soon.

If functions such as equation (\ref{eq:gaussian_distribution}) have persisted for so long as operational underpinnings, what happen to them in machine learning? The pages of a book such as _Elements of Statistical Learning_ show many signs of this ongoing re-distribution of distributions. At a broad level, we could simply note their abundance. Hastie and co-authors diversely invoke probability distributions. They speak of 'Gaussian mixtures,' 'bivariate Gaussian distributions,' standard Gaussian,' 'Gaussian kernels,' 'Gaussian errors', 'Gaussian assumptions,' 'Gaussian errors,' 'Gaussian noise,' 'Gaussian radial basis function,' 'Gaussian variables,' 'Gaussian densities,' 'Gaussian process,' and so forth. The term 'normal' appears in an even wider spectrum of similar guises, but they uniformly entail treating events, things, properties or attributes as probability distributions. The wide distribution of the Gaussian function does not mean that one function, the Gaussian probability density function reigns supreme above all machine learners. While heavily reliant on the normal distribution, much machine learning seeks to loosen that reliance by, for instance, developing _non-parametric models_,  

The diverse curves of probability distributions — and we will see below some reasons why we can expect them to proliferate in certain settings — attests to the variety of ways in which events might be mapped to real numbers. Despite  the sometime forbidding mathematical equations, the term *distribution* emphasises a quite material or tangible way of thinking about how events or possible outcomes vary as a function controlled by parameters. Different machine learning techniques adjust these parameters in different ways. For instance, parametric and non-parametric models differ  in that the former have a limited number of parameters and the latter an undefined number of parameters (not no parameters). But the critical point is the assumption that an underlying probability distribution is in principle ‘unobservable’ or perhaps itself changing. A probability density function under these assumptions becomes the closest reality we have to whatever process generated  all the variations in data gathered through experiments and observations. From a probabilistic perspective, the task of machine learning is to estimate the parameters (the mean $\mu$ and variance $\sigma$ in the case of Gaussian curve) that shape of the curve of the probability distribution. Given the shape of that curve, many inferences and predictions become possible. The probability distribution function is a crucial control surface for machine learning understood as a form of movement through data. 

## Naive Bayes and the distribution of probabilities

What does this mean in practice? The mathematical expression for one of the most popular of all machine learning classifiers, the Naive Bayes classifier, is:

\begin {equation}
\label {eq:naive_bayes}
f_j(X) = \prod_{k=1}^{p}f_{jk}(X_k)
\end {equation}

>[@Hastie_2009, 211]

Some machine learning techniques are so simple that they can be implemented in a few lines of code. Their simplicity, however, belies their power. The function shown above in equation (\ref{eq:naive_bayes}) is about the simplest one to be found in most machine textbooks. While the Naive Bayes classifier is one of the most popular machine learning algorithms, it is more than 50 years old [@Hand_2001]. The key diagrammatic elements of the classifier as expressed in the equation are $\prod$, an operator that multiplies all the values (from $1$ to $p$) to generate a product. What product does the Naive Bayes classifier produce? The expression $f_j(X)$ refers is a probability density; that is, it describes the probability that a particular thing (a document, an image, an email message, a set of URLs, etc.) belongs to the class of things $j$. In constructing this estimate of the probability that a thing is an instance of class $j$, $p$ different features of the thing are taken into account. The subscripts $k=1$ on the $\prod$ operator, and $k$ on the data $X_k$ indicate that the Naive Bayes classifier makes use of a series of probabilities in calculating the overall probability that a given thing belongs to a class. The classifier produces a product $f_j(X)$ by calculating the _joint probability_ of all the _conditional_ probabilities of the features or predictor variables in $X$ for the class $j$. As  _Elements of Statistical Learning_ rather tersely puts it, 'each of the class densities are products of the marginal densities' [@Hastie_2009,108]. We could note that  almost everything about the  Naive Bayes classifier concerns probability (including its name, with its reference to the Bayes Theorem, an important late eighteenth century concept), yet there is little obvious connection to statistics in its modern form of tests and hypotheses. As Drew Conway and John Myles-White write in _Machine Learning for Hackers_, 

>At its core, [Naive Bayes] ... is a 20th century application of the 18th century concept of _conditional probability_. A conditional probability is the likelihood of observing some thing given some other thing we already know about [@Conway_2012, 77] 

They point here to the role of 'conditional probability,' a kind of probability that lies at the heart of many of the data transformation associated with prediction or pattern recognition. As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (again dating from the eighteenth century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation (hence the name 'naive' Bayes or previously 'idiots' Bayes). The classifier has a simple probabilistic architecture based on the concepts of conditional probability and joint probability; it calculates a probability density function $f_j(X)$ for each class of things. Its architecture is simple, however, because it makes a drastically reductionist assumption that features are independent of each other (hence the name 'naive'), where 'independent' means that they do not affect each other, or that they have no relation to each other. We will see below that dramatic simplifications do not necessarily weaken the referential grasp of machine learners on the world, but in certain ways allow them to relate to it more directly.

While the equation shown in (\ref{eq:naive_bayes}) does not set out all the steps in transforming some training data into a predictive model, the lines of code needed to do this are relatively brief. In _Doing Data Science_, Rachel Schutt and Cathy O'Neill furnish a bash script (that is, command line instructions) to download a well-known email dataset and build a Naive Bayes classifier for spam. In many ways, this a canonical machine learner pedagogy, and for Naive Bayes, email spam detection is the standard example. Nevertheless, the completeness of the small script, reproduced below, in fetching the Enron email dataset, calculating the 'marginal densities' or conditional probabilities for each word given how often it is associated with either spam or non-spam email, and then outputting the probability that a particular is spam, is striking.[^4.7] In the space of these few dozen lines, some part of a world -- the Enron email dataset -- becomes a predictive engine for dealing with decisions in the world: is an email spam or not?

A typical spam email in the Enron dataset looks like this:

> Subject: it's cheating, but it works !
can you guess how old she is ? the woman in this photograph looks like a happy teenager about to go to her high school prom, doesn' t she ? she' s an international, professional model whose photographs have appeared in hundreds of ads and articles whenever a client needs a photo of an attractive, teenage girl.but guess what ? this model is not a teenager ! no, she is old enough to have a 7-year-old daughter.. and...the model' s real age is in her 30' s.all she will say about her age to her close friends is, " i'm dangerously close to 40." she also says, " if it weren't for this amazing new cosmetic cream called ' deception, ' i would lose hundreds of modeling assignments...because...there is no way i could pass myself off as a teenager." learn more about this amazing new product...please refer all questions, opinions or additional feedback to :
service dept
9420 reseda blvd # 133
northridge, ca 91324

The text of a typical non-spam email like this:

> Subject: industrials suggestions......
----------------------forwarded by kenneth seaman / hou / ect on 01 / 04 / 2000
12 : 47 pm-------------------------- -
pat clynes @ enron
01 / 04 / 2000 12 : 46 pm
to : kenneth seaman / hou / ect @ ect, robert e lloyd / hou / ect @ ect
cc :
subject : industrials
ken and robert,
the industrials should be completely transitioned to robert as of january 1, 2000.please let me know if this is not complete and what else is left to transition .
thanks, pat

These are both recognisable things in the world for anyone who uses email. How do they become $X$ or even $f_j(X)$ in the Naive Bayes classifier? How do words in a document because probability distributions? The code is instructive:

[^4.7]: The input to the script is a single word such as 'finance' or 'deal'. The model is so simple that it only classifies a single word as spam. The `bash` script carries out four different transformations of the data in building the model. It uses only command line tools such as `wc` (word count), `bc` (basic calculator), `grep` (text search using pattern matching) and `echo` (display a line of text). These tools or utilities are readily available in almost any UNIX-based operating system (e.g. Linux, MacOS, etc). The point of using only these utilities is to illustrate the simplicity of the algorithmic implementation of the model.  The first part of the code downloads the sample dataset of Enron emails (and I will discuss spam emails and their role in machine learning below). Note that this dataset has already been divided into two classes - 'spam' and 'ham' -- and emails of each class have been placed in separate directories or folders as individual text files.

```{r enron_nb_bash, engine='bash', echo=TRUE}
    #!/bin/bash
    # file: enron_naive_bayes.sh
    # description: trains a simple one-word naive bayes spam
    # filter using enron email data
    # usage: ./enron_naive_bayes.sh <word>
    # requirements:
    #    wget
    #
    # author: jake hofman (gmail: jhofman)
    # how to use the code
    if [ $# -eq 1 ]
    then
        word=$1
    else
        echo "usage: enron_naive_bayes.sh <word>"
        exit
    fi

     ### PART 1
    if ! [ -e enron1.tar.gz ]
    then
        wget 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'
    fi

    if ! [ -d enron1 ]
    then
        tar zxvf enron1.tar.gz
    fi

    cd enron1

    ### PART 2
    Nspam=`ls -l spam/*.txt | wc -l`
    Nham=`ls -l ham/*.txt | wc -l`
    Ntot=$Nspam+$Nham
    echo $Nspam spam examples
    echo $Nham ham examples

    Nword_spam=`grep -il $word spam/*.txt | wc -l`
    Nword_ham=`grep -il $word ham/*.txt | wc -l`
    echo $Nword_spam "spam examples containing $word"
    echo $Nword_ham "ham examples containing $word"

    ### PART 3
    Pspam=`echo "scale=4; $Nspam / ($Nspam+$Nham)" | bc`
    Pham=`echo "scale=4; 1-$Pspam" | bc`
    echo
    echo "estimated P(spam) =" $Pspam
    echo "estimated P(ham) =" $Pham
    Pword_spam=`echo "scale=4; $Nword_spam / $Nspam" | bc`
    Pword_ham=`echo "scale=4; $Nword_ham / $Nham" | bc`
    echo "estimated P($word|spam) =" $Pword_spam
    echo "estimated P($word|ham) =" $Pword_ham

    ### PART 4
    Pspam_word=`echo "scale=4; $Pword_spam*$Pspam" | bc`
    Pham_word=`echo "scale=4; $Pword_ham*$Pham" | bc`
    Pword=`echo "scale=4; $Pspam_word+$Pham_word" | bc`
    Pspam_word=`echo "scale=4; $Pspam_word / $Pword" | bc`
    echo
    echo "P(spam|$word) =" $Pspam_word
    cd ..
```

[@Schutt_2013, 105-106]

```{r enron_nb_r, echo = FALSE}

        f_ham = list.files('enron1/ham', full.names = TRUE)
        f_spam = list.files('enron1/spam', full.names = TRUE)
        ham_count = length(f_ham) 
        spam_count = length(f_spam)

        ham = sapply(f_ham, readLines, warn=FALSE)
        spam = sapply(f_spam, readLines, warn=FALSE)
        P_ham = ham_count/(ham_count + spam_count)
        P_spam = spam_count/(ham_count + spam_count)
        word = 'gas'

        predict_spam <- function(word) {
            ## count the occurrence of the word in each
            Nword_in_ham = sum(grepl(word, ham))
            Nword_in_spam = sum(grepl(word, spam))
            #cat(Nword_in_ham, ' ham examples contain ', word,  '\n')
            #cat(Nword_in_spam, ' spam examples contain ', word,  '\n')

            #cat('estimated P(spam) = ', P_spam,  '\n')
            #cat('estimated P(ham) = ', P_ham,  '\n')

            Pword_spam = Nword_in_spam/spam_count
            Pword_ham = Nword_in_ham/ham_count
            #cat("P(spam|", word, ") = ", Pword_spam,  '\n')
            #cat("P(ham|", word, ") = ", Pword_ham,  '\n')

            Pham_word = Pword_ham * P_ham
            Pspam_word = Pword_spam * P_spam
            Pword = Pspam_word + Pham_word
            Pspam_word = Pspam_word/Pword

            #cat("P(spam|", word, ")=",  Pspam_word,  '\n')
            return(Pspam_word)
        }
```

After fetching the dataset from a website, the code counts the number of emails in each category and prints them, and the counts the number of times that the chosen word (e.g. 'finance' or 'deal') occurs in both the spam and non-spam or ham categories. Using these counts, the script  estimates probabilities of any email being spam or ham, and then given that email is spam or ham, that the particular word occurs. To estimate a probability means, in this case, to divide the word count for the chosen word by the count of the number of spam emails, and ditto for the ham emails.  In Part 4, the final transformation of the data, these probabilities are used to calculate the probability of an email being spam given the presence of that word. Again, the mathematical operations here are no more complicated than adding, multiplying and dividing. The probability that the chosen word is a spam word is, for instance, the probability of occurrence of the word in a spam email multiplied by the overall probability that an email is spam. Finally, given that the overall probability of the chosen word occurring in the email dataset is the probability of it occurring in spam plus the probability of it occurring in ham, the overall probability that an email in the Enron data is spam given the presence of that word can be calculated. It is the probability that the chosen word is a spam word divide by the probability of that word in general. 

The point of this slightly bamboozling and mechanical description of what the script does is to show something of how the diagrammatic form of the function written as a joint probability in equation (\ref{eq:naive_bayes}) moves into the world. Not all machine learning models are so simple that they can be conveyed in 30 lines of code (including downloading the data and comments). The advantage of this script is that it shows that nothing that occurs in relation to probability, probability distributions or classification is intrinsically mysterious, elusive or indeed particularly abstract. On the contrary, everything here is counting, adding, multiplying (that is, repeated adding) and dividing (that is, multiplying by parts or fractions). Everything is constrained by a certain re-distribution of things such as emails or documents as, in this case, events in a population of words. Every word found in the Enron datasets becomes a probability density, and the classification of each email becomes a matter of estimating a conditional probability based on the joint probability of events occurring together.  Probabilities are always between `0` and `1`, or between 0% and 100%, and classification entails selected a cutoff or dividing line. For instance, greater than `0.5` might result in a classification.   'Finance' has a  `r round(predict_spam('finance'),2)` chance of being spam, while 'sexy' has a chance of `r round(predict_spam('sexy'), 2)`, at least in the Enron emails.   The transformation of textual forms -- the emails -- into probabilistic distributions is practically straightforward. Ironically, like the Naive Bayes classifier's own reliance on seventeenth and eighteenth century probability calculus, the frequent application of this machine learner to document classification and retrieval echoes the seventeenth century thinking   that first conceived of the very notion of 'probability' in relation to the evidential weight of documents [@Hacking_1975, 85]. 

## The improbable success of the Naive Bayes classifier

Like most machine learning techniques applied to texts, classifiers such as Naive Bayes ignore many obvious structural features of emails as documents (for instance, word order, or co-occurrences of words). Yet this very artificiality or limitation of its reference to the world is typical. The contrast with modern statistical practice starts to appear in Naive Bayes and only intensifies in the ongoing development of machine learning classifiers (for instance, in topic models). On the one hand, like statistics more generally, Naive Bayes treats the world probabilistically. The world comprises populations, and populations generate events. Emails have a real probability of being spam, and this probability is acted on many times every day for email users. On the other hand, there is something quite artificial or contrived about these populations and their associated probability distributions.  They are intentionally artificial and limited; they do not correspond or refer directly to what we know, for instance, of how language works, but instead to a rather different set of concerns that remain statistical but somehow less directly invested in the reality of, for instance, the normal distribution as the defining property of the population.

Hastie, Tibshirani and Friedman characterise the value of the  Naive Bayes technique in relation to its capacity to deal with high dimensional data[^4.20] :    


>It is especially appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class $G = j$, the features $X_k$ are independent [@Hastie_2009, 211]. 

[^4.20]: See Chapter 2 on this notion of data dimensionality.

In equation (\ref{eq:naive_bayes}), $p$ stands for the number of different dimensions or variables in the data set. In the spam classifier, the number of dimensions is probably quite large because every unique word adds a new dimension to the 'feature space.' By contrast, there are are only two classes $G$, spam and non-spam.  Yet compared to the complications of logistic regression, neural networks or support vector machines, \ref{eq:naive_bayes}  seems incredibly simple. How is it that a simple multiplication of probabilities can, as Hastie and co-authors write: 'often outperform far more sophisticated alternatives' [@Hastie_2009, 211]. Similar formulations can  be found in most of the machine learning books and instructional materials currently available.

I want to suggest two different ways to view the improbable success of Naive Bayes. Both are very germane to the broader question we are pursuing of how machine learning changes the 'taming of chance' accomplished by modern statistics. The first way to view this success is in terms of _ancestral communities_ of classification, inference and prediction.  Along with linear regression, Naive Bayes might be _the_ standard introductory machine learning technique, where the Naive Bayes classifier is almost always demonstrated to the problem of filtering spam email [@Conway_2012; @Schutt_2013, 93-113; @Kirk_2014, 53; @Lantz_2013, 92-93; @Flach_2012, @Ng_2008b], and in particular dealing with the abundance of spam emails concerning Viagra (itself a byproduct of the failure of a statistical model). It is remarkable how often Naive Bayes comes up in machine learning textbooks especially given how simple the technique is. Admittedly spam, and spam trying to sell Viagra in particular, has been a very familiar part of most email users lives for a decade and more. In practice, Naive Bayes classifiers and variations of them have become an integral part of managing email traffic for most people, whether they know it or not. And large sample datasets of email tagged as 'spam' or 'ham' are widely available.

I wonder, however, whether the constant reiteration of email spam filtering using Naive Bayes stands in as some kind of prototypical learning about machine learning situation, and in this learning situation, the application of probability to language and texts has a central importance,[HERE]

## Ancestral communities of documents

[stuff here from suchman, on maron etc]

## High bias, low variance and observation of errors

A much broader way to view the persistence of a manifestly reductionist model, a model that manifestly eschews any modelling of relations between things in the word such as words, is in terms of another of the structuring differences of machine learning: bias and variance. These terms overtly statistical, and they are heavily used in machine learning discussions of model performance and application. 

## Generative: the model makes the data?

[stuff here from alpaydin; deals with generative; etc]

The quantification entailed in statistics is a counting of events that have been named or labelled in some way. The perhaps more profound point is that this counting of atomic events can be combined in a seemingly limitless variety of combinations. In the emails, individual words are events, and therefore each email is a complicated composite event. Similarly, in machine learning on images, each pixel could be treated as an event, and the image as a whole becomes an immensely complicated aggregate colour and light event. As we will see, many other machine learning techniques try to deal with this combinatory character of composite events directly. 




There is some new operating terminology in this chapter (terms such as random variable, probability distribution and likelihood).  While I attempt to be both mathematically and conceptually concise in my use of these terms, the terms themselves are somewhat troubled by the transformations I'm describing. The underpinnings of taken-for-granted and everyday statistical such as random variable or probability are not immune from change. Like all technical formalisms they took hold at a certain practical and historical conjuncture that will not and perhaps already does not hold entirely still. As the philosophy Ian Hacking suggests in his discussion of early 20th century changes in probability in _The Taming of Chance_,

>By the 1930s, however, the world teemed with frequencies, and the 'objective' notion would come to seem more important than the 'subjective' one for the rest of the century -- simply because there were so many more frequencies to be known [@Hacking_1990, 97].

I'm suggesting that we countenance a scene in which multiple different probability practices stack on top of each other in a somewhat untamed way. Amidst these, random variables and their associated probability distributions particularly concern us. As we will see, the power of the transformation in probability associated with machine learning algorithms resides in their capacity to draw in many more relations, features and components of data in support of a probabilistic outcome. When things are best described as probability distributions, they take on a different form of temporal and multiplicative existence, and this no longer easily attributed to either 'subjective' or 'objective' probability, or to a shift in balance between the long-standing poles of 'subjective' and 'objective' probability. A probabilistically generated airline seat price attracts different kinds of transactions than a fixed priced seat. Similarly, a probabilistically generated tumour classification implies different modes of responsiveness and care. Gaining some understanding of the concrete arrangements of forces in these probabilistic modes might allow us to account for the ways in which certain methods -- Bayesian inference is a striking example of transverse momentum of methods across fields  -- go on the move, or why certain problems -- automatic text classification, image recognition, etc -- suddenly hove into feasibility.

The chapter traces two important implications of probabilistic model for machine learning. First, because it is so computationally intensive, MCMC and Bayesian inference, although statistically powerful, are difficult to apply to many dimensional datasets. So Bayesian computation iconically figures the limits of contemporary data practices, with their ambitions to incorporate all available data into calculation. Second, in certain ways this technique challenges us to re-evaluate how we think about numbers. By following some of the ways numbers circulate through MCMC algorithms, we can discern to a semiotic-material faultline running through contemporary number formations. Numbers semiotically and materially embrace both events and degrees of belief. If numbers are crucial in the data economy, then instabilities in their mode of existence will affect much of what happens to data. While much of the machine learning taking place in commercial and operational settings is decidedly non-Bayesian, the popularity of MCMC and Bayesian approaches in contemporary sciences suggests a tension in what counts as number.

[HERE]

Certain strands of social and cultural theory have taken a strong interest in algorithmic processes. For instance, the sociologist Scott Lash distinguishes  the operational rules found in  algorithms from the regulative and constitutive rules in many social settings and studied by social scientists:

>in a society of pervasive media and ubiquitous coding, at stake is a third type of rule, algorithmic, generative rules. ‘Generative’ rules are, as it were, virtuals that generate a whole variety of actuals. They are compressed and hidden and we do not encounter them in the way that we encounter constitutive and regulative rules. Yet this third type of generative rules is more and more pervasive in our social and cultural life of the post-hegemonic order. They do not merely open up opportunity for invention, however. They are also pathways through which capitalist power works, in, for example, biotechnology companies and software giants more generally [@Lash_2007a, 71].

The term 'generative' is somewhat resonant in the field of machine learning as generative models, models that treat modelling as a problem of specifying the operations or dynamics that could have given rise to the observed data, are extremely important. If we consider only Andrew Ng's CS229 machine learning lectures  on Youtube [@Ng_2008], we can see that they introduce generative models in Lecture 5 and 6. Although this seems to be only a small part of the 18 lectures given in the course, later lectures on the expectation maximisation algorithm (12-13), and then on unsupervised learning techniques such as factor analysis and principal component analysis, independent component analysis, are also effectively exploring generative models.  A similar distribution of topics can be found in _Elements of Statistical Machine Learning_[@Hastie_2009].   Generative models, while perhaps slightly less common in practice than discriminative models, nevertheless capture the sense that algorithms are not just implementations of rules for filtering, sorting, or deciding, but carry within them ontological commitments that might actually challenge social theory in interesting ways. In contrast to Lash, I would suggest that the generativity of these algorithms needs to be differentiated from the algorithmic processes that implement rules more generally. Moving into the data via a generative probabilistic model is very different to moving into the data through say a database query. The models, whether generative or discriminative (models  such as decision tree,  logistic regression or even neural networks that are more limited in their probabilistic underpinnings), are more like meta-algorithms that reorganize other algorithmic processes on varying scales. 

## Conclusion

Machine learning inhabits worlds that had already become fundamentally statistical at least a century earlier, whether through the social physics of Quetelet, the biopolitical normals of Francis Galton and his regression to the mean (remember that the linear model of regression is probably the basic machine learning model) or later in the probability functions of quantum mechanics in early twentieth century physics. I have been suggesting the machine learning should be understood as a reversal or an inversion of statistical thought. In this inversion, the probability distributions that had become the underlying reality of many different kinds of populations fold back or re-distribute themselves in devices such as machine learners whose variations and uncertainties powerfully generate classifications, predictions and knowledge statements (scientific and otherwise).

The re-distribution of probability that takes place in machine learning through multiple-swapping of observational positions bears several implications for thinking about the power of the techniques. The lure of the techniques is a kind of trap since the swapping or reverse of statistical thought takes what was put into the world by modern statistical thought and puts it into machines whose intricate workings themselves become the event to be observed. At the same time, although taking back the real quantities, the machine learners remain closely in contact with worlds populated by texts, documents, images, prices, sensor measurements, transactions and records. 

The underlying instabilities of probabilistic thought remain in play. 

## References
