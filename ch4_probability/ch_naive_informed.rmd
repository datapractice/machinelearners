
\chapter{$N = all$: machine learning gets all the data}
\label{ch:probability}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, 
                      echo=FALSE, warning=FALSE, message=FALSE, dev='pdf')
```

## Introduction

In the final pages of _The Taming of Chance_, the philosopher Ian Hacking describes the work of the geodesic philosopher C.S. Peirce in terms of a twin affirmation of chance. On the one hand, Peirce, following the work of the psychophysicist Gustav Fechner and before him the astronomer-sociologist Adolphe Quetelet, makes the normal curve into an underlying reality.[^4.1] The 'personal equation,' the variation in measurements made by any observer, become 'a reality underneath the phenomena of consciousness' [@Hacking_1990, 205]. At the same time, and in order to show the underlying reality of the curve, 'Peirce deliberately used the properties of chance devices to introduce a new level of control into his experimentation. Control not by getting rid of chance fluctuations, but by adding some more' [@Hacking_1990, 205]. Peirce's belief in absolute chance, 'a universe of chance' as Hacking puts it, came in the way of a series of 'realization' of curves, in which first social, then biological and finally psychological variations were all understood as evidence of a generative function, the normal distribution or Gaussian function. In the century or so since, what happened to the thorough-going affirmation of statistical thought and probabilistic practice epitomised by Peirce? Hacking stresses that he does not understand Peirce as the precursor or the innovator of twentieth century statistical thought (Hacking's _Taming of Chance_ ends at 1900), but rather as 'the first philosopher completely to internalize the way chance had been tamed in the nineteenth century' (215). What would the equivalent philosopher-machine learner internalize today? What would such persons, working in science or media or government, hold firm in relation to chance, probability and statistics? 

[^4.1]: The historian of statistics Stephen Stigler provides a lengthy account of Fechner's work in [@Stigler_1986, 239-259].

In a broad sense, the setting that concerned Peirce in his work at the U.S. Government Coast Survey in the 1870s does not differ greatly from the setting that machine learners encounter. In the opening lines of the First Edition of _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman write:

>The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope [@Hastie_2009, xi]

At the end of the preface, they also cite, we might note in passing, Hacking's work: 'The quiet statisticians have changed our world' [@Hastie_2009, xii]. With some justification, we might ask therefore: what difference do the 'vast amounts of data ... generated in many fields' (xi) make to what machine learners internalize of their world? This question of what kind of world becomes thinkable through machine learning can be addressed partly by contrasting the 'taming of chance' achieved during the eighteenth and nineteenth centuries, and the statistical practices of machine learning today. Is machine learning a further taming of chance? What role does randomness and probability play in machine learning?

The broadest claim associated with statistical machine learning might be the simple expression shown in:

\begin {equation}
\label {eq:n_all}
N = \forall X
\end {equation}


In Equation \ref{eq:n_all}, $N$ refers to the number of observations (and hence the size of the dataset), the symbol $\forall$ means 'all' since this is the level of inclusion which many fields of knowledge in science, government, media, commerce and industry envisage, and $X$ refers to the data itself arrayed in common vector space. Note that this expression leaves some things out. $Y$, the response variable, for instance, may or may not be known.  While statistical techniques and practices have appeared in previous chapters, I focus here  on changes in probability practices associated with machine learning, and in particular, $N = \forall $X, the claim that with all the data, statistical thinking and the potentials of the statistical reasoning fundamentally change. The claim that with $N=\forall X$ everything changes has been widely discussed.[^4.2]  Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different ways in the course of the vignettes and comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data:

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]

In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is never really discussed. While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they do say that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of specific techniques of data crunching or the math is largely left aside. This is not to criticize a book that sets out to describe trends affecting business and government for a general readership, but without a sense of how statistical thinking animates almost all salient features of crunching the data and particularly the predictions, it becomes hard to see how the 'revolution' takes place. In other words, the shift from $N=n$ (some of the data) to $N=\forall X$ does not occur without other transpositions and rearrangements that do not simply concern choices about how much data to use, but also concern how data is given in the world and how it is thinkable.[^4.3] Following Hacking's core argument about how nineteenth century statistics transformed measurements (for instance, the mean as average of all measured values) into real quantities (for instance, mean as the ideal or abstract property of a population; e.g. life expectancy), we might see this shift between $n$ and $\forall X$, a shift very much animating and dependent on  machine learning, as an event akin to the advent of the Normal distribution (and indeed, $\mathnormal{N}$ is a standard symbol for the Normal distribution in statistics textbooks) as a way of thinking about populations and the control of populations [@Hacking_1975, 108]. Aligning the development of machine learning with the longer duration of statistical thinking might allow us in short to gauge a little better what is changing and where. 

[^4.2]: Rob Kitchin provides a very useful overview of these claims in [@Kitchin_2014]. While I will not analyse the claims about 'big data' in specific cases in any great detail, I  

[^4.3]: Part of this development has already been related in the previous chapter on 'learning' and function estimation. That chapter avoided any real discussion of statistical thought. Instead it explored the various sense of function and function finding that underpin machine learning. But much of that function finding and approximation garners referential weight through the statistical practices and modes of thought that accompany them. 

## Machine learning as statistics in reverse

One of Hacking's core arguments in _The Taming of Chance_ is that modern statistical thought transposed what had initially been a way of thinking about errors in measurement, and particularly astronomical observations, into real quantities, typically described by the normal distribution. Much could be said about the normal distribution and I turn to some of that below. But for the moment, the important point is that this transposition or inversion relied on several intermediate steps passing through probability calculus (particularly the work of Jacob Bernoulli and the binomial or heads-tails probability distribution in the 1690s [@Hacking_1975, 143]), on large numbers of measurements (the most famous being the chest measurements of soldiers in Scottish regiments, but these were only one flurry amidst an avalanche of numbers in the 1830-1840s), on an idea of many independent but minute causes producing events (particularly as developed in medicine but also in studies of crime), and the law of errors applying to measurements made by, amongst others, astronomers [@Hacking_1990, 111-112]. As Hacking points out, coins, suicides, crime, chest measurements, and astronomical observations all come together in a picture of statistical stability which remains, although somewhat blurred, indelibly legible in contemporary statistical thought. In this entanglement, observers and the observed changed places. The distribution of errors made by astronomers measuring the position of stars or planets because a distribution or variation inherent in things.  All of this seems a long way from machine learning, and in terms of years, the work of figures such as Poisson, Laplace, Quetelet and even Galton, is well-removed. In terms of tables and functions (the concerns of the preceding two chapters), the distance is not so great. There is greater variety in tables (partly due to the common vector space) and in functions, but the entwining or even swapping between what relates to an observation and what concerns the real, continues. Machine learners engage in that swapping or re-distributing of numbers all the time. Viewed from the standpoint of a Hacking, machine learning reverse-engineers the invention of modern statistical thinking.  It takes back the 'real quantities' that modern statistics had attributed to the populations of the world and puts them into devices, machine learners 
that people then observe, monitor and indeed measure again in many ways. The direct swapping between uncertainty in measurement and variation in real attributes that statistics achieved now finds itself re-routed and intensified because machine learning measures devices. 

This swapping or re-distribution is not a simple operation of attribution, as if machine learners somehow mistake a measurement for the world. Machine learning does take statistical thinking as given. When Hastie and co-authors write (as we saw in the last chapter) 'our goal is to find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@Hastie_2009, 28], they invoke the 'real quantities' first elaborated and articulated by statistical thinkers such as Quetelet. At the same time, the major structuring differences in machine learning as a field of knowledge-practice show the marks of this commitment to the reality of the statistical. 

----        -----
supervised  unsupervised
parametric  non-parametric
bias        variance
prediction  inference
generative  discriminative
-----------------------------------

Table: Some structuring differences in machine learning


Every text on machine learning is structured by this basic set of contrasts or indeed oppositions. The contrasts shown in Table \ref{table:} all have a statistical facet and sometimes anchoring  to them. Some refer to variations and errors (bias and variance), some refer to the underpinning statistical intuition in particular techniques (e.g. Naive Bayes or Latent Dirichlet Allocation are _generative_ models whereas logistic regression or support vector machines are _discriminative_), and others indicate different kinds of statistical knowledge (prediction seeks to anticipate while inference seeks to interpret, etc.). These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects and computer code associated with $N=\forall X$. Because they  anchor basic elements of  machine learning, a much more profuse set of techniques and formalisms derived from statistics more generally populate the field and organise its knowledge of its own techniques and its orientation to the worlds of industry, agriculture, earth science, genomics, etc. Reading and working with machine learning techniques usually means encountering and responding to some of that statistical apparatus drawn from statistics, but these are not typically the statistical tests of significance or variation. In contrast to a statistics textbook as the widely used _Basic Practice of Statistics_ [@Moore_2009] or even a more advanced guide such as _All of Statistics_ [@Wasserman_2003], where statistics (t-test, chi-squared test, etc.) hypothesis testing, and analysis of uncertainties (confidence intervals, etc) order the exposition, the machine learning texts invoke a thoroughly probabilistic conceptual apparatus, without much of the practice found in statistics. Statistical underpinnings may be fundamental, but this does not mean that  machine learners simply automate statistics.[^4.4]

[^4.4]: Leo Breiman writing in 2001 during the heyday of academic development of machine learning argues, describes the 'two cultures' of statistics: 'in the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling' [@Breiman_2001a, 200].

While these structuring differences are practically very important, and deeply shape certain kinds of practice in machine learning, the underlying component that allows swapping between knowledge and the world, between measurements and events is probability thinking, and in particular, the functions that describe variations in probability, probability distributions. Historically, two distributions loom large. The binomial distribution was explored extensively in the seventeenth and eighteenth centuries in the context of games of chance [@Hacking_1975, 57-134]. The normal distribution pervades nineteenth century statistical thinking as it generalizes across law, medicine, agriculture, finance and not least, sociology. In all of these settings, probability distributions are a common way of showing and  talking about _random variables_ in statistics. These distributions appear in countless shapes and forms in scientific, government and popular literature of many different kinds. Statistical graphics have a rich history and semiology that I do not discuss here (see [@Bertin_1983]). Perhaps the most famous function or mapping is the normal or Gaussian distribution:

\begin {equation}
\label {eq:gaussian_distribution}
f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end {equation}


The  function shown in equation (\ref{eq:gaussian_distribution}) is the so-called normal or Gaussian distribution. Its  mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed 'one of the major success stories in the history of science' [@Stigler_1986, 158], and it has a power-laden biopolitical history closely tied with knowledges and governing of  national and other  populations in terms of morality, health, and wealth (see [@Hacking_1975, 113-124]. The key symbols here include $\mu$, the mean and $\sigma$, the variance, a number that describes how widely dispersed the values of the variable, $x$ are. These two parameters together describe the shape of the curve. Given $\mu$ and $\sigma$, it become possible to map different outcomes to probabilities. Put more statistically, viewed in terms of functions such as the Gaussian distribution, events become random variables. Following the kind of definition that we find in a standard statistics textbook, every variable potentially becomes a function: 'a random variable is a mapping that assigns a real number to each outcome' [@Wasserman_2003,19]. The possibility of treating all variables as random variables, that is, as probability distributions was a significant historical achievement, but one that continues to develop. We might note the real power of probability distributions when conceptualised as real quantities in the world, not epiphenomenal by-products of inaccuracies in our observations or measuring devices. For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak in Figure 2 (and hence less than the population mean), they quickly render them somehow subject to a potentially institutionally and economically consequential trajectory. This is not a recent development. Since its inception in the social physics of Quetelet as a way of referring to a property of populations, the normal curve has promised or threatened to support the re-shaping and control of populations (in terms of health, morality and wealth). Since then, probability distributions and their corresponding curves have multiplies. Subsequent statisticians used dozens of different probability distributions to map continuous and discrete variations to real numbers. Other probability distributions abound — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see [@NIST_2012] for a gallery of distributions) — because outcomes occur in widely differing patterns. The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Queues are usually modelled using a Poisson distribution, which unfortunately for travellers, distributes waiting times very differently.  Similarly, it might be better to think of the probability of rain today in Lancaster in terms of a Poisson distribution that models that queue of clouds in the Atlantic just waiting to land on the northwest coast of England. Rather than addressing the question of if it will rain or not, a Poisson-based model might address the question of how soon.

If functions such as equation (\ref{eq:gaussian_distribution}) have persisted for so long as operational underpinnings, what happen to them in machine learning? The pages of a book such as _Elements of Statistical Learning_ show many signs of this ongoing re-distribution of distributions. At a broad level, we could simply note their abundance. Hastie and co-authors diversely invoke probability distributions. They speak of 'Gaussian mixtures,' 'bivariate Gaussian distributions,' standard Gaussian,' 'Gaussian kernels,' 'Gaussian errors', 'Gaussian assumptions,' 'Gaussian errors,' 'Gaussian noise,' 'Gaussian radial basis function,' 'Gaussian variables,' 'Gaussian densities,' 'Gaussian process,' and so forth. The term 'normal' appears in an even wider spectrum of similar guises, but they uniformly entail treating events, things, properties or attributes as probability distributions. The wide distribution of the Gaussian function does not mean that one function, the Gaussian probability density function reigns supreme above all machine learners. While heavily reliant on the normal distribution, much machine learning seeks to loosen that reliance by, for instance, developing _non-parametric models_,  

The diverse curves of probability distributions — and we will see below some reasons why we can expect them to proliferate in certain settings — attests to the variety of ways in which events might be mapped to real numbers. Despite  the sometime forbidding mathematical equations, the term *distribution* emphasises a quite material or tangible way of thinking about how events or possible outcomes vary as a function controlled by parameters. Different machine learning techniques adjust these parameters in different ways. For instance, parametric and non-parametric models differ  in that the former have a limited number of parameters and the latter an undefined number of parameters (not no parameters). But the critical point is the assumption that an underlying probability distribution is in principle ‘unobservable’ or perhaps itself changing. A probability density function under these assumptions becomes the closest reality we have to whatever process generated  all the variations in data gathered through experiments and observations. From a probabilistic perspective, the task of machine learning is to estimate the parameters (the mean $\mu$ and variance $\sigma$ in the case of Gaussian curve) that shape of the curve of the probability distribution. Given the shape of that curve, many inferences and predictions become possible. The probability distribution function is a crucial control surface for machine learning understood as a form of movement through data. 

## Naive Bayes and the distribution of probabilities

What does this mean in practice? The mathematical expression for one of the most popular of all machine learning classifiers, the Naive Bayes classifier, is:

\begin {equation}
\label {eq:naive_bayes}
f_j(X) = \prod_{k=1}^{p}f_{jk}(X_k)
\end {equation}

>[@Hastie_2009, 211]

Some machine learning techniques are so simple that they can be implemented in a few lines of code. Their simplicity, however, belies their power. The function shown above in equation (\ref{eq:naive_bayes}) is about the simplest one to be found in most machine textbooks. While the Naive Bayes classifier is one of the most popular machine learning algorithms, it is more than 50 years old [@Hand_2001]. The key diagrammatic elements of the classifier as expressed in the equation are $\prod$, an operator that multiplies all the values (from $1$ to $p$) to generate a product. What product does the Naive Bayes classifier produce? The expression $f_j(X)$ refers is a probability density; that is, it describes the probability that a particular thing (a document, an image, an email message, a set of URLs, etc.) belongs to the class of things $j$. In constructing this estimate of the probability that a thing is an instance of class $j$, $p$ different features of the thing are taken into account. The subscripts $k=1$ on the $\prod$ operator, and $k$ on the data $X_k$ indicate that the Naive Bayes classifier makes use of a series of probabilities in calculating the overall probability that a given thing belongs to a class. The classifier produces a product $f_j(X)$ by calculating the _joint probability_ of all the _conditional_ probabilities of the features or predictor variables in $X$ for the class $j$. As  _Elements of Statistical Learning_ rather tersely puts it, 'each of the class densities are products of the marginal densities' [@Hastie_2009,108]. We could note that  almost everything about the  Naive Bayes classifier concerns probability (including its name, with its reference to the Bayes Theorem, an important late eighteenth century concept), yet there is little obvious connection to statistics in its modern form of tests and hypotheses. As Drew Conway and John Myles-White write in _Machine Learning for Hackers_, 

>At its core, [Naive Bayes] ... is a 20th century application of the 18th century concept of _conditional probability_. A conditional probability is the likelihood of observing some thing given some other thing we already know about [@Conway_2012, 77] 

They point here to the role of 'conditional probability,' a kind of probability that lies at the heart of many of the data transformation associated with prediction or pattern recognition. As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (again dating from the eighteenth century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation (hence the name 'naive' Bayes or previously 'idiots' Bayes). The classifier has a simple probabilistic architecture based on the concepts of conditional probability and joint probability; it calculates a probability density function $f_j(X)$ for each class of things. Its architecture is simple, however, because it makes a drastically reductionist assumption that features are independent of each other (hence the name 'naive'), where 'independent' means that they do not affect each other, or that they have no relation to each other. We will see below that dramatic simplifications do not necessarily weaken the referential grasp of machine learners on the world, but in certain ways allow them to relate to it more directly.

While the equation shown in (\ref{eq:naive_bayes}) does not set out all the steps in transforming some training data into a predictive model, the lines of code needed to do this are relatively brief. In _Doing Data Science_, Rachel Schutt and Cathy O'Neill furnish a bash script (that is, command line instructions) to download a well-known email dataset and build a Naive Bayes classifier for spam. In many ways, this a canonical machine learner pedagogy, and for Naive Bayes, email spam detection is the standard example. Nevertheless, the completeness of the small script, reproduced below, in fetching the Enron email dataset, calculating the 'marginal densities' or conditional probabilities for each word given how often it is associated with either spam or non-spam email, and then outputting the probability that a particular is spam, is striking.[^4.7] In the space of these few dozen lines, some part of a world -- the Enron email dataset -- becomes a predictive engine for dealing with decisions in the world: is an email spam or not?

A typical spam email in the Enron dataset looks like this:

> Subject: it's cheating, but it works !
can you guess how old she is ? the woman in this photograph looks like a happy teenager about to go to her high school prom, doesn' t she ? she' s an international, professional model whose photographs have appeared in hundreds of ads and articles whenever a client needs a photo of an attractive, teenage girl.but guess what ? this model is not a teenager ! no, she is old enough to have a 7-year-old daughter.. and...the model' s real age is in her 30' s.all she will say about her age to her close friends is, " i'm dangerously close to 40." she also says, " if it weren't for this amazing new cosmetic cream called ' deception, ' i would lose hundreds of modeling assignments...because...there is no way i could pass myself off as a teenager." learn more about this amazing new product...please refer all questions, opinions or additional feedback to :
service dept
9420 reseda blvd # 133
northridge, ca 91324

The text of a typical non-spam email like this:

> Subject: industrials suggestions......
----------------------forwarded by kenneth seaman / hou / ect on 01 / 04 / 2000
12 : 47 pm-------------------------- -
pat clynes @ enron
01 / 04 / 2000 12 : 46 pm
to : kenneth seaman / hou / ect @ ect, robert e lloyd / hou / ect @ ect
cc :
subject : industrials
ken and robert,
the industrials should be completely transitioned to robert as of january 1, 2000.please let me know if this is not complete and what else is left to transition .
thanks, pat

These are both recognisable things in the world for anyone who uses email. How do they become $X$ or even $f_j(X)$ in the Naive Bayes classifier? How do words in a document because probability distributions? The code is instructive:

[^4.7]: The input to the script is a single word such as 'finance' or 'deal'. The model is so simple that it only classifies a single word as spam. The `bash` script carries out four different transformations of the data in building the model. It uses only command line tools such as `wc` (word count), `bc` (basic calculator), `grep` (text search using pattern matching) and `echo` (display a line of text). These tools or utilities are readily available in almost any UNIX-based operating system (e.g. Linux, MacOS, etc). The point of using only these utilities is to illustrate the simplicity of the algorithmic implementation of the model.  The first part of the code downloads the sample dataset of Enron emails (and I will discuss spam emails and their role in machine learning below). Note that this dataset has already been divided into two classes - 'spam' and 'ham' -- and emails of each class have been placed in separate directories or folders as individual text files.

```{r enron_nb_bash, engine='bash', echo=TRUE}
    #!/bin/bash
    # file: enron_naive_bayes.sh
    # description: trains a simple one-word naive bayes spam
    # filter using enron email data
    # usage: ./enron_naive_bayes.sh <word>
    # requirements:
    #    wget
    #
    # author: jake hofman (gmail: jhofman)
    # how to use the code
    if [ $# -eq 1 ]
    then
        word=$1
    else
        echo "usage: enron_naive_bayes.sh <word>"
        exit
    fi

     ### PART 1
    if ! [ -e enron1.tar.gz ]
    then
        wget 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'
    fi

    if ! [ -d enron1 ]
    then
        tar zxvf enron1.tar.gz
    fi

    cd enron1

    ### PART 2
    Nspam=`ls -l spam/*.txt | wc -l`
    Nham=`ls -l ham/*.txt | wc -l`
    Ntot=$Nspam+$Nham
    echo $Nspam spam examples
    echo $Nham ham examples

    Nword_spam=`grep -il $word spam/*.txt | wc -l`
    Nword_ham=`grep -il $word ham/*.txt | wc -l`
    echo $Nword_spam "spam examples containing $word"
    echo $Nword_ham "ham examples containing $word"

    ### PART 3
    Pspam=`echo "scale=4; $Nspam / ($Nspam+$Nham)" | bc`
    Pham=`echo "scale=4; 1-$Pspam" | bc`
    echo
    echo "estimated P(spam) =" $Pspam
    echo "estimated P(ham) =" $Pham
    Pword_spam=`echo "scale=4; $Nword_spam / $Nspam" | bc`
    Pword_ham=`echo "scale=4; $Nword_ham / $Nham" | bc`
    echo "estimated P($word|spam) =" $Pword_spam
    echo "estimated P($word|ham) =" $Pword_ham

    ### PART 4
    Pspam_word=`echo "scale=4; $Pword_spam*$Pspam" | bc`
    Pham_word=`echo "scale=4; $Pword_ham*$Pham" | bc`
    Pword=`echo "scale=4; $Pspam_word+$Pham_word" | bc`
    Pspam_word=`echo "scale=4; $Pspam_word / $Pword" | bc`
    echo
    echo "P(spam|$word) =" $Pspam_word
    cd ..
```

[@Schutt_2013, 105-106]

```{r enron_nb_r, echo = FALSE}

        f_ham = list.files('enron1/ham', full.names = TRUE)
        f_spam = list.files('enron1/spam', full.names = TRUE)
        ham_count = length(f_ham) 
        spam_count = length(f_spam)

        ham = sapply(f_ham, readLines, warn=FALSE)
        spam = sapply(f_spam, readLines, warn=FALSE)
        P_ham = ham_count/(ham_count + spam_count)
        P_spam = spam_count/(ham_count + spam_count)
        word = 'gas'

        predict_spam <- function(word) {
            ## count the occurrence of the word in each
            Nword_in_ham = sum(grepl(word, ham))
            Nword_in_spam = sum(grepl(word, spam))
            #cat(Nword_in_ham, ' ham examples contain ', word,  '\n')
            #cat(Nword_in_spam, ' spam examples contain ', word,  '\n')

            #cat('estimated P(spam) = ', P_spam,  '\n')
            #cat('estimated P(ham) = ', P_ham,  '\n')

            Pword_spam = Nword_in_spam/spam_count
            Pword_ham = Nword_in_ham/ham_count
            #cat("P(spam|", word, ") = ", Pword_spam,  '\n')
            #cat("P(ham|", word, ") = ", Pword_ham,  '\n')

            Pham_word = Pword_ham * P_ham
            Pspam_word = Pword_spam * P_spam
            Pword = Pspam_word + Pham_word
            Pspam_word = Pspam_word/Pword

            #cat("P(spam|", word, ")=",  Pspam_word,  '\n')
            return(Pspam_word)
        }
```

After fetching the dataset from a website, the code counts the number of emails in each category and prints them, and the counts the number of times that the chosen word (e.g. 'finance' or 'deal') occurs in both the spam and non-spam or ham categories. Using these counts, the script  estimates probabilities of any email being spam or ham, and then given that email is spam or ham, that the particular word occurs. To estimate a probability means, in this case, to divide the word count for the chosen word by the count of the number of spam emails, and ditto for the ham emails.  In Part 4, the final transformation of the data, these probabilities are used to calculate the probability of an email being spam given the presence of that word. Again, the mathematical operations here are no more complicated than adding, multiplying and dividing. The probability that the chosen word is a spam word is, for instance, the probability of occurrence of the word in a spam email multiplied by the overall probability that an email is spam. Finally, given that the overall probability of the chosen word occurring in the email dataset is the probability of it occurring in spam plus the probability of it occurring in ham, the overall probability that an email in the Enron data is spam given the presence of that word can be calculated. It is the probability that the chosen word is a spam word divide by the probability of that word in general. 

The point of this slightly bamboozling and mechanical description of what the script does is to show something of how the diagrammatic form of the function written as a joint probability in equation (\ref{eq:naive_bayes}) moves into the world. Not all machine learning models are so simple that they can be conveyed in 30 lines of code (including downloading the data and comments). The advantage of this script is that it shows that nothing that occurs in relation to probability, probability distributions or classification is intrinsically mysterious, elusive or indeed particularly abstract. On the contrary, everything here is counting, adding, multiplying (that is, repeated adding) and dividing (that is, multiplying by parts or fractions). Everything is constrained by a certain re-distribution of things such as emails or documents as, in this case, events in a population of words. Every word found in the Enron datasets becomes a probability density, and the classification of each email becomes a matter of estimating a conditional probability based on the joint probability of events occurring together.  Probabilities are always between `0` and `1`, or between 0% and 100%, and classification entails selected a cutoff or dividing line. For instance, greater than `0.5` might result in a classification.   'Finance' has a  `r round(predict_spam('finance'),2)` chance of being spam, while 'sexy' has a chance of `r round(predict_spam('sexy'), 2)`, at least in the Enron emails.   The transformation of textual forms -- the emails -- into probabilistic distributions is practically straightforward. Ironically, like the Naive Bayes classifier's own reliance on seventeenth and eighteenth century probability calculus, the frequent application of this machine learner to document classification and retrieval echoes the seventeenth century thinking   that first conceived of the very notion of 'probability' in relation to the evidential weight of documents [@Hacking_1975, 85]. 

## The improbable success of the Naive Bayes classifier

Like most machine learning techniques applied to texts, classifiers such as Naive Bayes ignore many obvious structural features of emails as documents (for instance, word order, or co-occurrences of words). Yet this very artificiality or limitation of its reference to the world is typical. The contrast with modern statistical practice starts to appear in Naive Bayes and only intensifies in the ongoing development of machine learning classifiers (for instance, in topic models). On the one hand, like statistics more generally, Naive Bayes treats the world probabilistically. The world comprises populations, and populations generate events. Emails have a real probability of being spam, and this probability is acted on many times every day for email users. On the other hand, there is something quite artificial or contrived about these populations and their associated probability distributions.  They are intentionally artificial and limited; they do not correspond or refer directly to what we know, for instance, of how language works, but instead to a rather different set of concerns that remain statistical but somehow less directly invested in the reality of, for instance, the normal distribution as the defining property of the population.

Hastie, Tibshirani and Friedman characterise the value of the  Naive Bayes technique in relation to its capacity to deal with high dimensional data[^4.20] :    


>It is especially appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class $G = j$, the features $X_k$ are independent [@Hastie_2009, 211]. 

[^4.20]: See Chapter 2 on this notion of data dimensionality.

In equation (\ref{eq:naive_bayes}), $p$ stands for the number of different dimensions or variables in the data set. In the spam classifier, the number of dimensions is probably quite large because every unique word adds a new dimension to the 'feature space.' By contrast, there are are only two classes $G$, spam and non-spam.  Yet compared to the complications of logistic regression, neural networks or support vector machines, \ref{eq:naive_bayes}  seems incredibly simple. How is it that a simple multiplication of probabilities can, as Hastie and co-authors write: 'often outperform far more sophisticated alternatives' [@Hastie_2009, 211]. Similar formulations can  be found in most of the machine learning books and instructional materials currently available.

I want to explore two different ways to view the improbable success of Naive Bayes. Both are germane to the broader question we are pursuing of how machine learning changes the 'taming of chance' accomplished by modern statistics. The first way to view this success is in terms of _ancestral communities_ of classification, inference and prediction. The second concerns the structuring statistical difference between bias and variance. 

## Ancestral probabilities of documents

Along with linear regression, Naive Bayes might be _the_ standard introductory machine learning technique. The Naive Bayes classifier is almost always demonstrated on the problem of filtering spam email [@Conway_2012; @Schutt_2013, 93-113; @Kirk_2014, 53; @Lantz_2013, 92-93; @Flach_2012, @Ng_2008b], and in particular dealing with the abundance of spam emails concerning Viagra (itself a byproduct of the failure of a statistical model). What are we to make of this reiteration of the same problem?  It is remarkable how often Naive Bayes comes up in machine learning textbooks especially given how simple the technique is. Admittedly spam, and spam trying to sell Viagra in particular, has been a very familiar part of most email users lives for a decade and more, and of all the documents that machine learners encounter in quantity, email might be the most numerous as well as one of the simplest. Moreover, in practice, Naive Bayes classifiers and variations of them have become an integral part of managing email traffic for most people, whether they know it or not, since the mid-1990s (see for instance, [SpamAssassin](http://spamassassin.apache.org/). By virtue of their constant operation as part of mundane infrastructures of organisational communication, large sample datasets of email tagged as 'spam' or 'ham' have become widely available.

I wonder, however, whether the constant reiteration of email spam filtering using Naive Bayes creates the kind of statistical transposition akin to those occurring in the nineteenth century. In effect, reality becomes more statistical through its constant treatment as statistical. Like many of machine learning techniques, Naive Bayes has a multi-stranded genealogy, and this genealogy runs through all current demonstrations and implementations of the technique. As Lucy Suchman and Randall Trigg wrote in their study of work on artificial intelligence, 

>rather than beginning with documented instances of situated inference ... researchers begin with ... postulates and problems handed down by the ancestral communities of computer science, systems engineering, philosophical logic, and the like [@Suchman_1992,174]. 

Not only does Bayes Theorem date from the 18th century, but the highly successive use of Naive Bayes classifiers in email spam filtering for several decades means that machine learners effectively can draw on an ancestral community of document classification, or more pointedly, the probabilisation of documents in certain settings.  The first attempts to use what is now called Naive Bayes in the early 1960s already re-iterated the engagements with the evidential weight of documents that accompanied the emergence of probabilistic thinking in the seventeenth century. For instance, starting in 1960, Homer Warner, Alan Toronto and George Veasy, working at the University of Utah and Latter-day Saints Hospital in Salt Lake City, began to develop a probabilistic computer model for diagnosis of heart disease [@Warner_1961; @Warner_1964]. Their model used exactly the same 'equation of conditional probability' we see in \ref{eq:naive_bayes} but now used to 'express the logical process used by a clinician in making a diagnosis based on clinical data' [@Warner_1961, 177]. Despite the mention of logic in this description, the diagnostic model was thoroughly probabilistic in the sense that the model itself has no representation of logic included in its workings. Rather it calculates the probability of a given type of heart disease given 'statistical data on the incidence of symptoms' [@Warner_1964, 558]. Somewhat ironically, as they point out, physicians involved in preparing and submitting data to the diagnostic program improved the accuracy in their own diagnoses.[^4.51]

Similarly, working at the RAND Corporation in the early 1960s, M.E. Maron described how 'automatic indexing' of documents -- Maron used papers published in computer engineering journals -- could become 'probabilistic automatic indexing.' The necessary statistical assumption was:

>The fundamental thesis says, in effect, that statistics on kind, frequency, location, order, etc., of selected words are adequate to make reasonably good predictions about the subject matter of documents containing those words [@Maron_1961, 406]

This fundamental thesis has since remained somewhat fundamental in text classification and information retrieval applications, as well as many other machine learning approaches since. A typical contemporary information retrieval textbook such as [@Manning_2008] devotes a chapter to the topic. In the weighting of clinical evidence in diagnosis,  in the classification of scientific papers and in spam filtering, the statistical predictions gain traction in the repeated alignments of data with the model.  We might attend to the shape of the datasets in order to see these alignments. Maron's work focused on a collection of several hundred abstracts of papers published in the March and June 1959 issues of the _IRE Transactions on Electronic Computers_. As in contemporary supervised learning, these abstracts were divided into two groups, a training and a test set ('group 1' and 'group 2' in Maron's terminology [@Maron_1961, 407]), and the training set was classified according to 32 different categories that had already been in use by the Professional Group on Electronic Computers, the publishers of the _IRE Transactions_. Given these classifications, word counts for all distinct words in the abstracts were made, the most common terms ('the', 'is', 'of', 'machine', 'data', 'computer') and the most uncommon words removed, and the remaining set of around 1000 words were actually used for classification. This treatment of the abstracts as documents, then as lists of words, and then as frequencies of terms, and finally as a filtered list of most information rich terms continues in much text classification work today. We saw similar operations occurring in the spam filter code above.  The point here is that whenever machine learners affirm the unlikely efficacy of Naive Bayes classifiers, we might attend to the ways in which that the ground for that success has been prepared by previous statistical classifications  or 'ancestral probabilisations' of the domain in question. 

[^4.51]: In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965].  

Statistics on kind, frequency, order and location are often the only chains of reference that these techniques maintain in relating to their source data. The fact that certain complexities or relationalities might be filtered out by the fundamental thesis is acknowledged explicitly, but compensated by a practical interest in automatic classification when confronted by large numbers of documents. As we will see, even in the most recent iterations of these techniques in topic modelling [@Blei_2011], with all its statistical sophistication, the fundamental thesis largely applies.  

## Decomposition: high bias, low variance and observation of errors

Even with an eye on the ancestral communities that always accompany and shape any actual reference of machine learning to the world, we still need a way of accounting for the unreasonable artificiality of something like the Naive Bayes. A second much broader way to view the persistence of a manifestly reductionist model, a model that eschews any modelling of relations between things in the word such as words, is in terms of another of the structuring differences of machine learning: the so-called _bias-variance_decomposition_[@Hastie_2009,24]. The terms 'bias' and 'variance' are overtly statistical, and they are heavily used in machine learning discussions of model performance and application. The terms point to tensions that all machine learners experience. On the one hand, _variance_ refers to the inevitable reliance of a machine learner on the data it 'learns.' To put it more formally, 'variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set' [@James_2013,34]. On the other hand, _bias_ 'refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model' [@James_2013, 35]. These two sources of error, one which results from sampling and the other arising from the structure of the model or approximating function, can be reduced or at least subject to trade-off. Another source of error, the 'irreducible error' [@Hastie_2009, 37] is noise that no model can eliminate.  From the standpoint of the bias-variance decomposition, every  machine learner makes a trade-off between the errors deriving from differences between samples, and errors due to the difference between the approximating function and the actual process that generated the data. Note that both sources of error in the bias-variance decomposition derive from the relation between the model and the data, one relating to how the model 'sees' the world (as a set of almost coin-toss like independent events, as a geometrical problem of finding a line or curve that runs through a cloud of points, etc.) and the other relating to how the model encounters the world (as a set of small samples or as, at the other end, a massive $N=\forall{X}$ dataset). Both of these referential relations matter greatly to  machine learners, no matter how much data they have. 

A rather elaborate set of concepts and techniques address the bias-variance decomposition. These techniques focus on managing the _test_ or _generalization_ error, the difference between the actual and predicted values produced by the  machine learner when it encounters a fresh, hitherto unseen data sample. Note that this problem of generalization error is not eliminated in any of the 'big data' or $N=\forall{X}$ scenarios often discussed in association with contemporary information infrastructures. Here too, prediction and inference encounters fresh data in some form or other, and  machine learners in such settings still suffer from the  bias-variance trade-off. This trade-off has to deal with the fact that training errors -- the difference between what the model predicts and what the training data actually shows -- is not a good guide to the test or generalization error.  The process of fitting a model or finding a function (see previous chapter) will tend to reduce the training error by fitting the function more and more closely to the shape of the training data, but when it encounters fresh data that function might no longer fit well. In other words, a more sophisticated function may well reduce the bias but increase the variance. 'Richer collections of models' [@Hastie_2009, 224] reduce bias, but tend increase variance. Conversely, models that cope well with fresh data (and Naive Bayes is a good example of such a machine learner), display low variance but high bias.The exact nature of the trade-offs between bias and variance shift  markedly  between different types of models, and generates many different conceptual and technical treatments ('optimism of the training error rate' (228), 'estimates of in-sample prediction error' (230), 'Bayesian information criterion' (233), 'Vapnik-Chervonenkis dimension' (237), 'minimum description length' (235)) and technical treatments ('cross-validation' (241), 'bootstrap methods'  (249), 'maximum likelihood inference' (265), 'expectation-maximization algorithm' (272), 'bagging' (282), or 'Markov Chain Monte Carlo (MCMC)' (279)). The daunting armory of methods all respond to the same underlying problems of reference. They invoke in some cases sophisticated mathematical or statistical constructs, but also very often rely on computational iteration to optimise values of parameters in models who underlying intuitions remain quite simple and straightforward (as in a linear regression or Naive Bayes). In some cases, and Naive Bayes would be a good example, the implementation of a model may be very simple, but analysis of how the machine learner manages to control a source of error such as bias or variance requires much more sophisticated statistical understanding.  Importantly, many of these techniques and re-conceptualisations of what a model as 'useful approximation' is doing treat the models themselves as a kind of population whose variations and uncertainties, whose tendencies and predispositions must be sampled, tested and monitored. Again, in terms of the structuring differences shown in Table \ref{table:differences}, while the model may be predictive in its relation to a given domain or field, the model itself as a machine learner  is the object of inference, and sometimes prediction (as in the case of the generalization error). 

For our purposes, the bias-variance decomposition introduces an irreducible tension to the way that machine learning structures the world. If, following the broadly Foucaultean line of argument proposed by Hacking, statistical thinking and practice  in the nineteenth and early twentieth century configured things in terms of probability distributions (and the Gaussian distribution in particular), what happens in worlds where the bias-variance decomposition structures the underlying reality? The referential structure of the bias-variance decomposition is diagrammatic in the sense that it includes both tightly bound points  and certainly 'relatively free or unbound points, points of creativity, change and resistance' [@Deleuze_1988, 44] that matter greatly to the relations of force constituting the knowledge or decision-making power of  machine learning. The referential power of the models varies, and every attempt to construct a machine learner  in  a given setting relies either on the re-iteration of ancestral probabilities (that is, prior structurings of experience in conformity with the curve of some probability distribution) or on the many interactive adjustments, re-distriubtions and re-samplings of the world _and_ of the models associated with the bias-variance decomposition. 



## Conclusion

Mayer-Schönberger and Cukier's argument that having much data or all data ($N=\forall{X}$) is a leitmotif in accounts of predictive modelling and analytics during the last decade.

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 27]. 

Versions of this claim can be found running through various scientific and business settings throughout the 20th century.[^4.61] In certain settings, $N=all$ has been around for quite a while (as for instance, in many document classification settings where the whole corpus of documents have been electronically curated for decades). Mayer-Schönberger and Cukier rightly emphasize, it seems to me, that the huge quantities of data sluicing through some contemporary infrastructures support inferences of probabilities (11). I rather less like the way they describe statistical sampling as a concept 'developed to solve a particular problem at a particular moment in time under specific technological constraints' [@Mayer-Schonberger_2013, 31]] since this does not take into account the material consequences of statistical thinking. They do not, however, suggest that the very possibility of spotting connections or details that might matter deeply itself relies on statistical configurations that deeply affect all machine learners.  Whether or not someone uses Naive Bayes, a topic model, neural networks or logistic regression, probabilistic practice, random variables,  probability distributions and statistical inferences run deep in the model. In many ways, the Mayer-Schönberger and Cukier account pays so much attention to the potentials of data accumulation that they cannot easily attend to the question of what happens to the data as people try to 'spot connections and details', or of how what is put into the data that goes beyond $N=all$.  Here sampling, estimation, likelihoods, and a whole gamut of dynamic relationships between random variables in joint probability distributions reassert themselves. The data may not be sampled, but the model is sampling as it tries to move through the high-dimensional feature spaces opened up by having 'all' the data. 

[^4.61]: Part  II of this book will track several actual instances of having all the data in the sciences, in government and in business in order to show what having all the data entails in different settings. 


Machine learning inhabits worlds that had already witnessed the advent of  statistical realities  at least a century earlier, whether through the social physics of Quetelet, the biopolitical normals of Francis Galton and his regression to the mean (remember that the linear model of regression is probably the basic machine learning model) or later in the probability functions of quantum mechanics in early twentieth century physics. I have been suggesting the machine learning should be understood as a re-inversion of statistical thought. In this inversion, the probability distributions that had become the underlying reality of many different kinds of populations fold back or re-distribute themselves in devices such as machine learners whose variations and uncertainties powerfully generate classifications, predictions and knowledge statements (scientific and otherwise). Populations of models are sampled, measured, and aggregated in the ongoing production of statistical realities whose object is no longer a property of individual members of a population (their height, their life-expectancy, their chance of HIV/AIDS), but a meta-population of models of populations and their probability distributions. We should note that not all machine learners are strictly speaking probabilistic models [^4.60], but the referential relation of any machine learner to the world is statistical by virtue of the both the ancestral probabilities and the bias-variance decomposition. 

[^4.60]:  Machine learning textbooks written by computer scientists tend to define probabilistic models more narrowly.  As Peter Flach suggests:

    >Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution $P(Y|X)$ (or its counterpart, the likelihood function $P(X|Y)$) which tells me the class distribution $Y$ after observing the features values $X$ [@Flach_2012, 47]

    But whether they are probabilistic in this sense or not, the evaluation and configuring of machine learners irreducibly depends on a statistical treatment of errors and their trade-offs. 

The re-distribution of probability that takes place in machine learning through multiple-swapping of observational positions bears several implications for thinking about the power of the techniques. The lure of the techniques is a kind of trap since the swapping or inversion of statistical thought takes what was put into the world by modern statistical thought and puts it into machines whose intricate workings themselves become the event to be observed. At the same time, although taking back the real quantities, the machine learners remain closely in contact with worlds populated by texts, documents, images, prices, sensor measurements, transactions and records. 

## extra stuff

- the whole topic model discussion

[stuff here from alpaydin; deals with generative; etc; but also discriminative -- generating the data vs a conditional probability of a particular variable; modelling the whole thing versus modelling some particular quantity; ]

The quantification entailed in statistics is a counting of events that have been named or labelled in some way. The perhaps more profound point is that this counting of atomic events can be combined in a seemingly limitless variety of combinations. In the emails, individual words are events, and therefore each email is a complicated composite event. Similarly, in machine learning on images, each pixel could be treated as an event, and the image as a whole becomes an immensely complicated aggregate colour and light event. As we will see, many other machine learning techniques try to deal with this combinatory character of composite events directly. 

There is some new operating terminology in this chapter (terms such as random variable, probability distribution and likelihood).  While I attempt to be both mathematically and conceptually concise in my use of these terms, the terms themselves are somewhat troubled by the transformations I'm describing. The underpinnings of taken-for-granted and everyday statistical such as random variable or probability are not immune from change. Like all technical formalisms they took hold at a certain practical and historical conjuncture that will not and perhaps already does not hold entirely still. As the philosophy Ian Hacking suggests in his discussion of early 20th century changes in probability in _The Taming of Chance_,

>By the 1930s, however, the world teemed with frequencies, and the 'objective' notion would come to seem more important than the 'subjective' one for the rest of the century -- simply because there were so many more frequencies to be known [@Hacking_1990, 97].

I'm suggesting that we countenance a scene in which multiple different probability practices stack on top of each other in a somewhat untamed way. Amidst these, random variables and their associated probability distributions particularly concern us. As we will see, the power of the transformation in probability associated with machine learning algorithms resides in their capacity to draw in many more relations, features and components of data in support of a probabilistic outcome. When things are best described as probability distributions, they take on a different form of temporal and multiplicative existence, and this no longer easily attributed to either 'subjective' or 'objective' probability, or to a shift in balance between the long-standing poles of 'subjective' and 'objective' probability. A probabilistically generated airline seat price attracts different kinds of transactions than a fixed priced seat. Similarly, a probabilistically generated tumour classification implies different modes of responsiveness and care. Gaining some understanding of the concrete arrangements of forces in these probabilistic modes might allow us to account for the ways in which certain methods -- Bayesian inference is a striking example of transverse momentum of methods across fields  -- go on the move, or why certain problems -- automatic text classification, image recognition, etc -- suddenly hove into feasibility.

Certain strands of social and cultural theory have taken a strong interest in algorithmic processes. For instance, the sociologist Scott Lash distinguishes  the operational rules found in  algorithms from the regulative and constitutive rules in many social settings and studied by social scientists:

>in a society of pervasive media and ubiquitous coding, at stake is a third type of rule, algorithmic, generative rules. ‘Generative’ rules are, as it were, virtuals that generate a whole variety of actuals. They are compressed and hidden and we do not encounter them in the way that we encounter constitutive and regulative rules. Yet this third type of generative rules is more and more pervasive in our social and cultural life of the post-hegemonic order. They do not merely open up opportunity for invention, however. They are also pathways through which capitalist power works, in, for example, biotechnology companies and software giants more generally [@Lash_2007a, 71].

The term 'generative' is somewhat resonant in the field of machine learning as generative models, models that treat modelling as a problem of specifying the operations or dynamics that could have given rise to the observed data, are extremely important. If we consider only Andrew Ng's CS229 machine learning lectures  on Youtube [@Ng_2008], we can see that they introduce generative models in Lecture 5 and 6. Although this seems to be only a small part of the 18 lectures given in the course, later lectures on the expectation maximisation algorithm (12-13), and then on unsupervised learning techniques such as factor analysis and principal component analysis, independent component analysis, are also effectively exploring generative models.  A similar distribution of topics can be found in _Elements of Statistical Machine Learning_[@Hastie_2009].   Generative models, while perhaps slightly less common in practice than discriminative models, nevertheless capture the sense that algorithms are not just implementations of rules for filtering, sorting, or deciding, but carry within them ontological commitments that might actually challenge social theory in interesting ways. In contrast to Lash, I would suggest that the generativity of these algorithms needs to be differentiated from the algorithmic processes that implement rules more generally. Moving into the data via a generative probabilistic model is very different to moving into the data through say a database query. The models, whether generative or discriminative (models  such as decision tree,  logistic regression or even neural networks that are more limited in their probabilistic underpinnings), are more like meta-algorithms that reorganize other algorithmic processes on varying scales. 

## References
