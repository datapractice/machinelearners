
@article{wu_top_2008,
	title = {Top 10 algorithms in data mining},
	volume = {14},
	number = {1},
	journal = {Knowledge and Information Systems},
	author = {Wu, X. and Kumar, V. and Ross Quinlan, J. and Ghosh, J. and Yang, Q. and Motoda, H. and {McLachlan}, G. J and Ng, A. and Liu, B. and Yu, P. S and others},
	year = {2008},
	pages = {1–37}
},

@misc{stanforduniversity_lecture_2008,
	title = {Lecture 2 {\textbar} Machine Learning (Stanford)},
	lccn = {0000},
	url = {http://www.youtube.com/watch?v=5u4G23_OohI&feature=youtube_gdata_player},
	abstract = {Lecture by Professor Andrew Ng for Machine Learning ({CS} 229) in the Stanford Computer Science department.  Professor Ng lectures on linear regression, gradient descent, and normal equations and discusses how they relate to machine learning. 

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed.

Complete Playlist for the Course:
{http://www.youtube.com/view\_play\_list?p=A89DCFA6ADACE599}

{CCS} 229 Course Website:
http://www.stanford.edu/class/cs229/

Stanford University:
http://www.stanford.edu/

Stanford University Channel on {YouTube:}
http://www.youtube.com/stanford},
	urldate = {2013-02-11},
	collaborator = {{{StanfordUniversity}}},
	month = jul,
	year = {2008}
},

@article{_grill:_2010,
	title = {The Grill: Tom Mitchell},
	lccn = {0000},
	shorttitle = {The Grill},
	url = {http://www.computerworld.com/s/article/346917/The_Grill_Tom_Mitchell},
	urldate = {2012-03-09},
	journal = {Computerworld},
	month = feb,
	year = {2010},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/NKP5QXI2/The_Grill_Tom_Mitchell.html:text/html}
},

@misc{_company_????,
	title = {The Company for Apache Lucene Solr Open Source Search {\textbar} Lucid Imagination},
	url = {http://www.lucidimagination.com/},
	urldate = {2012-03-15}
},

@book{clarke_information_2010,
	title = {Information Retrieval: Implementing and Evaluating Search Engines},
	isbn = {0262026511},
	lccn = {0000},
	shorttitle = {Information Retrieval},
	publisher = {The {MIT} Press},
	author = {Clarke, Charles L. A. and Buettcher, Stefan and Cormack, Gordon V.},
	month = jul,
	year = {2010}
},

@book{manning_introduction_2008,
	edition = {1},
	title = {Introduction to Information Retrieval},
	isbn = {0521865719},
	lccn = {0036},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	month = jul,
	year = {2008}
},

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	lccn = {6381},
	number = {1},
	journal = {Machine learning},
	author = {Breiman, L.},
	year = {2001},
	pages = {5–32}
},

@article{ensmenger_is_2012,
	title = {Is Chess the Drosophila of Artificial Intelligence? A Social History of an Algorithm},
	volume = {42},
	issn = {0306-3127, 1460-3659},
	lccn = {0000},
	shorttitle = {Is chess the drosophila of artificial intelligence?},
	url = {http://sss.sagepub.com.ezproxy.lancs.ac.uk/content/42/1/5},
	doi = {10.1177/0306312711424596},
	abstract = {Since the mid 1960s, researchers in computer science have famously referred to chess as the ‘drosophila’ of artificial intelligence ({AI).} What they seem to mean by this is that chess, like the common fruit fly, is an accessible, familiar, and relatively simple experimental technology that nonetheless can be used productively to produce valid knowledge about other, more complex systems. But for historians of science and technology, the analogy between chess and drosophila assumes a larger significance. As Robert Kohler has ably described, the decision to adopt drosophila as the organism of choice for genetics research had far-reaching implications for the development of 20th century biology. In a similar manner, the decision to focus on chess as the measure of both human and computer intelligence had important and unintended consequences for {AI} research. This paper explores the emergence of chess as an experimental technology, its significance in the developing research practices of the {AI} community, and the unique ways in which the decision to focus on chess shaped the program of {AI} research in the decade of the 1970s. More broadly, it attempts to open up the virtual black box of computer software – and of computer games in particular – to the scrutiny of historical and sociological analysis.},
	language = {en},
	number = {1},
	urldate = {2012-05-28},
	journal = {Social Studies of Science},
	author = {Ensmenger, Nathan},
	month = feb,
	year = {2012},
	keywords = {artificial intelligence, computing, drosophila, experimental technology},
	pages = {5--30},
	file = {Is chess the drosophila of artificial intelligence? A social history of an algorithm:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/V3KGKEKN/5.html:text/html;Is chess the drosophila of artificial intelligence? A social history of an algorithm:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/SXFNK8TD/5.html:text/html}
},

@misc{bacon_hilary_2012,
	title = {Hilary Mason - Machine Learning for Hackers},
	url = {http://vimeo.com/43547079},
	abstract = {Vimeo is the home for high-quality videos and the people who love them.},
	urldate = {2012-07-06},
	author = {Bacon},
	month = jun,
	year = {2012},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XXH9QQJS/43547079.html:text/html}
},

@book{koren_matrix_2009,
	title = {Matrix Factorization Techniques for Recommender Systems},
	abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest-neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels. Modern consumers are inundated with choices. Electronic retailers and content providers offer a huge selection of products, with unprecedented opportunities to meet a variety of special needs and tastes. Matching consumers with the most appropriate products is key to enhancing user satisfaction and loyalty. Therefore, more retailers have become interested in recommender systems, which analyze patterns of user interest in products to provide personalized recommendations that suit a user’s taste. Because good personalized recommendations can add another dimension to the user experience, e-commerce leaders like Amazon.com and Netflix have made recommender systems a salient part of their websites. Such systems are particularly useful for entertainment products such as movies, music, and {TV} shows. Many customers will view the same movie, and each customer is likely to view numerous different movies. Customers have proven willing to indicate their level of satisfaction with particular movies, so a huge volume of data is available about which movies appeal to which customers. Companies can analyze this data to recommend movies to particular customers. Recommender system strategies Broadly speaking, recommender systems are based on one of two strategies. The content filtering approach creates a profile for each user or product to characterize its nature. For example, a movie profile could include attributes regarding its genre, the participating actors, its box office popularity, and so forth. User profiles might include demographic information or answers provided on a suitable questionnaire. The profiles allow programs to associate users with matching products. Of course, content-based strategies require gathering external information that might not be available or easy to collect. A known successful realization of content filtering is the Music Genome Project, which is used for the Internet radio service Pandora.com. A trained music analyst scores},
	author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
	year = {2009},
	file = {Citeseer - Full Text PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/KSM4IF25/Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf:application/pdf;Citeseer - Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/RHNCXHWT/summary.html:text/html}
},

@misc{_dataists_????,
	title = {dataists » A Taxonomy of Data Science},
	url = {http://www.dataists.com/2010/09/a-taxonomy-of-data-science/},
	urldate = {2012-07-06},
	file = {dataists » A Taxonomy of Data Science:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/6VAB8TUH/a-taxonomy-of-data-science.html:text/html}
},

@article{wagstaff_machine_2012,
	title = {Machine Learning that Matters},
	url = {http://arxiv.org/abs/1206.4656},
	abstract = {Much of current machine learning ({ML)} research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that {ML} has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on {ML} that matters.},
	urldate = {2012-07-16},
	journal = {{arXiv:1206.4656}},
	author = {Wagstaff, Kiri},
	month = jun,
	year = {2012},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
	file = {1206.4656 PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/UZRQSNZ9/Wagstaff - 2012 - Machine Learning that Matters.pdf:application/pdf;arXiv.org Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XGPZ3HCN/1206.html:text/html}
},

@article{langley_changing_2011,
	title = {The changing science of machine learning},
	volume = {82},
	lccn = {0003},
	url = {http://www.springerlink.com/index/J067H855N8223338.pdf},
	number = {3},
	urldate = {2012-06-21},
	journal = {Machine Learning},
	author = {Langley, P.},
	year = {2011},
	pages = {275–279}
},

@article{carstens_sentiment_2011,
	title = {Sentiment Analysis},
	lccn = {0000},
	url = {http://www.doc.ic.ac.uk/teaching/distinguished-projects/2011/l.carstens.pdf},
	urldate = {2012-06-21},
	author = {Carstens, L. and Intelligence, S. A},
	year = {2011}
},

@article{wagstaff_machine_2012-1,
	title = {Machine Learning that Matters},
	lccn = {0000},
	url = {http://ml.jpl.nasa.gov/papers/wagstaff/wagstaff-MLmatters-icml12.pdf},
	urldate = {2012-06-21},
	author = {Wagstaff, K. L},
	year = {2012}
},

@book{conway_machine_2012,
	edition = {1},
	title = {Machine Learning for Hackers},
	isbn = {1449303714},
	lccn = {0000},
	publisher = {{O'Reilly} Media},
	author = {Conway, Drew and White, John Myles},
	year = {2012}
},

@book{vapnik_nature_1999,
	edition = {2nd ed. 2000},
	title = {The Nature of Statistical Learning Theory},
	isbn = {0387987800},
	lccn = {0267},
	publisher = {Springer},
	author = {Vapnik, Vladimir},
	month = dec,
	year = {1999}
},

@inproceedings{ma_identifying_2009,
	title = {Identifying suspicious {URLs:} an application of large-scale online learning},
	lccn = {0108},
	shorttitle = {Identifying suspicious {URLs}},
	url = {http://dl.acm.org/citation.cfm?id=1553462},
	urldate = {2013-03-18},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	year = {2009},
	pages = {681–688},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/BS4JEB3N/citation.html:text/html}
},

@inproceedings{ma_beyond_2009,
	title = {Beyond blacklists: learning to detect malicious web sites from suspicious {URLs}},
	lccn = {0121},
	shorttitle = {Beyond blacklists},
	url = {http://dl.acm.org/citation.cfm?id=1557153},
	urldate = {2013-03-18},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	year = {2009},
	pages = {1245–1254},
	file = {[PDF] from sinica.edu.tw:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/V3934UNU/Ma et al. - 2009 - Beyond blacklists learning to detect malicious we.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3GI8RQQN/citation.html:text/html}
},

@article{ma_learning_2011,
	title = {Learning to detect malicious {URLs}},
	volume = {2},
	lccn = {0009},
	url = {http://dl.acm.org/citation.cfm?id=1961202},
	number = {3},
	urldate = {2013-03-18},
	journal = {{ACM} Transactions on Intelligent Systems and Technology ({TIST)}},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	year = {2011},
	pages = {30},
	file = {[PDF] from berkeley.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/BJ2GZCKP/Ma et al. - 2011 - Learning to detect malicious URLs.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/VS6CUR7Z/citation.html:text/html}
},

@article{le_building_2011,
	title = {Building high-level features using large scale unsupervised learning},
	lccn = {0046},
	url = {http://arxiv.org/abs/1112.6209},
	abstract = {We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous {SGD} on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8\% accu- racy in recognizing 20,000 object categories from {ImageNet}, a leap of 70\% relative im- provement over the previous state-of-the-art.},
	urldate = {2013-04-21},
	journal = {{arXiv:1112.6209}},
	author = {Le, Quoc V. and Ranzato, {Marc'Aurelio} and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
	month = dec,
	year = {2011},
	keywords = {Computer Science - Learning},
	file = {1112.6209 PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/QUF2DCRS/Le et al. - 2011 - Building high-level features using large scale uns.pdf:application/pdf;arXiv.org Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/TU8JVCMI/1112.html:text/html}
},

@book{bishop_pattern_2006,
	title = {Pattern recognition and machine learning},
	volume = {1},
	lccn = {9696},
	url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
	urldate = {2013-07-12},
	publisher = {springer New York},
	author = {Bishop, Christopher M. and Nasrabadi, Nasser M.},
	year = {2006},
	file = {[PDF] from wisc.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/DWIDSVQD/Bishop and Nasrabadi - 2006 - Pattern recognition and machine learning.pdf:application/pdf}
},

@book{murphy_machine_2012,
	address = {Cambridge, {MA}},
	title = {Machine learning: a probabilistic perspective},
	isbn = {9780262018029  0262018020},
	lccn = {0053},
	shorttitle = {Machine learning},
	abstract = {{"This} textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a {MATLAB} software package--{PMTK} (probabilistic modeling toolkit)--that is freely available online"--Back cover.},
	language = {English},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P},
	year = {2012}
},

@article{bbc_google_2012,
	chapter = {Technology},
	title = {Google 'brain' machine spots cats},
	url = {http://www.bbc.co.uk/news/technology-18595351},
	abstract = {A Google research team has trained a network of 1,000 computers wired up like the human brain to recognise cats.},
	urldate = {2013-06-06},
	journal = {{BBC} News},
	author = {{BBC}},
	month = jun,
	year = {2012},
	file = {BBC News Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/GX9SSXDE/technology-18595351.html:text/html}
},

@article{breiman_statistical_2001,
	title = {Statistical modeling: The two cultures (with comments and a rejoinder by the author)},
	volume = {16},
	shorttitle = {Statistical modeling},
	url = {http://projecteuclid.org/euclid.ss/1009213726},
	number = {3},
	urldate = {2013-06-10},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	year = {2001},
	pages = {199–231},
	file = {[PDF] from recognition.su:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/U23JA82S/Breiman - 2001 - Statistical modeling The two cultures (with comme.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XNW8S5Q5/DPubS.html:text/html}
},

@misc{stanforduniversity_lecture_2008-1,
	title = {Lecture 1 {\textbar} Machine Learning (Stanford)},
	url = {http://www.youtube.com/watch?v=UzxYlbK2c7E&feature=youtube_gdata_player},
	abstract = {Lecture by Professor Andrew Ng for Machine Learning ({CS} 229) in the Stanford Computer Science department.  Professor Ng provides an overview of the course in this introductory meeting. 

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed.

Complete Playlist for the Course:
{http://www.youtube.com/view\_play\_list?p=A89DCFA6ADACE599}

{CS} 229 Course Website:
http://www.stanford.edu/class/cs229/

Stanford University:
http://www.stanford.edu/

Stanford University Channel on {YouTube:}
http://www.youtube.com/stanford},
	urldate = {2013-06-10},
	collaborator = {{{StanfordUniversity}}},
	month = jul,
	year = {2008}
},

@article{wacquant_13._2010,
	title = {13. Participant {Observation/Observant} Participation},
	url = {http://books.google.co.uk/books?hl=en&lr=&id=pLSAay_xwjEC&oi=fnd&pg=PA69&dq=wacquant+observant+&ots=LUgGkfHYRD&sig=WwrRprm32d0QB4H8LScW8zlINHc},
	urldate = {2013-06-11},
	journal = {Sociology: Introductory Readings},
	author = {Wacquant, Loïc},
	year = {2010},
	pages = {69},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/JJ9N4N2X/books.html:text/html}
},

@book{barber_bayesian_2011,
	address = {Cambridge; New York},
	title = {Bayesian reasoning and machine learning},
	isbn = {9780521518147 0521518148},
	lccn = {0000},
	abstract = {{"Machine} learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, {DNA} sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a {MATLAB} toolbox, are available online"-- {"Vast} amounts of data present amajor challenge to all thoseworking in computer science, and its many related fields, who need to process and extract value from such data. Machine learning technology is already used to help with this task in a wide range of industrial applications, including search engines, {DNA} sequencing, stock market analysis and robot locomotion. As its usage becomes more widespread, no student should be without the skills taught in this book. Designed for final-year undergraduate and graduate students, this gentle introduction is ideally suited to readers without a solid background in linear algebra and calculus. It covers everything from basic reasoning to advanced techniques in machine learning, and rucially enables students to construct their own models for real-world problems by teaching them what lies behind the methods. Numerous examples and exercises are included in the text. Comprehensive resources for students and instructors are available online"--},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	year = {2011}
},

@book{wacquant_body_2004,
	address = {Oxford},
	title = {Body and Soul. Notebooks of an apprentice boxer},
	publisher = {Oxford University Press},
	author = {Wacquant, Loic},
	year = {2004}
},

@article{olazaran_sociological_1996,
	title = {A Sociological Study of the Official History of the Perceptrons Controversy},
	volume = {26},
	issn = {0306-3127, 1460-3659},
	lccn = {0023},
	url = {http://sss.sagepub.com/content/26/3/611},
	doi = {10.1177/030631296026003005},
	abstract = {In this paper, I analyze the controversy within Artificial Intelligence ({AI)} which surrounded the `perceptron' project (and neural nets in general) in the late 1950s and early 1960s. I devote particular attention to the proofs and arguments of Minsky and Papert, which were interpreted as showing that further progress in neural nets was not possible, and that this approach to {AI} had to be abandoned. I maintain that this official interpretation of the debate was a result of the emergence, institutionalization and (importantly) legitimation of the symbolic {AI} approach (with its resource allocation system and authority structure). At the `research-area' level, there was considerable interpretative flexibility. This interpretative flexibility was further demonstrated by the revival of neural nets in the late 1980s, and subsequent rewriting of the official history of the debate.},
	language = {en},
	number = {3},
	urldate = {2013-06-17},
	journal = {Social Studies of Science},
	author = {Olazaran, Mikel},
	month = jan,
	year = {1996},
	pages = {611--659},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/U5C4XRIM/611.html:text/html}
},

@misc{ng_|_????,
	title = {{\textbar} Machine Learning {III:} Linear Algebra Review},
	url = {https://class.coursera.org/ml-003/lecture/},
	abstract = {Video Lecture:  in Machine Learning on Coursera.},
	urldate = {2013-06-14},
	journal = {Coursera},
	author = {Ng, Andrew},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/N72CWJG4/index.html:text/html}
},

@article{rosenblatt_perceptron:_1958,
	title = {The perceptron: A probabilistic model for information storage and organization in the brain},
	volume = {65},
	copyright = {(c) 2012 {APA}, all rights reserved},
	issn = {1939-1471(Electronic);0033-{295X(Print)}},
	lccn = {0004},
	shorttitle = {The perceptron},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	keywords = {brain, information storage, probabilistic model},
	pages = {386--408}
},

@article{minsky_perceptron:_1969,
	title = {Perceptron: an introduction to computational geometry},
	volume = {19},
	lccn = {0299},
	shorttitle = {Perceptron},
	journal = {The {MIT} Press, Cambridge, expanded edition},
	author = {Minsky, Marvin and Papert, Seymour},
	year = {1969},
	pages = {88}
},

@misc{dahl_deep_????,
	title = {Deep Learning How I Did It: Merck 1st place interview},
	shorttitle = {Deep Learning How I Did It},
	url = {http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/},
	abstract = {What was your background prior to entering this challenge? We are a team of computer science and statistics academics. Ruslan Salakhutdinov and Geoff Hinton are professors at the University of Toro...},
	urldate = {2013-06-17},
	journal = {no free hunch},
	author = {Dahl, George},
	keywords = {code, geoff hinton, learning, machine, merck, model, neural, professor hinton},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/2M52PT7J/deep-learning-how-i-did-it-merck-1st-place-interview.html:text/html}
},

@article{hinton_reducing_2006,
	title = {Reducing the dimensionality of data with neural networks},
	volume = {313},
	lccn = {0928},
	url = {http://www.sciencemag.org/content/313/5786/504.short},
	number = {5786},
	urldate = {2013-06-17},
	journal = {Science},
	author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
	year = {2006},
	pages = {504–507},
	file = {[PDF] from uni-saarland.de:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/KGM75BT4/Hinton and Salakhutdinov - 2006 - Reducing the dimensionality of data with neural ne.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/WRD6BG4A/504.html:text/html}
},

@article{ackley_learning_1985,
	title = {A learning algorithm for Boltzmann machines},
	volume = {9},
	lccn = {2119},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
	number = {1},
	urldate = {2013-06-17},
	journal = {Cognitive science},
	author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
	year = {1985},
	pages = {147–169},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/G6CJZC4J/S0364021385800124.html:text/html}
},

@misc{kdd_call_2013,
	title = {Call For {KDD} Cup},
	url = {http://www.kdd.org/kdd2013/call-for-cup},
	urldate = {2013-07-23},
	author = {{KDD}},
	year = {2013},
	file = {Call For KDD Cup | http://www.kdd.org/kdd2013:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/WG5FVTJU/call-for-cup.html:text/html}
},

@misc{_perceptron_2013,
	title = {Perceptron},
	copyright = {Creative Commons Attribution-{ShareAlike} License},
	lccn = {0001},
	url = {http://en.wikipedia.org/w/index.php?title=Perceptron&oldid=557301943},
	abstract = {In computational geometry, the perceptron is an algorithm for supervised classification of an input into one of several possible non-binary outputs. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector describing a given input using the delta rule. The learning algorithm for perceptrons is an online algorithm, in that it processes elements in the training set one at a time.},
	language = {en},
	urldate = {2013-06-17},
	journal = {Wikipedia, the free encyclopedia},
	month = may,
	year = {2013},
	note = {Page Version {ID:} 557301943},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/P3MWEXNI/index.html:text/html}
},

@article{patil_pymc:_2010,
	title = {{PyMC:} Bayesian stochastic modelling in Python},
	volume = {35},
	lccn = {0071},
	shorttitle = {{PyMC}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/},
	number = {4},
	urldate = {2013-06-17},
	journal = {Journal of statistical software},
	author = {Patil, Anand and Huard, David and Fonnesbeck, Christopher J.},
	year = {2010},
	pages = {1},
	file = {[HTML] from nih.gov:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3NWB7TGN/PMC3097064.html:text/html}
},

@misc{koller_daphne_2012,
	title = {Daphne Koller: What we're learning from online education {\textbar} Video on {TED.com}},
	shorttitle = {Daphne Koller},
	url = {http://www.ted.com/talks/daphne_koller_what_we_re_learning_from_online_education.html},
	abstract = {Daphne Koller is enticing top universities to put their most intriguing courses online for free -- not just as a service, but as a way to research how people learn. With Coursera (cofounded by Andrew Ng), each keystroke, quiz, peer-to-peer discussion and self-graded assignment builds an unprecedented pool of data on how knowledge is processed.},
	urldate = {2013-06-24},
	collaborator = {Koller, Daphne},
	month = aug,
	year = {2012},
	keywords = {Computers, education, global issues, Internet, Talks, {TED}},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/83HZTXGW/daphne_koller_what_we_re_learning_from_online_education.html:text/html}
},

@book{alpaydin_introduction_2010,
	address = {Cambridge, Massachusetts; London},
	title = {Introduction to machine learning},
	isbn = {9780262012430  {026201243X}},
	lccn = {0006},
	language = {English},
	publisher = {The {MIT} Press},
	author = {Alpaydin, E},
	year = {2010}
},

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass.},
	title = {Gaussian processes for machine learning},
	isbn = {{026218253X} 9780262182539},
	lccn = {3404},
	abstract = {{"Gaussian} processes ({GPs)} provide a principled, practical, probabilistic approach to learning in kernel machines. {GPs} have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of {GPs} in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics."--Jacket.},
	language = {English},
	publisher = {{MIT} Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I},
	year = {2006}
},

@book{marsland_machine_2009,
	address = {Boca Raton, Mass. [u.a.},
	title = {Machine learning: an algorithmic perspective},
	isbn = {9781420067187  1420067184},
	lccn = {0001},
	shorttitle = {Machine learning},
	language = {English},
	publisher = {{CRC} {Press/Taylor} \& Francis},
	author = {Marsland, Stephen},
	year = {2009}
},

@book{mitchell_machine_1997,
	address = {New York, {NY} [u.a.},
	title = {Machine learning},
	isbn = {0071154671 9780071154673},
	lccn = {25784},
	language = {English},
	publisher = {{McGraw-Hill}},
	author = {Mitchell, Tom M},
	year = {1997}
},

@article{ward_short-term_2006,
	title = {Short-term prediction of mortality in patients with systemic lupus erythematosus: Classification of outcomes using random forests},
	volume = {55},
	copyright = {Copyright © 2006 by the American College of Rheumatology},
	issn = {1529-0131},
	lccn = {0039},
	shorttitle = {Short-term prediction of mortality in patients with systemic lupus erythematosus},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/art.21695/abstract},
	doi = {10.1002/art.21695},
	abstract = {{ObjectiveTo} identify demographic and clinical characteristics that classify patients with systemic lupus erythematosus ({SLE)} at risk for in-hospital {mortality.MethodsPatients} hospitalized in California from 1996 to 2000 with a principal diagnosis of {SLE} (N = 3,839) were identified from a state hospitalization database. As candidate predictors of mortality, we used patient demographic characteristics; the presence or absence of 40 different clinical conditions listed among the discharge diagnoses; and 2 summary indexes derived from the discharge diagnoses, the Charlson Index and the {SLE} Comorbidity Index. Predictors of patients at increased risk of mortality were identified and validated using random forests, a statistical procedure that is a generalization of single classification trees. Random forests use bootstrapped samples of patients and randomly selected subsets of predictors to create individual classification trees, and this process is repeated to generate multiple trees (a forest). Classification is then done by majority vote across all {trees.ResultsOf} the 3,839 patients, 109 died during hospitalization. Selecting from all available predictors, the random forests had excellent predictive accuracy for classification of death. The mean classification error rate, averaged over 10 forests of 500 trees each, was 11.9\%. The most important predictors were the Charlson Index, respiratory failure, {SLE} Comorbidity Index, age, sepsis, nephritis, and {thrombocytopenia.ConclusionInformation} on clinical diagnoses can be used to accurately predict mortality among hospitalized patients with {SLE.} Random forests represent a useful technique to identify the most important predictors from a larger (often much larger) number and to validate the classification.},
	language = {en},
	number = {1},
	urldate = {2013-07-16},
	journal = {Arthritis Care \& Research},
	author = {Ward, Michael M. and Pajevic, Sinisa and Dreyfuss, Jonathan and Malley, James D.},
	year = {2006},
	keywords = {Classification tree, Hospitalization, Mortality, Random forest, Systemic lupus erythematosus},
	pages = {74–80},
	file = {Full Text PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/TC4ZHWXS/Ward et al. - 2006 - Short-term prediction of mortality in patients wit.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/DIDEBIB3/abstract.html:text/html}
},

@article{steinberg_cart:_2009,
	title = {{CART:} classification and regression trees},
	shorttitle = {{CART}},
	url = {http://books.google.co.uk/books?hl=en&lr=&id=_kcEn-c9kYAC&oi=fnd&pg=PA179&dq=dan+steinberg+cart&ots=eQ7jtfUODm&sig=Xs8kegu_D4DcrPhT6TUkB0LCV1A},
	urldate = {2013-09-12},
	journal = {The Top Ten Algorithms in Data Mining},
	author = {Steinberg, Dan and Colla, Phillip},
	year = {2009},
	pages = {179–201},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/CMM3XGQE/books.html:text/html}
},

@article{morgan_problems_1963,
	title = {Problems in the analysis of survey data, and a proposal},
	volume = {58},
	url = {http://amstat.tandfonline.com/doi/full/10.1080/01621459.1963.10500855},
	number = {302},
	urldate = {2013-09-17},
	journal = {Journal of the American Statistical Association},
	author = {Morgan, James N. and Sonquist, John A.},
	year = {1963},
	pages = {415–434},
	file = {[PDF] from uiuc.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/8UI33RE4/Morgan and Sonquist - 1963 - Problems in the analysis of survey data, and a pro.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/7HCASV3V/01621459.1963.html:text/html}
},

@article{cortes_support-vector_1995,
	title = {Support-Vector Networks},
	volume = {20},
	issn = {0885-6125},
	doi = {10.1023/A:1022627411411},
	abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
	language = {English},
	number = {3},
	journal = {Machine Learning},
	author = {Cortes, C. and Vapnik, V.},
	month = sep,
	year = {1995},
	note = {{WOS:A1995RX35400003}},
	keywords = {efficient learning algorithms, neural networks, pattern recognition, polynomial classifiers, radial basis function classifiers},
	pages = {273--297}
},

@article{einhorn_alchemy_1972,
	title = {Alchemy in the Behavioral Sciences},
	volume = {36},
	issn = {0033-{362X}, 1537-5331},
	url = {http://poq.oxfordjournals.org/content/36/3/367},
	doi = {10.1086/268019},
	abstract = {Access to powerful new computers has encouraged routine use of highly complex analytic techniques, often in the absence of any theory, hypotheses, or model to guide the researcher's expectations of results. The author examines the potential of such techniques for generating spurious results, and urges that in exploratory work the outcome be subjected to a more rigorous criterion than the usual tests of statistical significance.},
	language = {en},
	number = {3},
	urldate = {2013-09-18},
	journal = {Public Opinion Quarterly},
	author = {Einhorn, Hillel J.},
	month = sep,
	year = {1972},
	pages = {367--378},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/X3J3BMFX/367.html:text/html}
},

@article{breiman_cart:_1984,
	title = {{CART:} Classification and regression trees},
	volume = {156},
	shorttitle = {{CART}},
	journal = {Wadsworth: Belmont, {CA}},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, Richard and Stone, Charles and Steinberg, D. and Colla, P.},
	year = {1984}
},

@article{doyle_use_1973,
	title = {The use of automatic interaction detector and similar search procedures},
	url = {http://www.jstor.org/stable/10.2307/3008131},
	urldate = {2013-09-18},
	journal = {Operational Research Quarterly},
	author = {Doyle, Peter},
	year = {1973},
	pages = {465–467},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/DXU7KQHG/3008131.html:text/html}
},

@article{cover_nearest_1967,
	title = {Nearest neighbor pattern classification},
	volume = {13},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1053964},
	number = {1},
	urldate = {2013-09-19},
	journal = {Information Theory, {IEEE} Transactions on},
	author = {Cover, Thomas and Hart, Peter},
	year = {1967},
	pages = {21–27},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/AQB243F7/login.html:text/html}
},

@article{friedman_recursive_1977,
	title = {Recursive Partitioning Decision Rule for Nonparametric Classification},
	volume = {26},
	number = {4},
	journal = {Ieee Transactions on Computers},
	author = {Friedman, Jh},
	year = {1977},
	note = {{WOS:A1977DG09400009}},
	pages = {404--408}
},

@article{quinlan_induction_1986,
	title = {Induction of decision trees},
	volume = {1},
	url = {http://link.springer.com/article/10.1023/A:1022643204877},
	number = {1},
	urldate = {2013-09-25},
	journal = {Machine learning},
	author = {Quinlan, J. Ross},
	year = {1986},
	pages = {81–106},
	file = {[PDF] from googlecode.com:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3VG28M3F/Quinlan - 1986 - Induction of decision trees.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/MAF3CH37/A1022643204877.html:text/html}
},

@article{domingos_few_2012,
	title = {A few useful things to know about machine learning},
	volume = {55},
	url = {http://dl.acm.org/citation.cfm?id=2347755},
	number = {10},
	urldate = {2013-09-25},
	journal = {Communications of the {ACM}},
	author = {Domingos, Pedro},
	year = {2012},
	pages = {78–87},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/K92BG2X7/citation.html:text/html}
},

@book{quinlan_c4._1993,
	title = {C4. 5: programs for machine learning},
	volume = {1},
	shorttitle = {C4. 5},
	url = {http://books.google.co.uk/books?hl=en&lr=&id=HExncpjbYroC&oi=fnd&pg=PR7&dq=quinlan+c4.5&ots=nKr8dYr51o&sig=e8CDMoHt0VGemFkhf8z9VMA8KlI},
	urldate = {2013-09-26},
	publisher = {Morgan kaufmann},
	author = {Quinlan, John Ross},
	year = {1993},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/N6GSGR8B/books.html:text/html}
},

@book{bellman_adaptive_1961,
	title = {Adaptive control processes: a guided tour},
	volume = {4},
	shorttitle = {Adaptive control processes},
	url = {http://www.getcited.org/pub/101191710},
	urldate = {2013-09-30},
	publisher = {Princeton university press Princeton},
	author = {Bellman, Richard},
	year = {1961},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/JA2JAC9D/101191710.html:text/html}
},

@article{chang_libsvm:_2011,
	title = {{LIBSVM:} a library for support vector machines},
	volume = {2},
	shorttitle = {{LIBSVM}},
	url = {http://dl.acm.org.ezproxy.lancs.ac.uk/citation.cfm?id=1961199},
	number = {3},
	urldate = {2013-10-04},
	journal = {{ACM} Transactions on Intelligent Systems and Technology ({TIST)}},
	author = {Chang, Chih-Chung and Lin, Chih-Jen},
	year = {2011},
	pages = {27},
	file = {[PDF] from 140.112.30.28:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/9X4U49SA/Chang and Lin - 2011 - LIBSVM a library for support vector machines.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/K9XNAVFS/citation.html:text/html}
},

@article{burges_tutorial_1998,
	title = {A tutorial on support vector machines for pattern recognition},
	volume = {2},
	url = {http://link.springer.com.ezproxy.lancs.ac.uk/article/10.1023/A:1009715923555},
	number = {2},
	urldate = {2013-10-04},
	journal = {Data mining and knowledge discovery},
	author = {Burges, Christopher {JC}},
	year = {1998},
	pages = {121–167},
	file = {[PDF] from mingzeng.net:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/29STDTR3/Burges - 1998 - A tutorial on support vector machines for pattern .pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/MQ5JNWFJ/A1009715923555.html:text/html}
},

@article{fisher_use_1936,
	title = {The use of multiple measurements in taxonomic problems},
	volume = {7},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/full},
	number = {2},
	urldate = {2013-10-04},
	journal = {Annals of eugenics},
	author = {Fisher, Ronald A.},
	year = {1936},
	pages = {179–188},
	file = {[PDF] from adelaide.edu.au:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/IIB6XP2G/Fisher - 1936 - The use of multiple measurements in taxonomic prob.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/MUEZJMAQ/full.html:text/html}
}